$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: azureml_translation_finetuning
version: 0.0.1
type: command

is_deterministic: True

display_name: FineTuning for Translation
description: Component to finetune model for Translation task

environment: azureml:azml-train-finetune-curated-acpt-pytorch-111-py38-cuda113-gpu:0.0.1

code: ../scripts/finetune

distribution:
  type: pytorch

inputs:
  # Lora parameters
  apply_lora:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: lora enabled

  merge_lora_weights:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: if set to true, the lora trained weights will be merged to base model before saving

  lora_alpha:
    type: integer
    default: 128
    optional: true
    description: lora attention alpha

  lora_r:
    type: integer
    default: 8
    optional: true
    description: lora dimension

  lora_dropout:
    type: number
    default: 0.0
    optional: true
    description: lora dropout value

  # Training parameters
  epochs:
    type: integer
    default: 1
    optional: true
    description: training epochs

  max_steps:
    type: integer
    default: -1
    optional: true
    description: If set to a positive number, the total number of training steps to perform. Overrides 'epochs'. In case of using a finite iterable dataset the training may stop before reaching the set number of steps when all data is exhausted.

  train_batch_size:
    type: integer
    default: 1
    optional: true
    description: Train batch size

  valid_batch_size:
    type: integer
    default: 1
    optional: true
    description: Validation batch size

  auto_find_batch_size:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: Flag to enable auto finding of batch size. If the provided 'train_batch_size' goes into Out Of Memory (OOM) enabling auto_find_batch_size will find the correct batch size by iteratively reducing 'train_batch_size' by a factor of 2 till the OOM is fixed

  optimizer:
    type: string
    default: adamw_hf
    optional: true
    enum:
      - adamw_hf
      - adamw_torch
      # - adamw_apex_fused
      - adafactor
    description: Optimizer to be used while training

  learning_rate:
    type: number
    default: 0.00002
    optional: true
    description: Start learning rate. Defaults to linear scheduler.

  warmup_steps:
    type: integer
    default: 0
    optional: true
    description: Number of steps used for a linear warmup from 0 to learning_rate

  weight_decay:
    type: number
    default: 0.0
    optional: true
    description: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer

  adam_beta1:
    type: number
    default: 0.9
    optional: true
    description: The beta1 hyperparameter for the AdamW optimizer

  adam_beta2:
    type: number
    default: 0.999
    optional: true
    description: The beta2 hyperparameter for the AdamW optimizer

  adam_epsilon:
    type: number
    default: 1e-8
    optional: true
    description: The epsilon hyperparameter for the AdamW optimizer

  gradient_accumulation_steps:
    type: integer
    default: 1
    optional: true
    description: Number of updates steps to accumulate the gradients for, before performing a backward/update pass

  lr_scheduler_type:
    type: string
    default: linear
    optional: true
    enum:
      - linear
      - cosine
      - cosine_with_restarts
      - polynomial
      - constant
      - constant_with_warmup
    description: The scheduler type to use

  precision:
    type: string
    enum:
      - "32"
      - "16"
    default: "32"
    optional: true
    description: Apply mixed precision training. This can reduce memory footprint by performing operations in half-precision.

  seed:
    type: integer
    default: 42
    optional: true
    description: Random seed that will be set at the beginning of training

  evaluation_strategy:
    type: string
    default: epoch
    optional: true
    enum:
      - epoch
      - steps
    description: The evaluation strategy to adopt during training

  evaluation_steps_interval:
    type: number
    default: 0.0
    optional: true
    description: The evaluation steps in fraction of an epoch steps to adopt during training. Overwrites evaluation_steps if not 0.

  evaluation_steps:
    type: integer
    default: 500
    optional: true
    description: Number of update steps between two evals if evaluation_strategy='steps'

  logging_strategy:
    type: string
    default: epoch
    optional: true
    enum:
      - epoch
      - steps
    description: The logging strategy to adopt during training.

  logging_steps:
    type: integer
    default: 500
    optional: true
    description: Number of update steps between two logs if logging_strategy='steps'

  metric_for_best_model:
    type: string
    default: loss
    optional: true
    enum:
      - loss
      - bleu
    description: Specify the metric to use to compare two different models

  resume_from_checkpoint:
    type: string
    default: "false"
    optional: true
    enum:
      - "true"
      - "false"
    description: Loads Optimizer, Scheduler and Trainer state for finetuning if true

  save_total_limit:
    type: integer
    default: -1
    optional: true
    description: If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir. If the value is -1 saves all checkpoints"

  # Early Stopping Parameters
  apply_early_stopping:
    type: string
    default: "false"
    optional: true
    enum:
      - "true"
      - "false"
    description: Enable early stopping

  early_stopping_patience:
    type: integer
    default: 1
    optional: true
    description: Stop training when the specified metric worsens for early_stopping_patience evaluation calls

  early_stopping_threshold:
    type: number
    default: 0.0
    optional: true
    description: Denotes how much the specified metric must improve to satisfy early stopping conditions

  # Deepspeed Parameters
  apply_deepspeed:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: If set to true, will enable deepspeed for training

  # ORT Parameters
  apply_ort:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: If set to true, will use the ONNXRunTime training

  deepspeed_config:
    type: uri_file
    optional: true
    description: If apply_deepspeed is set to True, if provided, this file will be used as deepspeed config else default deep speed config is used. Check the code section for the default config.

  # MLFlow Parameters
  save_as_mlflow_model:
    type: string
    enum:
      - "true"
      - "false"
    default: "true"
    optional: true
    description: If set to true, will save as mlflow model with pyfunc as flavour

  # Dataset parameters
  model_path:
    type: uri_folder
    optional: false
    description: output folder of model selector containing model metadata like config, checkpoints, tokenizer config

  dataset_path:
    type: uri_folder
    optional: false
    description: output folder of preprocessor containing encoded train.jsonl valid.jsonl and the model pretrained info

outputs:
  output_dir_pytorch:
    type: uri_folder
    description: Output dir to save the finetune model and other metadata

  output_dir_mlflow:
    type: mlflow_model
    description: Output dir to save the finetune model as mlflow model

command: >-
  python finetune.py $[[--apply_lora ${{inputs.apply_lora}}]] $[[--merge_lora_weights ${{inputs.merge_lora_weights}}]] $[[--lora_alpha ${{inputs.lora_alpha}}]] $[[--lora_r ${{inputs.lora_r}}]] $[[--lora_dropout ${{inputs.lora_dropout}}]] $[[--epochs ${{inputs.epochs}}]] $[[--max_steps ${{inputs.max_steps}}]] $[[--train_batch_size ${{inputs.train_batch_size}}]] $[[--valid_batch_size ${{inputs.valid_batch_size}}]] $[[--auto_find_batch_size ${{inputs.auto_find_batch_size}}]] $[[--optimizer ${{inputs.optimizer}}]] $[[--lr ${{inputs.learning_rate}}]] $[[--warmup_steps ${{inputs.warmup_steps}}]] $[[--weight_decay ${{inputs.weight_decay}}]] $[[--adam_beta1 ${{inputs.adam_beta1}}]] $[[--adam_beta2 ${{inputs.adam_beta2}}]] $[[--adam_epsilon ${{inputs.adam_epsilon}}]] $[[--gradient_accumulation_steps ${{inputs.gradient_accumulation_steps}}]] $[[--lr_scheduler_type ${{inputs.lr_scheduler_type}}]] $[[--precision ${{inputs.precision}}]] $[[--seed ${{inputs.seed}}]] $[[--evaluation_strategy ${{inputs.evaluation_strategy}}]] $[[--evaluation_steps_interval ${{inputs.evaluation_steps_interval}}]] $[[--evaluation_steps ${{inputs.evaluation_steps}}]] $[[--logging_strategy ${{inputs.logging_strategy}}]] $[[--logging_steps ${{inputs.logging_steps}}]] $[[--metric_for_best_model ${{inputs.metric_for_best_model}}]] $[[--resume_from_checkpoint ${{inputs.resume_from_checkpoint}}]] $[[--save_total_limit ${{inputs.save_total_limit}}]] $[[--apply_early_stopping ${{inputs.apply_early_stopping}}]] $[[--early_stopping_patience ${{inputs.early_stopping_patience}}]] $[[--early_stopping_threshold ${{inputs.early_stopping_threshold}}]] $[[--apply_ort ${{inputs.apply_ort}}]] $[[--apply_deepspeed ${{inputs.apply_deepspeed}}]] $[[--deepspeed_config ${{inputs.deepspeed_config}}]] $[[--save_as_mlflow_model ${{inputs.save_as_mlflow_model}}]] --model_path ${{inputs.model_path}} --dataset_path ${{inputs.dataset_path}} --output_dir_pytorch ${{outputs.output_dir_pytorch}} --output_dir_mlflow ${{outputs.output_dir_mlflow}}

