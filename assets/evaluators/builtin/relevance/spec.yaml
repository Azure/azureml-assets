type: "evaluator"
name: "builtin.relevance"
version: 2
displayName: "Relevance-Evaluator"
description: "| \t| |\n| -- | -- |\n| Score range |\tInteger [1-5]: 1 is the lowest quality and 5 is the highest quality. |\n| What is this metric? | Coherence measures the logical and orderly presentation of ideas in a response, allowing the reader to easily follow and understand the writer's train of thought. A coherent response directly addresses the question with clear connections between sentences and paragraphs, using appropriate transitions and a logical sequence of ideas. |\n| How does it work? | The coherence metric is calculated by instructing a language model to follow the definition (in the description) and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). Learn more about our [definition and grading rubrics](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#ai-assisted-relevance). |\n| When to use it? |\tThe recommended scenario is generative business writing such as summarizing meeting notes, creating marketing materials, and drafting email. |\n| What does it need as input? |\tQuery, Response |\n"
evaluatorType: "builtin"
evaluatorSubType: "code"
categories: ["quality"]
tags:
  provider: "Microsoft"
initParameterSchema:
  type: "object"
  properties:
    deployment_name:
      type: "string"
    threshold:
      type: "number"
      minimum: 1
      maximum: 5
      multipleOf: 1
  required: ["deployment_name"]
dataMappingSchema:
  type: "object"
  properties:
    query:
      type: "string"
    response:
      type: "string"
  required: ["query", "response"]
outputSchema:
  relevance:
    type: "ordinal"
    desirable_direction: "increase"
    min_value: 1
    max_value: 5
path: ./evaluator