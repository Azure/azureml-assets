$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
type: command

version: 0.0.3
name: multimodal_classification_finetune
display_name: Multimodal Classification using MMEFT
description: Component to finetune multimodal models for classification using MMEFT

is_deterministic: True

environment: azureml://registries/azureml/environments/acft-multimodal-gpu/versions/20

code: ../../../src/finetune

distribution:
  type: pytorch

inputs:
  problem_type:
    type: string
    default: multimodal-classification-singlelabel
    optional: false
    enum:
      - multimodal-classification-singlelabel
      - multimodal-classification-multilabel
    description: Specify whether its single-label or multi-label multimodal classification task.

  # Training parameters
  number_of_epochs:
    type: integer
    default: 1
    optional: true
    description: training epochs

  max_steps:
    type: integer
    default: -1
    optional: true
    description: If set to a positive number, the total number of training steps to perform. Overrides 'epochs'. In case of using a finite iterable dataset the training may stop before reaching the set number of steps when all data is exhausted.

  training_batch_size:
    type: integer
    default: 8
    optional: true
    description: Train batch size

  validation_batch_size:
    type: integer
    default: 64
    optional: true
    description: Validation batch size

  auto_find_batch_size:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: Flag to enable auto finding of batch size. If the provided 'training_batch_size' goes into Out Of Memory (OOM) enabling auto_find_batch_size will find the correct batch size by iteratively reducing 'training_batch_size' by a factor of 2 till the OOM is fixed

  optimizer:
    type: string
    default: adamw_hf
    optional: true
    enum:
      - adamw_hf
      - adamw_torch
      # - adamw_apex_fused
      - adafactor
    description: Optimizer to be used while training

  learning_rate:
    type: number
    default: 0.001
    optional: true
    description: Start learning rate. Defaults to linear scheduler.

  warmup_steps:
    type: integer
    default: 0
    optional: true
    description: Number of steps used for a linear warmup from 0 to learning_rate

  weight_decay:
    type: number
    default: 0.0
    optional: true
    description: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer

  adam_beta1:
    type: number
    default: 0.9
    optional: true
    description: The beta1 hyperparameter for the AdamW optimizer

  adam_beta2:
    type: number
    default: 0.999
    optional: true
    description: The beta2 hyperparameter for the AdamW optimizer

  adam_epsilon:
    type: number
    default: 1e-8
    optional: true
    description: The epsilon hyperparameter for the AdamW optimizer

  gradient_accumulation_steps:
    type: integer
    default: 64
    optional: true
    description: Number of updates steps to accumulate the gradients for, before performing a backward/update pass

  learning_rate_scheduler:
    type: string
    default: linear
    optional: true
    enum:
      - linear
      - cosine
      - cosine_with_restarts
      - polynomial
      - constant
      - constant_with_warmup
    description: The scheduler type to use

  precision:
    type: string
    enum:
      - "32"
      - "16"
    default: "32"
    optional: true
    description: Apply mixed precision training. This can reduce memory footprint by performing operations in half-precision.

  random_seed:
    type: integer
    default: 42
    optional: true
    description: Random seed that will be set at the beginning of training

  enable_full_determinism:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: Ensure reproducible behavior during distributed training

  dataloader_num_workers:
    type: integer
    default: 0
    optional: true
    description: Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.

  ignore_mismatched_sizes:
    type: string
    enum:
      - "true"
      - "false"
    default: "true"
    optional: true
    description: Whether or not to raise an error if some of the weights from the checkpoint do not have the same size as the weights of the model

  max_grad_norm:
    type: number
    default: 1.0
    optional: true
    description: "Maximum gradient norm (for gradient clipping)"

  evaluation_strategy:
    type: string
    default: epoch
    optional: true
    enum:
      - epoch
      - steps
    description: The evaluation strategy to adopt during training

  evaluation_steps_interval:
    type: number
    default: 0.0
    optional: true
    description: The evaluation steps in fraction of an epoch steps to adopt during training. Overwrites evaluation_steps if not 0.

  evaluation_steps:
    type: integer
    default: 500
    optional: true
    description: Number of update steps between two evals if evaluation_strategy='steps'

  logging_strategy:
    type: string
    default: epoch
    optional: true
    enum:
      - epoch
      - steps
    description: The logging strategy to adopt during training.

  logging_steps:
    type: integer
    default: 500
    optional: true
    description: Number of update steps between two logs if logging_strategy='steps'

  primary_metric:
    type: string
    default: loss
    optional: true
    enum:
      - loss
      - f1_macro
      - mcc
      - accuracy
      - precision_macro
      - recall_macro
    description: Specify the metric to use to compare two different models

  resume_from_checkpoint:
    type: string
    default: "false"
    optional: true
    enum:
      - "true"
      - "false"
    description: Loads Optimizer, Scheduler and Trainer state for finetuning if true

  save_total_limit:
    type: integer
    default: -1
    optional: true
    description: If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir. If the value is -1 saves all checkpoints"

  # Early Stopping Parameters
  apply_early_stopping:
    type: string
    default: "false"
    optional: true
    enum:
      - "true"
      - "false"
    description: Enable early stopping

  early_stopping_patience:
    type: integer
    default: 1
    optional: true
    description: Stop training when the specified metric worsens for early_stopping_patience evaluation calls

  early_stopping_threshold:
    type: number
    default: 0.0
    optional: true
    description: Denotes how much the specified metric must improve to satisfy early stopping conditions

  # Deepspeed Parameters
  apply_deepspeed:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: If set to true, will enable deepspeed for training

  deepspeed_config:
    type: uri_file
    optional: true
    description: Deepspeed config to be used for finetuning

  # ORT Parameters
  apply_ort:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: If set to true, will use the ONNXRunTime training

  # MLFlow Parameters
  save_as_mlflow_model:
    type: string
    enum:
      - "true"
      - "false"
    default: "true"
    optional: true
    description: If set to true, will save as mlflow model with pyfunc as flavour

  # Dataset parameterss
  preprocess_output:
    type: uri_folder
    optional: false
    description: output folder of preprocessor containing preprocessed metadata information

  model_selector_output:
    type: uri_folder
    optional: false
    description: output folder of model selector containing model metadata like config, checkpoints, tokenizer config

outputs:
  pytorch_model_folder:
    type: uri_folder
    description: Output dir to save the finetune model and other metadata

  mlflow_model_folder:
    type: mlflow_model
    description: Output dir to save the finetune model as mlflow model

command: >-
  python finetune.py
  --problem_type ${{inputs.problem_type}}
  --apply_lora "false"
  --merge_lora_weights "true"
  --lora_alpha 128
  --lora_r 8
  --lora_dropout 0.0
  $[[--num_train_epochs ${{inputs.number_of_epochs}}]]
  $[[--max_steps ${{inputs.max_steps}}]]
  $[[--per_device_train_batch_size ${{inputs.training_batch_size}}]]
  $[[--per_device_eval_batch_size ${{inputs.validation_batch_size}}]]
  $[[--auto_find_batch_size ${{inputs.auto_find_batch_size}}]]
  $[[--optim ${{inputs.optimizer}}]]
  $[[--learning_rate ${{inputs.learning_rate}}]]
  $[[--warmup_steps ${{inputs.warmup_steps}}]]
  $[[--weight_decay ${{inputs.weight_decay}}]]
  $[[--adam_beta1 ${{inputs.adam_beta1}}]]
  $[[--adam_beta2 ${{inputs.adam_beta2}}]]
  $[[--adam_epsilon ${{inputs.adam_epsilon}}]]
  $[[--gradient_accumulation_steps ${{inputs.gradient_accumulation_steps}}]]
  $[[--lr_scheduler_type ${{inputs.learning_rate_scheduler}}]]
  $[[--precision ${{inputs.precision}}]]
  $[[--seed ${{inputs.random_seed}}]]
  $[[--enable_full_determinism
  ${{inputs.enable_full_determinism}}]]
  $[[--dataloader_num_workers ${{inputs.dataloader_num_workers}}]]
  $[[--ignore_mismatched_sizes ${{inputs.ignore_mismatched_sizes}}]]
  $[[--max_grad_norm ${{inputs.max_grad_norm}}]]
  $[[--evaluation_strategy ${{inputs.evaluation_strategy}}]]
  $[[--evaluation_steps_interval ${{inputs.evaluation_steps_interval}}]]
  $[[--eval_steps ${{inputs.evaluation_steps}}]]
  $[[--logging_strategy ${{inputs.logging_strategy}}]]
  $[[--logging_steps ${{inputs.logging_steps}}]]
  $[[--metric_for_best_model ${{inputs.primary_metric}}]]
  $[[--resume_from_checkpoint ${{inputs.resume_from_checkpoint}}]]
  $[[--save_total_limit ${{inputs.save_total_limit}}]]
  $[[--apply_early_stopping ${{inputs.apply_early_stopping}}]]
  $[[--early_stopping_patience ${{inputs.early_stopping_patience}}]]
  $[[--early_stopping_threshold ${{inputs.early_stopping_threshold}}]]
  $[[--apply_ort ${{inputs.apply_ort}}]]
  $[[--apply_deepspeed ${{inputs.apply_deepspeed}}]]
  $[[--deepspeed ${{inputs.deepspeed_config}}]]
  $[[--save_as_mlflow_model ${{inputs.save_as_mlflow_model}}]]
  --model_selector_output ${{inputs.model_selector_output}}
  --preprocess_output ${{inputs.preprocess_output}}
  --pytorch_model_folder ${{outputs.pytorch_model_folder}}
  --mlflow_model_folder ${{outputs.mlflow_model_folder}}
