$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: compute_metrics
display_name: Compute Metrics
description: Calculate model performance metrics, given ground truth and prediction data.

version: 0.0.15
type: command
tags:
  type: evaluation
  sub_type: compute_metrics
  Preview: ""

inputs:
  task:
    type: string
    optional: false
    default: tabular-classification
    enum: [
      tabular-classification,
      tabular-classification-multilabel,
      tabular-regression,
      tabular-forecasting,
      text-classification,
      text-classification-multilabel,
      text-named-entity-recognition,
      text-summarization,
      question-answering,
      text-translation,
      text-generation,
      fill-mask,
      image-classification,
      image-classification-multilabel,
      image-object-detection,
      image-instance-segmentation,
    ]
    description: "Task type"
  ground_truth:
    type: uri_folder
    optional: false
    description: "Ground Truths of Test Data as a 1-column JSON Lines file"
  ground_truth_column_name:
    type: string
    optional: true
    description: "Column name which contains ground truths in provided uri file for ground_truth. (Optional if we have one column name.)"
  prediction:
    type: uri_folder
    optional: false
    description: "Model Predictions as a 1-column JSON Lines file"
  prediction_column_name:
    type: string
    optional: true
    description: "Column name which contains ground truths in provided uri file for prediction. (Optional if we have one column name.)"
  prediction_probabilities:
    type: uri_folder
    optional: true
    description: "Predictions Probabilities as 1-column JSON Lines file"
  evaluation_config:
    type: uri_file
    optional: true
    description: "Additional parameters required for evaluation."
  evaluation_config_params:
    type: string
    optional: true
    description: "JSON Serialized string of evaluation_config"

outputs:
  evaluation_result:
    type: uri_folder

is_deterministic: True
code: ../../src
environment: azureml://registries/azureml/environments/model-evaluation/versions/12

command: >-
  python compute_metrics.py
  --task ${{inputs.task}}
  --ground_truths '${{inputs.ground_truth}}'
  --predictions '${{inputs.prediction}}'
  --output '${{outputs.evaluation_result}}'
  $[[--prediction_probabilities '${{inputs.prediction_probabilities}}']]
  $[[--config-file-name '${{inputs.evaluation_config}}']]
  $[[--ground_truths_column_name ${{inputs.ground_truth_column_name}}]]
  $[[--predictions_column_name ${{inputs.prediction_column_name}}]]
  $[[--config_str '${{inputs.evaluation_config_params}}']]
