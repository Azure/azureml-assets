$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: evaluate_model
display_name: Evaluate Model
description: Evaluate MLFlow models for supported task types.

version: 0.0.19
type: command
tags:
  type: evaluation
  sub_type: evaluate_model

inputs:
  task:
    type: string
    optional: false
    default: tabular-classification
    enum: [
      tabular-classification,
      tabular-classification-multilabel,
      tabular-regression,
      tabular-forecasting,
      text-classification,
      text-classification-multilabel,
      text-named-entity-recognition,
      text-summarization,
      question-answering,
      text-translation,
      fill-mask,
      text-generation,
      image-classification,
      image-classification-multilabel,
      chat-completion,
      image-object-detection,
      image-instance-segmentation,
    ]
    description: "Task type"
  test_data:
    type: uri_folder
    optional: false
    mode: ro_mount
    description: "Test Data as JSON Lines URI_FILE"
  evaluation_config:
    type: uri_file
    optional: true
    mode: ro_mount
    description: "Additional parameters required for evaluation."
  test_data_label_column_name:
    type: string
    optional: false
    description: Column name of target values
  test_data_input_column_names:
    type: string
    optional: true
    description: Comma separated values of feature column names
  mlflow_model:
    type: mlflow_model
    optional: false
    mode: ro_mount
    description: "Mlflow Model - registered model or output of a job with type mlflow_model in a pipeline"
  device:
    type: string
    optional: false
    default: auto
    enum: [auto, cpu, gpu]
  batch_size:
    type: integer
    optional: true
  evaluation_config_params:
    type: string
    optional: true
    description: "JSON Serialized string of evaluation_config"

outputs:
  evaluation_result:
    type: uri_folder

is_deterministic: True
code: ../../src
environment: azureml://registries/azureml/environments/model-evaluation/versions/16

command: >-
  python download_dependencies.py
  --mlflow-model '${{inputs.mlflow_model}}' &&
  python evaluate_model.py
  --task '${{inputs.task}}'
  --output '${{outputs.evaluation_result}}'
  --label-column-name '${{inputs.test_data_label_column_name}}'
  --mlflow-model '${{inputs.mlflow_model}}'
  --device '${{inputs.device}}'
  --data '${{inputs.test_data}}'
  $[[--input-column-names '${{inputs.test_data_input_column_names}}']]
  $[[--config-file-name '${{inputs.evaluation_config}}']]
  $[[--batch-size '${{inputs.batch_size}}']]
  $[[--config_str '${{inputs.evaluation_config_params}}']]