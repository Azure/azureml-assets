$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
name: medimage_insight_ft_pipeline
version: 0.0.1
type: pipeline
display_name: Medical Image Insight Embedding Generator and Classification Adapter Pipeline
description: Pipeline Component to finetune Hugging Face pretrained models for chat completion task. The component supports optimizations such as LoRA, Deepspeed and ONNXRuntime for performance enhancement. See [docs](https://aka.ms/azureml/components/chat_completion_pipeline) to learn more.

inputs:
  mlflow_embedding_model_path:
    type: uri_folder
    optional: false
    description: Path to the MLflow model to be imported.
    mode: ro_mount
  
  eval_image_tsv:
    type: uri_file
    optional: false
    description: Path to the evaluation image TSV file.
    mode: ro_mount

  eval_text_tsv:
    type: uri_file
    optional: false
    description: Path to the evaluation text TSV file.
    mode: ro_mount

  image_tsv:
    type: uri_file
    optional: false
    description: Path to the image TSV file.
    mode: ro_mount

  text_tsv:
    type: uri_file
    optional: false
    description: Path to the text TSV file.
    mode: ro_mount

  label_file:
    type: uri_file
    optional: false
    description: Path to the label file.
    mode: ro_mount

  conf_files:
    type: uri_file
    optional: false
    description: Path to the configuration files.
    mode: ro_mount
  
  # preprocessing settings
  image_standardization_jpeg_compression_ratio:
    type: integer
    optional: true
    default: 75
    description: JPEG compression ratio for image standardization.

  image_standardization_image_size:
    type: integer
    optional: true
    default: 512
    description: Image size for standardization.

#start of adapter pipeline inputs 
  instance_type_preprocess:
      type: string
      optional: true
      default: Standard_d12_v2
      description: Instance type to be used for preprocess component in case of serverless compute, eg. standard_d12_v2. 
        The parameter compute_preprocess must be set to 'serverless' for instance_type to be used

  instance_type_finetune:
    type: string
    optional: true
    default: Standard_nc24rs_v3
    description: Instance type to be used for finetune component in case of serverless compute, eg. standard_nc24rs_v3. 
      The parameter compute_finetune must be set to 'serverless' for instance_type to be used

  compute_preprocess:
    type: string
    optional: true
    default: serverless
    description: compute to be used for preprocess eg. provide 'FT-Cluster' if your
      compute is named 'FT-Cluster'. Special characters like \ and ' are invalid in the parameter value.
      If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used

  compute_finetune:
    type: string
    optional: true
    default: serverless
    description: compute to be used for finetune eg. provide 'FT-Cluster' if your
      compute is named 'FT-Cluster'. Special characters like \ and ' are invalid in the parameter value.
      If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used

  train_dataloader_batch_size:
    type: integer
    min: 1
    default: 8
    optional: true
    description: Batch size for the training dataloader.

  validation_dataloader_batch_size:
    type: integer
    min: 1
    default: 1
    optional: true
    description: Batch size for the validation dataloader.

  train_dataloader_workers:
    type: integer
    min: 0
    default: 2
    optional: true
    description: Number of workers for the training dataloader.

  validation_dataloader_workers:
    type: integer
    min: 0
    default: 2
    optional: true
    description: Number of workers for the validation dataloader.

  hidden_dimensions:
    type: integer
    min: 1
    default: 512
    optional: true
    description: Number of hidden dimensions.

  input_channels:
    type: integer
    min: 1
    default: 1024
    optional: true
    description: Number of input channels.

  learning_rate:
    type: number
    default: 0.0003
    optional: true
    description: Learning rate for training.

  max_epochs:
    type: integer
    min: 1
    default: 10
    optional: true
    description: Maximum number of epochs for training.

  track_metric:
    type: string
    default: "acc"
    optional: true
    description: Metric to track when calculating best model. acc or auc supported.

  process_count_per_instance:
    type: integer
    min: 1
    default: 1
    optional: true
    description: Number of processes to run per instance. This is used to set the number of GPUs to use for training.

  instance_count:
    type: integer
    min: 1
    default: 1
    optional: true
    description: Number of instances to use for training.
outputs:
  save_dir:
    type: uri_folder
    description: Directory to save the model.
    mode: rw_mount
  
  mlflow_model_folder:
    type: mlflow_model
    description: Directory to save the MLflow model.
    mode: rw_mount
  
  merged_mlfow_model:
    type: mlflow_model
    description: Path to save the output model.
    mode: rw_mount


jobs:
  medical_image_embedding_model_finetune:
    type: command
    component: azureml://registries/azureml/components/medimgage_embedding_finetune/versions/0.0.1
    compute: '${{parent.inputs.compute_finetune}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_finetune}}'
      instance_count: ${{parent.inputs.instance_count}}
    distribution:
      type: mpi
      process_count_per_instance: ${{parent.inputs.process_count_per_instance}}
    inputs:
      mlflow_model_path: '${{parent.inputs.mlflow_embedding_model_path}}'
      eval_image_tsv: '${{parent.inputs.eval_image_tsv}}'
      eval_text_tsv: '${{parent.inputs.eval_text_tsv}}'
      image_tsv: '${{parent.inputs.image_tsv}}'
      text_tsv: '${{parent.inputs.text_tsv}}'
      label_file: '${{parent.inputs.label_file}}'
      conf_files: '${{parent.inputs.conf_files}}'
    outputs:
      save_dir: '${{parent.outputs.save_dir}}'
      mlflow_model_folder: '${{parent.outputs.mlflow_model_folder}}'
  medical_image_embedding_datapreprocessing_train:
    type: command
    component: azureml://registries/azureml/components/medical_image_embedding_datapreprocessing/versions/0.0.1
    compute: '${{parent.inputs.compute_preprocess}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_preprocess}}'
    inputs:
      mlflow_model_path: '${{parent.jobs.medical_image_embedding_model_finetune.outputs.mlflow_model_folder}}'
      image_tsv: '${{parent.inputs.image_tsv}}'
      image_standardization_jpeg_compression_ratio: '${{parent.inputs.image_standardization_jpeg_compression_ratio}}'
      image_standardization_image_size: '${{parent.inputs.image_standardization_image_size}}'
  medical_image_embedding_datapreprocessing_validation:
    type: command
    component: azureml://registries/azureml/components/medical_image_embedding_datapreprocessing/versions/0.0.1
    compute: '${{parent.inputs.compute_preprocess}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_preprocess}}'
    inputs:
      mlflow_model_path: '${{parent.jobs.medical_image_embedding_model_finetune.outputs.mlflow_model_folder}}'
      image_tsv: '${{parent.inputs.eval_image_tsv}}'  
      image_standardization_jpeg_compression_ratio: '${{parent.inputs.image_standardization_jpeg_compression_ratio}}'
      image_standardization_image_size: '${{parent.inputs.image_standardization_image_size}}'      
  medimgage_adapter_finetune:
    type: command
    component: azureml://registries/azureml/components/medimgage_adapter_finetune/versions/0.0.1
    compute: '${{parent.inputs.compute_finetune}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_finetune}}'
    inputs:
      train_data_path: '${{parent.jobs.medical_image_embedding_datapreprocessing_train.outputs.output_pkl}}'
      validation_data_path: '${{parent.jobs.medical_image_embedding_datapreprocessing_validation.outputs.output_pkl}}'
      train_text_tsv: '${{parent.inputs.text_tsv}}'
      validation_text_tsv: '${{parent.inputs.eval_text_tsv}}'
      train_dataloader_batch_size: '${{parent.inputs.train_dataloader_batch_size}}'
      validation_dataloader_batch_size: '${{parent.inputs.validation_dataloader_batch_size}}'
      train_dataloader_workers: '${{parent.inputs.train_dataloader_workers}}'
      validation_dataloader_workers: '${{parent.inputs.validation_dataloader_workers}}'
      label_file: '${{parent.inputs.label_file}}'
      hidden_dimensions: '${{parent.inputs.hidden_dimensions}}'
      input_channels: '${{parent.inputs.input_channels}}'
      learning_rate: '${{parent.inputs.learning_rate}}'
      max_epochs: '${{parent.inputs.max_epochs}}'
      track_metric: '${{parent.inputs.track_metric}}'
  medimage_embedding_adapter_merge:
    type: command
    component: azureml://registries/mablonde-registry-101/components/medimage_embedding_adapter_merge/versions/0.0.1
    compute: '${{parent.inputs.compute_finetune}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_finetune}}'
    inputs:
      adapter_model: '${{parent.jobs.medimgage_adapter_finetune.outputs.output_model_path}}'
      mlflow_model: '${{parent.jobs.medical_image_embedding_model_finetune.outputs.mlflow_model_folder}}'
      label_file: '${{parent.inputs.label_file}}'
      hidden_dimensions: '${{parent.inputs.hidden_dimensions}}'
      input_channels: '${{parent.inputs.input_channels}}'
    outputs:
      output_dir: '${{parent.outputs.merged_mlfow_model}}'