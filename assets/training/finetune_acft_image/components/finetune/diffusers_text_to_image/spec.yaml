$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
type: command

version: 0.0.9

name: diffusers_text_to_image_finetune
display_name: Text to Image Diffusers Model Finetune
description: Component to finetune stable diffusion models using diffusers for text to image.

is_deterministic: false

environment: azureml://registries/azureml/environments/acft-transformers-image-gpu/versions/46

code: ../../../src/finetune

distribution:
  type: pytorch

inputs:
  # # component input: model path
  model_path:
    type: uri_folder
    optional: false
    description: Output folder of model selector containing model metadata like config, checkpoints, tokenizer config.

  # component input: Instance data dir
  instance_data_dir:
    type: uri_folder
    optional: false
    description: A folder containing the training data of instance images.

  # optional component input: Class data dir
  class_data_dir:
    type: uri_folder
    optional: true
    mode: download
    description: A folder containing the training data of class images.

  task_name:
    type: string
    enum:
      - stable-diffusion-text-to-image
    description: Which task the model is solving.

  # Instance prompt
  instance_prompt:
    type: string
    optional: true
    description: The prompt with identifier specifying the instance.
  
  resolution:
    type: integer
    optional: true
    default: 512
    description: The image resolution for training.

  # Lora parameters
  # LoRA reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights. This vastly reduces the storage requirement for large language models adapted to specific tasks and enables efficient task-switching during deployment all without introducing inference latency. LoRA also outperforms several other adaptation methods including adapter, prefix-tuning, and fine-tuning. Currently, LoRA is supported for gpt2, bert, roberta, deberta, distilbert, t5, bart, mbart and camembert model families
  apply_lora:
    type: boolean
    default: true
    optional: false
    description: If "true" enables lora.

  lora_alpha:
    type: integer
    default: 128
    optional: true
    description: alpha attention parameter for lora.

  lora_r:
    type: integer
    default: 8
    optional: true
    description: lora dimension

  lora_dropout:
    type: number
    default: 0.0
    optional: true
    description: lora dropout value

  tokenizer_max_length:
    type: integer
    optional: true
    description: The maximum length of the tokenizer. If not set, will default to the tokenizer's max length.

  # Text Encoder
  text_encoder_type:
    type: string
    enum:
        - CLIPTextModel
        - T5EncoderModel
    optional: true
    description: Text encoder to be used.
  
  text_encoder_name:
    type: string
    optional: true
    description: Huggingface id of text encoder. This model should of type specified in `text_encoder_type`. If not specified the default from the model will be used.

  train_text_encoder:
    type: boolean
    default: false
    optional: true
    description: Whether to train the text encoder. If set, the text encoder should be float32 precision.

  pre_compute_text_embeddings:
    type: boolean
    default: true
    optional: true
    description: Whether or not to pre-compute text embeddings. If text embeddings are pre-computed, the text encoder will not be kept in memory during training and will leave more GPU memory available for training the rest of the model. This is not compatible with `--train_text_encoder`.

  text_encoder_use_attention_mask:
    type: boolean
    default: false
    optional: true
    description: Whether to use attention mask for the text encoder

  # UNET related
  class_labels_conditioning:
    type: string
    optional: true
    description: The optional `class_label` conditioning to pass to the unet, available values are `timesteps`.

  # Noise Scheduler
  noise_scheduler_name:
    type: string
    enum:
        - DPMSolverMultistepScheduler
        - DDPMScheduler
        - PNDMScheduler
    optional: true
    description: Noise scheduler to be used.

  noise_scheduler_num_train_timesteps:
    type: integer
    optional: true
    description: The number of diffusion steps to train the model. 

  noise_scheduler_variance_type:
    type: string
    enum:
        - fixed_small
        - fixed_small_log
        - fixed_large
        - fixed_large_log
        - learned
        - learned_range
    optional: true
    description: Clip the variance when adding noise to the denoised sample.

  noise_scheduler_prediction_type:
    type: string
    enum:
        - epsilon
        - sample
        - v_prediction
    optional: true
    description: Prediction type of the scheduler function; can be `epsilon` (predicts the noise of the diffusion process), `sample` (directly predicts the noisy sample`) or `v_prediction` (see section 2.4 of [Imagen Video](https://imagen.research.google/video/paper.pdf) paper).

  noise_scheduler_timestep_spacing:
    type: string
    optional: true
    description: The way the timesteps should be scaled. Refer to Table 2 of the [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) for more information.

  noise_scheduler_steps_offset:
    type: integer
    optional: true
    description: An offset added to the inference steps. You can use a combination of `offset=1` and `set_alpha_to_one=False` to make the last step use step 0 for the previous alpha product like in Stable Diffusion.

  extra_noise_scheduler_args:
    type: string
    optional: true
    description: Optional additional arguments that are supplied to noise scheduler. The arguments should be semi-colon separated key value pairs and should be enclosed in double quotes. For example, "clip_sample_range=1.0; clip_sample=True" for DDPMScheduler.

  # Offset Noise
  offset_noise: 
    type: boolean
    optional: true
    description: Fine-tuning against a modified noise. See https://www.crosslabs.org//blog/diffusion-with-offset-noise for more information.

  # Prior preservation loss
  with_prior_preservation:
      type: boolean
      default: true
      description: Flag to add prior preservation loss.
  class_prompt:
    type: string
    optional: true
    description: The prompt to specify images in the same class as provided instance images.
  num_class_images:
    type: integer
    default: 100
    optional: true
    description: Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.
  prior_generation_precision:
    type: string
    optional: true
    default: "fp32"
    enum:
      - "fp32"
      - "fp16"
      - "bf16"
    description: Choose prior generation precision between fp32, fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if a GPU is available else fp32.
  prior_loss_weight:
    type: number
    default: 1.0
    optional: true
    description: The weight of prior preservation loss.

  sample_batch_size:
    type: integer
    default: 4
    optional: true
    description: "Batch size (per device) for sampling class images when training with_prior_preservation set to True."

  num_validation_images:
    type: integer
    default: 0
    description: "Specify number of images to generate using instance_prompt. Images are stored in the output/checkpoint-* directories. Please note that this will increase the training time. If you select num_validation_images = 0, then run will generate 5 images in last checkpoint."

  number_of_workers:
    type: integer
    default: 6
    optional: true
    description: Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the main process.

  # Training parameters
  number_of_epochs:
    type: integer
    optional: true
    description: Number of training epochs. If left empty, will be chosen automatically based on the task type and model selected.

  max_steps:
    type: integer
    optional: true
    description: If set to a positive number, the total number of training steps to perform. Overrides 'number_of_epochs'. In case of using a finite iterable dataset the training may stop before reaching the set number of steps when all data is exhausted. If left empty, will be chosen automatically based on the task type and model selected.

  training_batch_size:
    type: integer
    default: 1
    optional: true
    description: Train batch size. If left empty, will be chosen automatically based on the task type and model selected.

  auto_find_batch_size:
    type: boolean
    default: false
    optional: true
    description: Flag to enable auto finding of batch size. If the provided 'per_device_train_batch_size' goes into Out Of Memory (OOM) enabling auto_find_batch_size will find the correct batch size by iteratively reducing 'per_device_train_batch_size' by a factor of 2 till the OOM is fixed.

  # learning rate and learning rate scheduler
  learning_rate:
    type: number
    optional: true
    description: Start learning rate. Defaults to linear scheduler. If left empty, will be chosen automatically based on the task type and model selected.

  learning_rate_scheduler:
    type: string
    optional: true
    enum:
      - warmup_linear
      - warmup_cosine
      - warmup_cosine_with_restarts
      - warmup_polynomial
      - constant
      - warmup_constant
    description: The scheduler type to use. If left empty, will be chosen automatically based on the task type and model selected.

  warmup_steps:
    type: integer
    default: 0
    optional: true
    description: Number of steps used for a linear warmup from 0 to learning_rate. If left empty, will be chosen automatically based on the task type and model selected.

  # optimizer
  optimizer:
    type: string
    optional: true
    enum:
      - adamw_hf
      - adamw
      # - adamw_torch_xla
      # - adamw_apex_fused
      # - adamw_bnb_8bit
      # - adamw_anyprecision
      - sgd
      - adafactor
      - adagrad
      - adamw_ort_fused
    description: optimizer to be used while training. 'adamw_ort_fused' optimizer is only supported for ORT training. If left empty, will be chosen automatically based on the task type and model selected.

  weight_decay:
    type: number
    default: 0
    optional: true
    description: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW and sgd optimizer. If left empty, will be chosen automatically based on the task type and model selected.

  extra_optim_args:
    type: string
    default: ""
    optional: true
    description: Optional additional arguments that are supplied to SGD Optimizer. The arguments should be semi-colon separated key value pairs and should be enclosed in double quotes. For example, "momentum=0.5; nesterov=True" for sgd. Please make sure to use a valid parameter names for the chosen optimizer. For exact parameter names, please refer https://pytorch.org/docs/1.13/generated/torch.optim.SGD.html#torch.optim.SGD for SGD. Parameters supplied in extra_optim_args will take precedence over the parameter supplied via other arguments such as weight_decay. If weight_decay is provided via "weight_decay" parameter and via extra_optim_args both, values specified in extra_optim_args will be used.


  # gradient accumulation
  gradient_accumulation_step:
    type: integer
    optional: true
    description: Number of update steps to accumulate the gradients for, before performing a backward/update pass. If left empty, will be chosen automatically based on the task type and model selected.

  # mixed precision training
  precision:
    type: string
    enum:
      - "32"
      - "16"
    default: "32"
    optional: true
    description: Apply mixed precision training. This can reduce memory footprint by performing operations in half-precision.

  # random seed
  random_seed:
    type: integer
    default: 42
    optional: true
    description: Random seed that will be set at the beginning of training.

  # logging strategy parameters
  logging_strategy:
    type: string
    default: epoch
    optional: true
    enum:
      - epoch
      - steps
    description: The logging strategy to adopt during training.

  logging_steps:
    type: integer
    default: 500
    optional: true
    description: Number of update steps between two logs if logging_strategy='steps'.

  # model checkpointing limit
  save_total_limit:
    type: integer
    default: 5
    optional: true
    description: If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir. If the value is -1 saves all checkpoints".

  # Grad Norm
  max_grad_norm:
    type: number
    optional: true
    description: Maximum gradient norm (for gradient clipping). If left empty, will be chosen automatically based on the task type and model selected.

  # save mlflow model
  save_as_mlflow_model:
    type: boolean
    default: true
    optional: true
    description: Save as mlflow model with pyfunc as flavour.

outputs:
  mlflow_model_folder:
    type: mlflow_model
    description: Output dir to save the finetune model as mlflow model.
  pytorch_model_folder:
    type: custom_model
    description: Output dir to save the finetune model as torch model.

command: >-

  python finetune.py
  --model_path ${{inputs.model_path}}
  --train_mltable_path ${{inputs.instance_data_dir}}
  $[[--class_data_dir ${{inputs.class_data_dir}}]]
  --task_name ${{inputs.task_name}}
  --apply_lora ${{inputs.apply_lora}}
  --num_validation_images ${{inputs.num_validation_images}}
  $[[--instance_prompt ${{inputs.instance_prompt}}]]
  $[[--tokenizer_max_length ${{inputs.tokenizer_max_length}}]]
  $[[--text_encoder_name ${{inputs.text_encoder_name}}]]
  $[[--text_encoder_type ${{inputs.text_encoder_type}}]]
  $[[--train_text_encoder ${{inputs.train_text_encoder}}]]
  $[[--pre_compute_text_embeddings ${{inputs.pre_compute_text_embeddings}}]]
  $[[--text_encoder_use_attention_mask ${{inputs.text_encoder_use_attention_mask}}]]
  $[[--class_labels_conditioning ${{inputs.class_labels_conditioning}}]]
  $[[--noise_scheduler_name ${{inputs.noise_scheduler_name}}]]
  $[[--noise_scheduler_num_train_timesteps ${{inputs.noise_scheduler_num_train_timesteps}}]]
  $[[--noise_scheduler_variance_type ${{inputs.noise_scheduler_variance_type}}]]
  $[[--noise_scheduler_prediction_type ${{inputs.noise_scheduler_prediction_type}}]]
  $[[--noise_scheduler_timestep_spacing ${{inputs.noise_scheduler_timestep_spacing}}]]
  $[[--noise_scheduler_steps_offset ${{inputs.noise_scheduler_steps_offset}}]]
  $[[--extra_noise_scheduler_args ${{inputs.extra_noise_scheduler_args}}]]
  $[[--offset_noise ${{inputs.offset_noise}}]]
  --with_prior_preservation ${{inputs.with_prior_preservation}}
  $[[--class_prompt ${{inputs.class_prompt}}]]
  $[[--num_class_images ${{inputs.num_class_images}}]]
  $[[--prior_generation_precision ${{inputs.prior_generation_precision}}]]
  $[[--prior_loss_weight ${{inputs.prior_loss_weight}}]]
  --apply_augmentations "true"
  $[[--dataloader_num_workers ${{inputs.number_of_workers}}]]
  $[[--sample_batch_size ${{inputs.sample_batch_size}}]]
  $[[--num_train_epochs ${{inputs.number_of_epochs}}]]
  $[[--max_steps ${{inputs.max_steps}}]]
  $[[--per_device_train_batch_size ${{inputs.training_batch_size}}]]
  $[[--auto_find_batch_size ${{inputs.auto_find_batch_size}}]]
  $[[--learning_rate ${{inputs.learning_rate}}]]
  $[[--lr_scheduler_type ${{inputs.learning_rate_scheduler}}]]
  $[[--warmup_steps ${{inputs.warmup_steps}}]]
  $[[--optim ${{inputs.optimizer}}]]
  $[[--weight_decay ${{inputs.weight_decay}}]]
  $[[--extra_optim_args ${{inputs.extra_optim_args}}]]
  $[[--gradient_accumulation_steps ${{inputs.gradient_accumulation_step}}]]
  $[[--precision ${{inputs.precision}}]]
  $[[--seed ${{inputs.random_seed}}]]
  $[[--logging_strategy ${{inputs.logging_strategy}}]]
  $[[--logging_steps ${{inputs.logging_steps}}]]
  $[[--save_total_limit ${{inputs.save_total_limit}}]]
  $$[[--max_grad_norm ${{inputs.max_grad_norm}}]]
  $[[--save_as_mlflow_model ${{inputs.save_as_mlflow_model}}]]
  --mlflow_model_folder ${{outputs.mlflow_model_folder}}
  --pytorch_model_folder ${{outputs.pytorch_model_folder}}
