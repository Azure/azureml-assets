$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
name: oss_text_generation_pipeline
version: 0.0.10
type: pipeline
display_name: OSS Text Generation Pipeline
description: FTaaS Pipeline component for text generation

inputs:
  # Compute parameters
  instance_type_data_import:
    type: string
    optional: true
    default: Singularity.D8_v3
    description: Instance type to be used for data_import component in case of virtual cluster compute, eg. Singularity.D8_v3. 
      The parameter compute_data_import must be set to 'virtual cluster' for instance_type to be used
  instance_type_finetune:
    type: string
    optional: true
    default: Singularity.ND40_v2
    description: Instance type to be used for finetune component in case of virtual cluster compute, eg. Singularity.ND40_v2. 
      The parameter compute_finetune must be set to 'virtual cluster' for instance_type to be used
  number_of_gpu_to_use_finetuning:
    type: integer
    default: 1
    optional: true
    description: >-
      number of gpus to be used per node for finetuning, should be equal
      to number of gpu per node in the compute SKU used for finetune

  # Continual-Finetuning model path
  mlflow_model_path:
    type: mlflow_model
    optional: false
    description: MLflow model asset path. Special characters like \ and ' are invalid in the parameter value.
    mode: download

  # Preprocessing parameters
  # TODO remove text key if the format is made similar to openai
  text_key:
    type: string
    optional: false
    description: >-
      key for text in an example. format your data keeping in mind that
      text is concatenated with ground_truth while finetuning in the form - text + groundtruth.
      for eg. "text"="knock knock\n", "ground_truth"="who's there"; will be treated as "knock knock\nwho's there"

  ground_truth_key:
    type: string
    optional: true
    description: >-
      key for ground_truth in an example. 
      we take separate column for ground_truth to enable use cases like summarization, translation, 
      question_answering, etc. which can be repurposed in form of text-generation where both text and ground_truth are needed.
      This separation is useful for calculating metrics.
      for eg. "text"="Summarize this dialog:\n{input_dialogue}\nSummary:\n", "ground_truth"="{summary of the dialogue}"

  # Dataset path Parameters
  train_file_path:
    type: uri_file
    optional: false
    description: Path to the registered training data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  validation_file_path:
    type: uri_file
    optional: true
    description: Path to the registered validation data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  # Finetuning parameters
  # Training parameters
  num_train_epochs:
    type: integer
    default: 1
    optional: true
    description: training epochs

  per_device_train_batch_size:
    type: integer
    default: 1
    optional: true
    description: Train batch size

  learning_rate:
    type: number
    default: 3e-04
    optional: true
    description: Start learning rate.

  # Validation parameters
  system_properties:
    type: string
    optional: true
    description: Validation parameters propagated from pipeline.

  # Compute parameters
  compute_data_import:
    type: string
    optional: true
    default: 'virtual cluster'
    description: >-
      compute to be used for model_import eg. provide 'FT-Cluster' if
      your compute is named 'FT-Cluster'. Special characters like \ and ' are invalid in the parameter value.
      If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used
  compute_finetune:
    type: string
    optional: true
    default: 'virtual cluster'
    description: >-
      compute to be used for finetune eg. provide 'FT-Cluster' if your
      compute is named 'FT-Cluster'. Special characters like \ and ' are invalid in the parameter value.
      If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used

  # Model registration
  registered_model_name:
    type: string
    optional: true
    description: Name of the registered model

outputs:
  output_model:
    type: uri_folder
    description: Output dir to save the finetuned lora weights
    mode: rw_mount

jobs:
  oss_text_generation_data_import:
    type: command
    component: azureml:oss_text_generation_data_import:0.0.10
    compute: '${{parent.inputs.compute_data_import}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_data_import}}'
      properties:
        singularity:
          imageVersion: ''
          SLATier: 'Premium'
          priority: 'Medium'
    environment_variables:
      _AZUREML_CR_ENABLE_ITP_CAP: "false"
    inputs:
      train_file_path: '${{parent.inputs.train_file_path}}'
      validation_file_path: '${{parent.inputs.validation_file_path}}'
      system_properties: '${{parent.inputs.system_properties}}'
      user_column_names: '${{parent.inputs.text_key}},${{parent.inputs.ground_truth_key}}'
  oss_text_generation_finetune:
    type: command
    component: azureml:oss_text_generation_finetune:0.0.10
    compute: '${{parent.inputs.compute_finetune}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_finetune}}'
      properties:
        singularity:
          imageVersion: ''
          SLATier: 'Premium'
          priority: 'Medium'
    environment_variables:
      _AZUREML_CR_ENABLE_ITP_CAP: "false"
    inputs:
      mlflow_model_path: '${{parent.inputs.mlflow_model_path}}'
      dataset_input: '${{parent.jobs.oss_text_generation_data_import.outputs.output_dataset}}'
      text_key: '${{parent.inputs.text_key}}'
      ground_truth_key: '${{parent.inputs.ground_truth_key}}'
      batch_size: 1000
      pad_to_max_length: "false"
      max_seq_length: 4096
      number_of_gpu_to_use_finetuning: '${{parent.inputs.number_of_gpu_to_use_finetuning}}'
      apply_lora: "true"
      lora_alpha: 128
      lora_r: 8
      lora_dropout: 0
      num_train_epochs: '${{parent.inputs.num_train_epochs}}'
      max_steps: -1
      per_device_train_batch_size: '${{parent.inputs.per_device_train_batch_size}}'
      per_device_eval_batch_size: '${{parent.inputs.per_device_train_batch_size}}'
      auto_find_batch_size: "false"
      optim: adamw_hf
      learning_rate: '${{parent.inputs.learning_rate}}'
      warmup_steps: 0
      weight_decay: 0.1
      adam_beta1: 0.9
      adam_beta2: 0.95
      adam_epsilon: 1e-05
      gradient_accumulation_steps: 1
      eval_accumulation_steps: 1
      lr_scheduler_type: cosine
      precision: 16
      seed: 42
      enable_full_determinism: "false"
      dataloader_num_workers: 0
      ignore_mismatched_sizes: "false"
      max_grad_norm: 1.0
      evaluation_strategy: epoch
      evaluation_steps_interval: 0.0
      eval_steps: 500
      logging_strategy: epoch
      logging_steps: 500
      metric_for_best_model: loss
      resume_from_checkpoint: "true"
      save_strategy: steps
      save_steps: 100
      save_total_limit: 1
      apply_early_stopping: "false"
      early_stopping_patience: 0
      apply_deepspeed: "true"
      deepspeed_stage: 3
      apply_ort: "false"
      system_properties: '${{parent.inputs.system_properties}}'
      registered_model_name: '${{parent.inputs.registered_model_name}}'
    outputs:
      output_model: '${{parent.outputs.output_model}}'
