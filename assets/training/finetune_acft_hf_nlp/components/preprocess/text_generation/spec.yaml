$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: text_generation_datapreprocess
version: 0.0.19
type: command

is_deterministic: True

display_name: Text Generation DataPreProcess
description: Component to preprocess data for text generation task

environment: azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/23

code: ../../../src/preprocess

inputs:
  # Text Generation task arguments
  text_key:
    type: string
    optional: false
    description: key for text in an example. format your data keeping in mind that
      text is concatenated with ground_truth while finetuning in the form - text + groundtruth.
      for eg. "text"="knock knock\n", "ground_truth"="who's there"; will be treated as "knock knock\nwho's there"

  ground_truth_key:
    type: string
    optional: false
    description: key for ground_truth in an example. 
      we take separate column for ground_truth to enable use cases like summarization, translation, 
      question_answering, etc. which can be repurposed in form of text-generation where both text and ground_truth are needed.
      This separation is useful for calculating metrics.
      for eg. "text"="Summarize this dialog:\n{input_dialogue}\nSummary:\n", "ground_truth"="{summary of the dialogue}"

  batch_size:
    type: integer
    optional: true
    default: 1000
    description: Number of examples to batch before calling the tokenization function

  # Tokenization params
  pad_to_max_length:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: If set to True, the returned sequences will be padded according to the model's padding side and padding index, up to their `max_seq_length`. If no `max_seq_length` is specified, the padding is done up to the model's max length.
  
  max_seq_length:
    type: integer
    optional: true
    default: -1
    description: Default is -1 which means the padding is done up to the model's max length. Else will be padded to `max_seq_length`.
  # Inputs
  train_file_path:
    type: uri_file
    optional: true
    description: Enter the train file path

  validation_file_path:
    type: uri_file
    optional: true
    description: Enter the validation file path

  test_file_path:
    type: uri_file
    optional: true
    description: Enter the test file path

  train_mltable_path:
    type: mltable
    optional: true
    description: Enter the train mltable path

  validation_mltable_path:
    type: mltable
    optional: true
    description: Enter the validation mltable path

  test_mltable_path:
    type: mltable
    optional: true
    description: Enter the test mltable path

  # Dataset parameters
  model_selector_output:
    type: uri_folder
    optional: false
    description: output folder of model selector containing model metadata like config, checkpoints, tokenizer config

outputs:
  output_dir:
    type: uri_folder
    description: folder to store preprocessed outputs of input data

command: >-
  python preprocess.py --task_name TextGeneration --text_key '${{inputs.text_key}}' --ground_truth_key '${{inputs.ground_truth_key}}' $[[--batch_size '${{inputs.batch_size}}']] $[[--pad_to_max_length '${{inputs.pad_to_max_length}}']] $[[--max_seq_length '${{inputs.max_seq_length}}']] $[[--train_file_path '${{inputs.train_file_path}}']] $[[--validation_file_path '${{inputs.validation_file_path}}']] $[[--test_file_path '${{inputs.test_file_path}}']] $[[--train_mltable_path '${{inputs.train_mltable_path}}']] $[[--validation_mltable_path '${{inputs.validation_mltable_path}}']] $[[--test_mltable_path '${{inputs.test_mltable_path}}']] --model_selector_output '${{inputs.model_selector_output}}' --output_dir '${{outputs.output_dir}}'

