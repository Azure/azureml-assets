$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: question_answering_datapreprocess
version: 0.0.19
type: command

is_deterministic: True

display_name: Question Answering DataPreProcess
description: Component to preprocess data for question answering task. See [docs](https://aka.ms/azureml/components/question_answering_datapreprocess) to learn more.

environment: azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/23

code: ../../../src/preprocess

inputs:
  # Task arguments
  # Sample example
  # {`question_column`: "In what year did Paul VI formally appoint Mary as mother of the Catholic church?", `context_column`: "Paul VI opened the third period on 14 September 1964, telling the Council Fathers that he viewed the text about the Church as the most important document to come out from the Council. As the Council discussed the role of bishops in the papacy, Paul VI issued an explanatory note confirming the primacy of the papacy, a step which was viewed by some as meddling in the affairs of the Council American bishops pushed for a speedy resolution on religious freedom, but Paul VI insisted this to be approved together with related texts such as ecumenism. The Pope concluded the session on 21 November 1964, with the formal pronouncement of Mary as Mother of the Church.", `answers_column`: {`answer_start_column`: [595], `text_column`: ['1964']}}
# If the dataset follows above pattern, `question_key`: "question_column"; `context_key`: "context_column"; `answers_key`: answers_column; `answer_start_key`: answer_start_column; `answer_text_key`: text_column

  question_key:
    type: string
    optional: false
    description: The question whose answer needs to be extracted from the provided context 

  context_key:
    type: string
    optional: false
    description: The context that contains the answer to the question

  answers_key:
    type: string
    optional: false
    description: "The value of this field is text in JSON format with two nested keys: answer_start_key and answer_text_key with their corresponding values"

  answer_start_key:
    type: string
    optional: false
    description: Refers to the position where the answer beings in context. Needs a value that maps to a nested key in the values of the answers_key parameter

  answer_text_key:
    type: string
    optional: false
    description: Contains the answer to the question. Needs a value that maps to a nested key in the values of the answers_key parameter

  doc_stride:
    type: integer
    default: 128
    optional: true
    description: The amount of context overlap to keep in case the number of tokens per example exceed __max_seq_length__

  n_best_size:
    type: integer
    default: 20
    optional: true
    description: The `top_n` max probable start tokens and end tokens to be consider while generating possible answers.

  max_answer_length_in_tokens:
    type: integer
    default: 30
    optional: true
    description: The maximum allowed answer length specified in token length. The default value for this parameter is 30. All the answers with above 30 tokens will not be considered as a possible answer.

  batch_size:
    type: integer
    optional: true
    min: 1
    default: 1000
    description: Number of examples to batch before calling the tokenization function

  # Tokenization params
  pad_to_max_length:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: If set to True, the returned sequences will be padded according to the model's padding side and padding index, up to their `max_seq_length`. If no `max_seq_length` is specified, the padding is done up to the model's max length.
  
  max_seq_length:
    type: integer
    optional: true
    default: -1
    description: Controls the maximum length to use when pad_to_max_length parameter is set to `true`. Default is -1 which means the padding is done up to the model's max length. Else will be padded to `max_seq_length`.

  # Data inputs
  # Please note that either `train_file_path` or `train_mltable_path` needs to be passed. In case both are passed, `mltable path` will take precedence. The validation and test paths are optional and an automatic split from train data happens if they are not passed.
  # If both validation and test files are missing, 10% of train data will be assigned to each of them and the remaining 80% will be used for training
  # If anyone of the file is missing, 20% of the train data will be assigned to it and the remaining 80% will be used for training
  train_file_path:
    type: uri_file
    optional: true
    description: Path to the registered training data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`.

  validation_file_path:
    type: uri_file
    optional: true
    description: Path to the registered validation data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`.

  test_file_path:
    type: uri_file
    optional: true
    description: Path to the registered test data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`.

  train_mltable_path:
    type: mltable
    optional: true
    description: Path to the registered training data asset in `mltable` format.

  validation_mltable_path:
    type: mltable
    optional: true
    description: Path to the registered validation data asset in `mltable` format.

  test_mltable_path:
    type: mltable
    optional: true
    description: Path to the registered test data asset in `mltable` format.

  # Model input
  model_selector_output:
    type: uri_folder
    optional: false
    description: output folder of model selector containing model metadata like config, checkpoints, tokenizer config

outputs:
  output_dir:
    type: uri_folder
    description: The folder contains the tokenized output of the train, validation and test data along with the tokenizer files used to tokenize the data

command: >-
  python preprocess.py --task_name QuestionAnswering --question_key '${{inputs.question_key}}' --context_key '${{inputs.context_key}}' --answers_key '${{inputs.answers_key}}' --answer_start_key '${{inputs.answer_start_key}}' --answer_text_key '${{inputs.answer_text_key}}' $[[--doc_stride '${{inputs.doc_stride}}']] $[[--n_best_size '${{inputs.n_best_size}}']] $[[--max_answer_length_in_tokens '${{inputs.max_answer_length_in_tokens}}']] $[[--batch_size '${{inputs.batch_size}}']] $[[--pad_to_max_length '${{inputs.pad_to_max_length}}']] $[[--max_seq_length '${{inputs.max_seq_length}}']] $[[--train_file_path '${{inputs.train_file_path}}']] $[[--validation_file_path '${{inputs.validation_file_path}}']] $[[--test_file_path '${{inputs.test_file_path}}']] $[[--train_mltable_path '${{inputs.train_mltable_path}}']] $[[--validation_mltable_path '${{inputs.validation_mltable_path}}']] $[[--test_mltable_path '${{inputs.test_mltable_path}}']] --model_selector_output '${{inputs.model_selector_output}}' --output_dir '${{outputs.output_dir}}'
