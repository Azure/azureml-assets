$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: text_generation_finetune
version: 0.0.78
type: command

is_deterministic: true

display_name: Text Generation Finetune
description: Component to finetune model for Text Generation task

environment: azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/105

code: ../../../src/finetune

distribution:
  type: pytorch

inputs:
  # Lora parameters
  apply_lora:
    type: string
    enum:
    - "true"
    - "false"
    default: "false"
    optional: true
    description: lora enabled

  merge_lora_weights:
    type: string
    enum:
    - "true"
    - "false"
    default: "true"
    optional: true
    description: if set to true, the lora trained weights will be merged to base model before saving

  lora_alpha:
    type: integer
    default: 128
    optional: true
    description: lora attention alpha

  lora_r:
    type: integer
    default: 8
    optional: true
    description: lora dimension

  lora_dropout:
    type: number
    default: 0.0
    optional: true
    description: lora dropout value

  # Training parameters
  num_train_epochs:
    type: integer
    default: 1
    optional: true
    description: training epochs

  max_steps:
    type: integer
    default: -1
    optional: true
    description: If set to a positive number, the total number of training steps to perform. Overrides 'epochs'. In case of using a finite iterable dataset the training may stop before reaching the set number of steps when all data is exhausted.

  per_device_train_batch_size:
    type: integer
    default: 1
    optional: true
    description: Train batch size

  per_device_eval_batch_size:
    type: integer
    default: 1
    optional: true
    description: Validation batch size

  auto_find_batch_size:
    type: string
    enum:
    - "true"
    - "false"
    default: "false"
    optional: true
    description: Flag to enable auto finding of batch size. If the provided 'per_device_train_batch_size' goes into Out Of Memory (OOM) enabling auto_find_batch_size will find the correct batch size by iteratively reducing 'per_device_train_batch_size' by a factor of 2 till the OOM is fixed

  optim:
    type: string
    default: adamw_torch
    optional: true
    enum:
    - adamw_torch      # - adamw_apex_fused
    - adafactor
    description: Optimizer to be used while training

  learning_rate:
    type: number
    default: 0.00002
    optional: true
    description: Start learning rate. Defaults to linear scheduler.

  warmup_steps:
    type: integer
    default: 0
    optional: true
    description: Number of steps used for a linear warmup from 0 to learning_rate

  weight_decay:
    type: number
    default: 0.0
    optional: true
    description: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer

  adam_beta1:
    type: number
    default: 0.9
    optional: true
    description: The beta1 hyperparameter for the AdamW optimizer

  adam_beta2:
    type: number
    default: 0.999
    optional: true
    description: The beta2 hyperparameter for the AdamW optimizer

  adam_epsilon:
    type: number
    default: 1e-8
    optional: true
    description: The epsilon hyperparameter for the AdamW optimizer

  gradient_accumulation_steps:
    type: integer
    default: 1
    optional: true
    description: Number of updates steps to accumulate the gradients for, before performing a backward/update pass

  eval_accumulation_steps:
    type: integer
    default: -1
    optional: true
    description: Number of predictions steps to accumulate before moving the tensors to the CPU, will be passed as None if set to -1

  lr_scheduler_type:
    type: string
    default: linear
    optional: true
    enum:
    - linear
    - cosine
    - cosine_with_restarts
    - polynomial
    - constant
    - constant_with_warmup
    description: learning rate scheduler to use.

  precision:
    type: string
    enum:
    - "32"
    - "16"
    default: "32"
    optional: true
    description: Apply mixed precision training. This can reduce memory footprint by performing operations in half-precision.

  seed:
    type: integer
    default: 42
    optional: true
    description: Random seed that will be set at the beginning of training

  enable_full_determinism:
    type: string
    enum:
    - "true"
    - "false"
    default: "false"
    optional: true
    description: Ensure reproducible behavior during distributed training

  dataloader_num_workers:
    type: integer
    default: 0
    optional: true
    description: Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.

  ignore_mismatched_sizes:
    type: string
    enum:
    - "true"
    - "false"
    default: "true"
    optional: true
    description: Whether or not to raise an error if some of the weights from the checkpoint do not have the same size as the weights of the model

  max_grad_norm:
    type: number
    default: 1.0
    optional: true
    description: "Maximum gradient norm (for gradient clipping)"

  evaluation_strategy:
    type: string
    default: epoch
    optional: true
    enum:
    - epoch
    - steps
    description: The evaluation strategy to adopt during training

  evaluation_steps_interval:
    type: number
    default: 0.0
    optional: true
    description: The evaluation steps in fraction of an epoch steps to adopt during training. Overwrites evaluation_steps if not 0.

  eval_steps:
    type: integer
    default: 500
    optional: true
    description: Number of update steps between two evals if evaluation_strategy='steps'

  logging_strategy:
    type: string
    default: steps
    optional: true
    enum:
    - epoch
    - steps
    description: The logging strategy to adopt during training.

  logging_steps:
    type: integer
    default: 10
    optional: true
    description: Number of update steps between two logs if logging_strategy='steps'

  metric_for_best_model:
    type: string
    default: loss
    optional: true
    enum:
    - loss
    description: Specify the metric to use to compare two different models

  resume_from_checkpoint:
    type: string
    default: "false"
    optional: true
    enum:
    - "true"
    - "false"
    description: Loads Optimizer, Scheduler and Trainer state for finetuning if true

  save_total_limit:
    type: integer
    default: -1
    optional: true
    description: If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir. If the value is -1 saves all checkpoints"

  # Early Stopping Parameters
  apply_early_stopping:
    type: string
    default: "false"
    optional: true
    enum:
    - "true"
    - "false"
    description: Enable early stopping

  early_stopping_patience:
    type: integer
    default: 1
    optional: true
    description: Stop training when the specified metric worsens for early_stopping_patience evaluation calls

  early_stopping_threshold:
    type: number
    default: 0.0
    optional: true
    description: Denotes how much the specified metric must improve to satisfy early stopping conditions

  # Deepspeed Parameters
  apply_deepspeed:
    type: string
    enum:
    - "true"
    - "false"
    default: "false"
    optional: true
    description: If set to true, will enable deepspeed for training

  deepspeed:
    type: uri_file
    optional: true
    description: Deepspeed config to be used for finetuning
    mode: rw_mount

  deepspeed_stage:
    type: string
    optional: true
    default: "2"
    enum:
    - "2"
    - "3"
    description: This parameter configures which DEFAULT deepspeed config to be used - stage2 or stage3. The default choice is stage2. Note that, this parameter is ONLY applicable when user doesn't pass any config information via deepspeed port.

  # ORT Parameters
  apply_ort:
    type: string
    enum:
    - "true"
    - "false"
    default: "false"
    optional: true
    description: If set to true, will use the ONNXRunTime training

  # Dataset parameters
  preprocess_output:
    type: uri_folder
    optional: false
    description: output folder of preprocessor containing encoded train.jsonl valid.jsonl and the model pretrained info
    mode: rw_mount

  model_selector_output:
    type: uri_folder
    optional: false
    description: output folder of model selector containing model metadata like config, checkpoints, tokenizer config
    mode: rw_mount

  # Validation parameters
  system_properties:
    type: string
    optional: true
    description: Validation parameters propagated from pipeline.

outputs:
  pytorch_model_folder:
    type: uri_folder
    description: Output dir to save the finetune model and other metadata
    mode: rw_mount

command: >-
  python finetune.py $[[--apply_lora '${{inputs.apply_lora}}']] $[[--merge_lora_weights '${{inputs.merge_lora_weights}}']] $[[--lora_alpha '${{inputs.lora_alpha}}']] $[[--lora_r '${{inputs.lora_r}}']] $[[--lora_dropout '${{inputs.lora_dropout}}']] $[[--num_train_epochs '${{inputs.num_train_epochs}}']] $[[--max_steps '${{inputs.max_steps}}']] $[[--per_device_train_batch_size '${{inputs.per_device_train_batch_size}}']] $[[--per_device_eval_batch_size '${{inputs.per_device_eval_batch_size}}']] $[[--auto_find_batch_size '${{inputs.auto_find_batch_size}}']] $[[--optim '${{inputs.optim}}']] $[[--learning_rate '${{inputs.learning_rate}}']] $[[--warmup_steps '${{inputs.warmup_steps}}']] $[[--weight_decay '${{inputs.weight_decay}}']] $[[--adam_beta1 '${{inputs.adam_beta1}}']] $[[--adam_beta2 '${{inputs.adam_beta2}}']] $[[--adam_epsilon '${{inputs.adam_epsilon}}']] $[[--gradient_accumulation_steps '${{inputs.gradient_accumulation_steps}}']] $[[--eval_accumulation_steps '${{inputs.eval_accumulation_steps}}']] $[[--lr_scheduler_type '${{inputs.lr_scheduler_type}}']] $[[--precision '${{inputs.precision}}']] $[[--seed '${{inputs.seed}}']] $[[--enable_full_determinism '${{inputs.enable_full_determinism}}']] $[[--dataloader_num_workers '${{inputs.dataloader_num_workers}}']] $[[--ignore_mismatched_sizes '${{inputs.ignore_mismatched_sizes}}']] $[[--max_grad_norm '${{inputs.max_grad_norm}}']] $[[--evaluation_strategy '${{inputs.evaluation_strategy}}']] $[[--evaluation_steps_interval '${{inputs.evaluation_steps_interval}}']] $[[--eval_steps '${{inputs.eval_steps}}']] $[[--logging_strategy '${{inputs.logging_strategy}}']] $[[--logging_steps '${{inputs.logging_steps}}']] $[[--metric_for_best_model '${{inputs.metric_for_best_model}}']] $[[--resume_from_checkpoint '${{inputs.resume_from_checkpoint}}']] $[[--save_total_limit '${{inputs.save_total_limit}}']] $[[--apply_early_stopping '${{inputs.apply_early_stopping}}']] $[[--early_stopping_patience '${{inputs.early_stopping_patience}}']] $[[--early_stopping_threshold '${{inputs.early_stopping_threshold}}']] $[[--apply_ort '${{inputs.apply_ort}}']] $[[--apply_deepspeed '${{inputs.apply_deepspeed}}']] $[[--deepspeed '${{inputs.deepspeed}}']] $[[--deepspeed_stage '${{inputs.deepspeed_stage}}']] --model_selector_output '${{inputs.model_selector_output}}' --preprocess_output '${{inputs.preprocess_output}}' $[[--system_properties '${{inputs.system_properties}}']] --pytorch_model_folder '${{outputs.pytorch_model_folder}}'

