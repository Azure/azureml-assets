$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: model_evaluation
version: 0.0.1
type: command
display_name: Component Model Evaluation
description: Component for model response evaluation across multiple models.

environment: azureml://registries/azureml/environments/acft-rft-training/versions/1

code: ../../src/llm_evaluation/

distribution:
  type: pytorch

inputs:
  checkpoint_base_path_1:
    type: uri_folder
    description: "Base path containing all model checkpoints or LoRA adapters (optional if using only hf_model_id)"
    mode: rw_mount
    optional: true

  checkpoint_base_path_2:
    type: uri_folder
    description: "Second base path containing model checkpoints or LoRA adapters (optional, for comparing models from different training runs)"
    mode: rw_mount
    optional: true

  base_path_1_label:
    type: string
    description: "Label to use as prefix in metrics for checkpoint_base_path_1 (e.g., 'experiment_a'). Defaults to 'base_path_1'"
    default: "base_path_1"
    optional: true

  base_path_2_label:
    type: string
    description: "Label to use as prefix in metrics for checkpoint_base_path_2 (e.g., 'experiment_b'). Defaults to 'base_path_2'"
    default: "base_path_2"
    optional: true

  evaluate_base_model:
    type: boolean
    description: "If true, also evaluate the base model after evaluating model checkpoints."
    default: false
    optional: true

  explore_pattern_1:
    type: string
    description: "Pattern to explore for checkpoint paths (e.g., global_step_{checkpoint}/actor/huggingface/). Only used with checkpoint_base_path_1."
    default: "global_step_{checkpoint}/actor/huggingface/"
    optional: true

  explore_pattern_2:
    type: string
    description: "Pattern to explore for checkpoint paths in checkpoint_base_path_2\
                  (e.g., global_step_{checkpoint}/actor/huggingface/). Only used with checkpoint_base_path_2."
    default: "global_step_{checkpoint}/actor/huggingface/"
    optional: true

  checkpoint_values_1:
    type: string
    description: "Comma-separated list of model checkpoint values to evaluate (e.g., '100,129,20'). Optional if using only hf_model_id."
    optional: true

  checkpoint_values_2:
    type: string
    description: "Comma-separated list of model checkpoint values to evaluate from checkpoint_base_path_2 (e.g., '100,129,20'). Only used with checkpoint_base_path_2."
    optional: true

  use_lora_adapters_1:
    type: boolean
    description: "If true, model checkpoints from checkpoint_base_path_1 will be treated as LoRA adapters to be loaded with base model\
                  Base model must be specified via base_model_path or hf_model_id."
    default: false
    optional: true

  use_lora_adapters_2:
    type: boolean
    description: "If true, model checkpoints from checkpoint_base_path_2 will be treated as LoRA adapters to be loaded with base model\
                  Base model must be specified via base_model_path or hf_model_id."
    default: false
    optional: true

  base_model_path:
    type: uri_folder
    description: "Local base model path (used when use_lora_adapters is true). Mutually exclusive with hf_model_id.
                  This takes higher precedence over hf_model_id when both are provided."
    optional: true
    mode: ro_mount

  hf_model_id:
    type: string
    description: "Hugging Face model ID (e.g., 'microsoft/Phi-4-reasoning'). Can be used alone for direct evaluation or with use_lora_adapters for base model.\
                  Mutually exclusive with base_model_path when use_lora_adapters is true."
    optional: true

  validation_file:
    type: uri_file
    description: "Path to validation JSONL file for evaluation."

  max_prompt_length:
    type: integer
    default: 2048
    optional: true
    description: "Maximum length for input prompts in tokens. Prompts exceeding this length will be truncated."

  max_response_length:
    type: integer
    default: 1024
    optional: true
    description: "Maximum length for model-generated responses in tokens. Controls the maximum number of new tokens to generate."

  batch_size:
    type: integer
    default: 16
    optional: true
    description: "Number of sequences to process in parallel during inference. Larger batch sizes improve GPU utilization but require more memory."

  temperature:
    type: number
    default: 0.7
    optional: true
    description: "Sampling temperature for text generation. Higher values (e.g., 1.0) increase randomness, lower values (e.g., 0.1) make outputs more deterministic."

  top_p:
    type: number
    default: 0.9
    optional: true
    description: "Top-p (nucleus) sampling parameter. Tokens with cumulative probability up to top_p are considered. Lower values make outputs more focused."

  tensor_parallel_size:
    type: integer
    default: 1
    optional: true
    description: "Number of GPUs for tensor parallelism (vLLM). Splits model weights across GPUs to enable inference of large models. Should not exceed available GPUs."

  gpu_memory_utilization:
    type: number
    default: 0.8
    optional: true
    description: "Fraction of GPU memory (0.0-1.0) to use for vLLM inference. Controls memory reserved for model weights and KV cache."

  dtype:
    type: string
    enum:
    - "float16"
    - "bfloat16"
    - "float32"
    default: "bfloat16"
    optional: true
    description: "Data type for model weights and computations in vLLM. bfloat16 recommended for modern GPUs (A100, H100). float16 uses less memory, float32 highest precision."

  extraction_method:
    type: string
    enum:
    - "strict"
    - "flexible"
    default: "strict"
    optional: true
    description: "Method for extracting model responses from generated text. 'strict' requires exact format matching, 'flexible' uses more lenient parsing"

  n_gpus_per_node:
    type: integer
    default: 1
    optional: true
    description: "Number of GPUs available per compute node. Used for distributed training/inference configuration"

  number_of_trials:
    type: integer
    default: 1
    optional: true
    description: "Number of evaluation trials per checkpoint"

outputs:
  evaluation_results:
    type: uri_folder
    description: "Directory containing all checkpoint evaluation results"

  intermediate_folder:
    type: uri_folder
    description: "Directory for intermediate files (reserved for future use)"

command: >-
  python model_evaluation.py
  $[[--checkpoint_base_path_1 '${{inputs.checkpoint_base_path_1}}']]
  $[[--checkpoint_base_path_2 '${{inputs.checkpoint_base_path_2}}']]
  $[[--base_path_1_label '${{inputs.base_path_1_label}}']]
  $[[--base_path_2_label '${{inputs.base_path_2_label}}']]
  $[[--evaluate_base_model '${{inputs.evaluate_base_model}}']]
  $[[--explore_pattern_1 '${{inputs.explore_pattern_1}}']]
  $[[--explore_pattern_2 '${{inputs.explore_pattern_2}}']]
  $[[--checkpoint_values_1 '${{inputs.checkpoint_values_1}}']]
  $[[--checkpoint_values_2 '${{inputs.checkpoint_values_2}}']]
  --validation_file '${{inputs.validation_file}}'
  $[[--use_lora_adapters_1 '${{inputs.use_lora_adapters_1}}']]
  $[[--use_lora_adapters_2 '${{inputs.use_lora_adapters_2}}']]
  $[[--base_model_path '${{inputs.base_model_path}}']]
  $[[--hf_model_id '${{inputs.hf_model_id}}']]
  $[[--max_prompt_length '${{inputs.max_prompt_length}}']]
  $[[--max_response_length '${{inputs.max_response_length}}']]
  $[[--batch_size '${{inputs.batch_size}}']]
  $[[--temperature '${{inputs.temperature}}']]
  $[[--top_p '${{inputs.top_p}}']]
  $[[--tensor_parallel_size '${{inputs.tensor_parallel_size}}']]
  $[[--gpu_memory_utilization '${{inputs.gpu_memory_utilization}}']]
  $[[--dtype '${{inputs.dtype}}']]
  $[[--extraction_method '${{inputs.extraction_method}}']]
  $[[--n_gpus_per_node '${{inputs.n_gpus_per_node}}']]
  $[[--number_of_trials '${{inputs.number_of_trials}}']]
  --output_dir '${{outputs.evaluation_results}}'
  --intermediate_dir '${{outputs.intermediate_folder}}'
