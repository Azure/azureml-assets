$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
name: verl_finetune_pipeline
version: 0.0.26
type: pipeline
display_name: Verl Finetune Pipeline
description: Pipeline component for fine-tuning models using Verl Package

inputs:
  # Infrastructure parameters
  instance_type_model_import:
    type: string
    optional: true
    default: Standard_d12_v2
    description: Instance type to be used for model_import component in case of serverless compute, eg. standard_d12_v2. The parameter compute_model_import must be set to 'serverless' for instance_type to be used
  instance_type_finetune:
    type: string
    optional: true
    default: Standard_ND96isr_H100_v5
    description: Instance type to be used for finetune component in case of serverless compute, eg. standard_nc24rs_v3. The parameter compute_finetune must be set to 'serverless' for instance_type to be used
  shm_size_finetune:
    type: string
    optional: true
    default: 5g
    description: Shared memory size to be used for finetune component. It is useful while using Nebula (via DeepSpeed) which uses shared memory to save model and optimizer states.
  num_nodes_finetune:
    type: integer
    min: 1
    default: 1
    optional: true
    description: number of nodes to be used for finetuning (used for distributed training)
  number_of_gpu_to_use_finetuning:
    type: integer
    min: 1
    default: 1
    optional: true
    description: number of gpus to be used per node for finetuning, should be equal to number of gpu per node in the compute SKU used for finetune

  # Model Import parameters
  huggingface_id:
    type: string
    description: The string can be any valid Hugging Face id from the [Hugging Face models webpage](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads). Models from Hugging Face are subject to third party license terms available on the Hugging Face model details page. It is your responsibility to comply with the model's license terms.
    optional: true

  pytorch_model_path:
    type: custom_model
    optional: true
    description: Pytorch model asset path. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  mlflow_model_path:
    type: mlflow_model
    optional: true
    description: MLflow model asset path. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  # DRO-specific parameters
  ENGINE:
    type: string
    default: "vllm"
    optional: true
    description: "Engine type (default: vllm)"

  data_train_files:
    type: uri_file
    optional: false
    description: "Path to the training parquet file"

  data_val_files:
    type: uri_file
    optional: false
    description: "Path to the validation parquet file"

  data_train_batch_size:
    type: integer
    default: 512
    optional: true
    description: "Training batch size"

  data_max_prompt_length:
    type: integer
    default: 1024
    optional: true
    description: "Maximum prompt length"

  data_max_response_length:
    type: integer
    default: 2048
    optional: true
    description: "Maximum response length"

  data_filter_overlong_prompts:
    type: boolean
    default: true
    optional: true
    description: "Filter overlong prompts"

  data_truncation:
    type: string
    default: "error"
    optional: true
    description: "Truncation strategy"

  data_image_key:
    type: string
    default: "images"
    optional: true
    description: "Image key column"

  actor_optim_lr:
    type: number
    default: 3e-6
    optional: true
    description: "Actor optimizer learning rate"

  actor_model_use_remove_padding:
    type: boolean
    default: true
    optional: true
    description: "Use remove padding in model"

  actor_strategy:
    type: string
    enum:
    - "fsdp"
    - "fsdp2"
    - "megatron"
    default: "fsdp2"
    optional: true
    description: "Actor training strategy. Valid values: fsdp (Fully Sharded Data Parallel v1), fsdp2 (Fully Sharded Data Parallel v2), megatron (Megatron-LM backend for large-scale distributed training)"

  actor_fsdp_config_offload_policy:
    type: boolean
    default: true
    optional: true
    description: "FSDP config offload policy to reduce memory usage"

  actor_ppo_mini_batch_size:
    type: integer
    default: 128
    optional: true
    description: "PPO mini batch size"

  actor_ppo_micro_batch_size_per_gpu:
    type: integer
    default: 10
    optional: true
    description: "PPO micro batch size per GPU"

  actor_model_lora_rank:
    type: integer
    default: 64
    optional: true
    description: "LoRA rank"

  actor_model_lora_alpha:
    type: integer
    default: 32
    optional: true
    description: "LoRA alpha"

  actor_model_target_modules:
    type: string
    default: "all-linear"
    optional: true
    description: "Target modules for LoRA"

  actor_model_exclude_modules:
    type: string
    default: ".*visual.*"
    optional: true
    description: "Exclude modules regex"

  actor_use_kl_loss:
    type: boolean
    default: true
    optional: true
    description: "Use KL loss"

  actor_kl_loss_coef:
    type: number
    default: 0.01
    optional: true
    description: "KL loss coefficient"

  actor_kl_loss_type:
    type: string
    default: "low_var_kl"
    optional: true
    description: "KL loss type"

  actor_entropy_coeff:
    type: integer
    default: 0
    optional: true
    description: "Entropy coefficient"

  actor_model_enable_gradient_checkpointing:
    type: boolean
    default: true
    optional: true
    description: "Enable gradient checkpointing"

  actor_fsdp_param_offload:
    type: boolean
    default: false
    optional: true
    description: "FSDP param offload"

  actor_fsdp_optimizer_offload:
    type: boolean
    default: false
    optional: true
    description: "FSDP optimizer offload"

  rollout_log_prob_micro_batch_size_per_gpu:
    type: integer
    default: 20
    optional: true
    description: "Rollout log prob micro batch size per GPU"

  rollout_tensor_model_parallel_size:
    type: integer
    default: 2
    optional: true
    description: "Rollout tensor model parallel size"

  rollout_name:
    type: string
    default: "vllm"
    optional: true
    description: "Rollout name (engine)"

  rollout_dtype:
    type: string
    default: "float16"
    optional: true
    description: "Rollout data type (e.g., float16, bfloat16, float32)"

  rollout_disable_mm_preprocessor_cache:
    type: boolean
    default: true
    optional: true
    description: "Disable MM preprocessor cache"

  rollout_gpu_memory_utilization:
    type: number
    default: 0.6
    optional: true
    description: "Rollout GPU memory utilization"

  rollout_enable_chunked_prefill:
    type: boolean
    default: false
    optional: true
    description: "Enable chunked prefill"

  rollout_enforce_eager:
    type: boolean
    default: false
    optional: true
    description: "Enforce eager execution"

  rollout_free_cache_engine:
    type: boolean
    default: false
    optional: true
    description: "Free cache engine"

  rollout_n:
    type: integer
    default: 5
    optional: true
    description: "Rollout n"

  ref_log_prob_micro_batch_size_per_gpu:
    type: integer
    default: 20
    optional: true
    description: "Ref log prob micro batch size per GPU"

  ref_fsdp_param_offload:
    type: boolean
    default: true
    optional: true
    description: "Ref FSDP param offload"

  algorithm_adv_estimator:
    type: string
    enum:
    - "gae"
    - "grpo"
    - "reinforce_plus_plus"
    - "reinforce_plus_plus_baseline"
    - "remax"
    - "rloo"
    - "opo"
    - "grpo_passk"
    - "gpg"
    default: "grpo"
    optional: true
    description: "Advantage estimator algorithm. Valid values: gae (Generalized Advantage Estimation - reduces variance in policy gradients), grpo (Group Relative Policy Optimization - critic-free GRPO for mathematical reasoning), reinforce_plus_plus (REINFORCE++ - efficient RLHF with global advantage normalization), reinforce_plus_plus_baseline (REINFORCE++ with baseline variant), remax (ReMax - simple and efficient RLHF method), rloo (REINFORCE Leave-One-Out - variance reduction via leave-one-out baseline), opo (On-Policy RL with Optimal Reward Baseline), grpo_passk (GRPO for Pass@k evaluation metrics), gpg (Group Policy Gradient - minimalist RL for reasoning tasks)"

  algorithm_use_kl_in_reward:
    type: boolean
    default: false
    optional: true
    description: "Use KL in reward"

  trainer_critic_warmup:
    type: integer
    default: 0
    optional: true
    description: "Critic warmup"

  trainer_n_gpus_per_node:
    type: integer
    default: 8
    optional: true
    description: "Number of GPUs per node"

  trainer_nnodes:
    type: integer
    default: 1
    optional: true
    description: "Number of nodes"

  trainer_save_freq:
    type: integer
    default: 20
    optional: true
    description: "Save frequency"

  trainer_test_freq:
    type: integer
    default: 5
    optional: true
    description: "Test frequency"

  trainer_total_epochs:
    type: integer
    default: 15
    optional: true
    description: "Total epochs"

  total_training_steps:
    type: integer
    optional: true
    description: "Total number of training steps"

  # Compute parameters
  compute_model_import:
    type: string
    optional: true
    default: serverless
    description: compute to be used for model_import eg. provide 'FT-Cluster' if your compute is named 'FT-Cluster'. Special characters like \ and ' are invalid in the parameter value. If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used
  compute_finetune:
    type: string
    optional: true
    default: serverless
    description: compute to be used for finetune eg. provide 'FT-Cluster' if your compute is named 'FT-Cluster'. Special characters like \ and ' are invalid in the parameter value. If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used

outputs:
  model_output:
    type: uri_folder
    description: "Directory containing the trained model artifacts"
    mode: rw_mount

jobs:
  chat_completion_model_import:
    type: command
    component: azureml:chat_completion_model_import:0.0.83
    compute: '${{parent.inputs.compute_model_import}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_model_import}}'
    inputs:
      huggingface_id: '${{parent.inputs.huggingface_id}}'
      pytorch_model_path: '${{parent.inputs.pytorch_model_path}}'
      mlflow_model_path: '${{parent.inputs.mlflow_model_path}}'
  verl_trainer_component:
    type: command
    component: azureml:verl_trainer_component:0.0.23
    compute: '${{parent.inputs.compute_finetune}}'
    environment_variables:
      _AZUREML_CR_ENABLE_ITP_CAP: "false"
    distribution:
      type: mpi
      process_count_per_instance: '${{parent.inputs.number_of_gpu_to_use_finetuning}}'
    resources:
      instance_count: '${{parent.inputs.num_nodes_finetune}}'
      instance_type: '${{parent.inputs.instance_type_finetune}}'
      shm_size: '${{parent.inputs.shm_size_finetune}}'
    inputs:
      ENGINE: '${{parent.inputs.ENGINE}}'
      data_train_files: '${{parent.inputs.data_train_files}}'
      data_val_files: '${{parent.inputs.data_val_files}}'
      data_train_batch_size: '${{parent.inputs.data_train_batch_size}}'
      data_max_prompt_length: '${{parent.inputs.data_max_prompt_length}}'
      data_max_response_length: '${{parent.inputs.data_max_response_length}}'
      data_filter_overlong_prompts: '${{parent.inputs.data_filter_overlong_prompts}}'
      data_truncation: '${{parent.inputs.data_truncation}}'
      data_image_key: '${{parent.inputs.data_image_key}}'
      actor_model_path: '${{parent.jobs.chat_completion_model_import.outputs.output_dir}}'
      actor_optim_lr: '${{parent.inputs.actor_optim_lr}}'
      actor_model_use_remove_padding: '${{parent.inputs.actor_model_use_remove_padding}}'
      actor_strategy: '${{parent.inputs.actor_strategy}}'
      actor_fsdp_config_offload_policy: '${{parent.inputs.actor_fsdp_config_offload_policy}}'
      actor_ppo_mini_batch_size: '${{parent.inputs.actor_ppo_mini_batch_size}}'
      actor_ppo_micro_batch_size_per_gpu: '${{parent.inputs.actor_ppo_micro_batch_size_per_gpu}}'
      actor_model_lora_rank: '${{parent.inputs.actor_model_lora_rank}}'
      actor_model_lora_alpha: '${{parent.inputs.actor_model_lora_alpha}}'
      actor_model_target_modules: '${{parent.inputs.actor_model_target_modules}}'
      actor_model_exclude_modules: '${{parent.inputs.actor_model_exclude_modules}}'
      actor_use_kl_loss: '${{parent.inputs.actor_use_kl_loss}}'
      actor_kl_loss_coef: '${{parent.inputs.actor_kl_loss_coef}}'
      actor_kl_loss_type: '${{parent.inputs.actor_kl_loss_type}}'
      actor_entropy_coeff: '${{parent.inputs.actor_entropy_coeff}}'
      actor_model_enable_gradient_checkpointing: '${{parent.inputs.actor_model_enable_gradient_checkpointing}}'
      actor_fsdp_param_offload: '${{parent.inputs.actor_fsdp_param_offload}}'
      actor_fsdp_optimizer_offload: '${{parent.inputs.actor_fsdp_optimizer_offload}}'
      rollout_log_prob_micro_batch_size_per_gpu: '${{parent.inputs.rollout_log_prob_micro_batch_size_per_gpu}}'
      rollout_tensor_model_parallel_size: '${{parent.inputs.rollout_tensor_model_parallel_size}}'
      rollout_name: '${{parent.inputs.rollout_name}}'
      rollout_dtype: '${{parent.inputs.rollout_dtype}}'
      rollout_disable_mm_preprocessor_cache: '${{parent.inputs.rollout_disable_mm_preprocessor_cache}}'
      rollout_gpu_memory_utilization: '${{parent.inputs.rollout_gpu_memory_utilization}}'
      rollout_enable_chunked_prefill: '${{parent.inputs.rollout_enable_chunked_prefill}}'
      rollout_enforce_eager: '${{parent.inputs.rollout_enforce_eager}}'
      rollout_free_cache_engine: '${{parent.inputs.rollout_free_cache_engine}}'
      rollout_n: '${{parent.inputs.rollout_n}}'
      ref_log_prob_micro_batch_size_per_gpu: '${{parent.inputs.ref_log_prob_micro_batch_size_per_gpu}}'
      ref_fsdp_param_offload: '${{parent.inputs.ref_fsdp_param_offload}}'
      algorithm_adv_estimator: '${{parent.inputs.algorithm_adv_estimator}}'
      algorithm_use_kl_in_reward: '${{parent.inputs.algorithm_use_kl_in_reward}}'
      trainer_critic_warmup: '${{parent.inputs.trainer_critic_warmup}}'
      trainer_n_gpus_per_node: '${{parent.inputs.trainer_n_gpus_per_node}}'
      trainer_nnodes: '${{parent.inputs.trainer_nnodes}}'
      trainer_save_freq: '${{parent.inputs.trainer_save_freq}}'
      trainer_test_freq: '${{parent.inputs.trainer_test_freq}}'
      trainer_total_epochs: '${{parent.inputs.trainer_total_epochs}}'
      total_training_steps: '${{parent.inputs.total_training_steps}}'
    outputs:
      model_output: '${{parent.outputs.model_output}}'
