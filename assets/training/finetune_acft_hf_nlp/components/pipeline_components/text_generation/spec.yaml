$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
name: text_generation_pipeline
version: 0.0.40
type: pipeline
display_name: Text Generation Pipeline
description: Pipeline component for text generation
inputs:
# Compute parameters
  instance_type_model_import:
    type: string
    optional: true
    default: Standard_d12_v2
    description: >-
      Instance type to be used for model_import component in case of serverless compute, eg. standard_d12_v2. 
      The parameter compute_model_import must be set to 'serverless' for instance_type to be used
  instance_type_preprocess:
    type: string
    optional: true
    default: Standard_d12_v2
    description: >-
      Instance type to be used for preprocess component in case of serverless compute, eg. standard_d12_v2. 
      The parameter compute_preprocess must be set to 'serverless' for instance_type to be used
  instance_type_finetune:
    type: string
    optional: true
    default: Standard_nc24rs_v3
    description: >-
      Instance type to be used for finetune component in case of serverless compute, eg. standard_nc24rs_v3. 
      The parameter compute_finetune must be set to 'serverless' for instance_type to be used
  instance_type_model_evaluation:
    type: string
    optional: true
    default: Standard_nc24rs_v3
    description: >-
      Instance type to be used for model_evaluation components in case of serverless compute, eg. standard_nc24rs_v3. 
      The parameter compute_model_evaluation must be set to 'serverless' for instance_type to be used
  shm_size_finetune:
    type: string
    optional: true
    default: 5g
    description: Shared memory size to be used for finetune component. It is useful while using Nebula (via DeepSpeed) which uses shared memory to save model and optimizer states.
  num_nodes_finetune:
    type: integer
    default: 1
    optional: true
    description: number of nodes to be used for finetuning (used for distributed training)
  number_of_gpu_to_use_finetuning:
    type: integer
    default: 1
    optional: true
    description: >-
      number of gpus to be used per node for finetuning, should be equal
      to number of gpu per node in the compute SKU used for finetune

  # ModelSelector parameters
  huggingface_id:
    type: string
    description: Input HuggingFace model id. Incase of continual finetuning provide proper id. Models from Hugging Face are subject to third party license terms available on the Hugging Face model details page. It is your responsibility to comply with the model's license terms.
    optional: true

  # Continual-Finetuning model path
  pytorch_model_path:
    type: custom_model
    optional: true
    description: Pytorch model asset path. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  mlflow_model_path:
    type: mlflow_model
    optional: true
    description: MLflow model asset path. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  # Preprocessing parameters
  task_name:
    type: string
    optional: false
    enum:
      - TextGeneration
    default: TextGeneration
    description: TextGeneration task type

  text_key:
    type: string
    optional: false
    description: >-
      key for text in an example. format your data keeping in mind that
      text is concatenated with ground_truth while finetuning in the form - text + groundtruth.
      for eg. "text"="knock knock\n", "ground_truth"="who's there"; will be treated as "knock knock\nwho's there"

  ground_truth_key:
    type: string
    optional: true
    description: >-
      key for ground_truth in an example. 
      we take separate column for ground_truth to enable use cases like summarization, translation, 
      question_answering, etc. which can be repurposed in form of text-generation where both text and ground_truth are needed.
      This separation is useful for calculating metrics.
      for eg. "text"="Summarize this dialog:\n{input_dialogue}\nSummary:\n", "ground_truth"="{summary of the dialogue}"

  batch_size:
    type: integer
    optional: true
    default: 1000
    description: Number of examples to batch before calling the tokenization function

  pad_to_max_length:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: If set to True, the returned sequences will be padded according to the model's padding side and padding index, up to their `max_seq_length`. If no `max_seq_length` is specified, the padding is done up to the model's max length.
  
  max_seq_length:
    type: integer
    optional: true
    default: -1
    description: Default is -1 which means the padding is done up to the model's max length. Else will be padded to `max_seq_length`.


  # Dataset path Parameters
  train_file_path:
    type: uri_file
    optional: true
    description: Path to the registered training data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  validation_file_path:
    type: uri_file
    optional: true
    description: Path to the registered validation data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  test_file_path:
    type: uri_file
    optional: true
    description: Path to the registered test data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  train_mltable_path:
    type: mltable
    optional: true
    description: Path to the registered training data asset in `mltable` format. Special characters like \ and ' are invalid in the parameter value.

  validation_mltable_path:
    type: mltable
    optional: true
    description: Path to the registered validation data asset in `mltable` format. Special characters like \ and ' are invalid in the parameter value.

  test_mltable_path:
    type: mltable
    optional: true
    description: Path to the registered test data asset in `mltable` format. Special characters like \ and ' are invalid in the parameter value.

  # Finetuning parameters
  # Lora parameters
  apply_lora:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: lora enabled

  merge_lora_weights:
    type: string
    enum:
      - "true"
      - "false"
    default: "true"
    optional: true
    description: if set to true, the lora trained weights will be merged to base model before saving

  lora_alpha:
    type: integer
    default: 128
    optional: true
    description: lora attention alpha

  lora_r:
    type: integer
    default: 8
    optional: true
    description: lora dimension

  lora_dropout:
    type: number
    default: 0.0
    optional: true
    description: lora dropout value

  # Training parameters
  num_train_epochs:
    type: integer
    default: 1
    optional: true
    description: training epochs

  max_steps:
    type: integer
    default: -1
    optional: true
    description: If set to a positive number, the total number of training steps to perform. Overrides 'epochs'. In case of using a finite iterable dataset the training may stop before reaching the set number of steps when all data is exhausted.

  per_device_train_batch_size:
    type: integer
    default: 1
    optional: true
    description: Train batch size

  per_device_eval_batch_size:
    type: integer
    default: 1
    optional: true
    description: Validation batch size

  auto_find_batch_size:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: Flag to enable auto finding of batch size. If the provided 'per_device_train_batch_size' goes into Out Of Memory (OOM) enabling auto_find_batch_size will find the correct batch size by iteratively reducing 'per_device_train_batch_size' by a factor of 2 till the OOM is fixed

  optim:
    type: string
    default: adamw_hf
    optional: true
    enum:
      - adamw_hf
      - adamw_torch
      # - adamw_apex_fused
      - adafactor
    description: Optimizer to be used while training

  learning_rate:
    type: number
    default: 0.00002
    optional: true
    description: Start learning rate. Defaults to linear scheduler.

  warmup_steps:
    type: integer
    default: 0
    optional: true
    description: Number of steps used for a linear warmup from 0 to learning_rate

  weight_decay:
    type: number
    default: 0.0
    optional: true
    description: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer

  adam_beta1:
    type: number
    default: 0.9
    optional: true
    description: The beta1 hyperparameter for the AdamW optimizer

  adam_beta2:
    type: number
    default: 0.999
    optional: true
    description: The beta2 hyperparameter for the AdamW optimizer

  adam_epsilon:
    type: number
    default: 1e-8
    optional: true
    description: The epsilon hyperparameter for the AdamW optimizer

  gradient_accumulation_steps:
    type: integer
    default: 1
    optional: true
    description: Number of updates steps to accumulate the gradients for, before performing a backward/update pass

  eval_accumulation_steps:
    type: integer
    default: -1
    optional: true
    description: Number of predictions steps to accumulate before moving the tensors to the CPU, will be passed as None if set to -1

  lr_scheduler_type:
    type: string
    default: linear
    optional: true
    enum:
      - linear
      - cosine
      - cosine_with_restarts
      - polynomial
      - constant
      - constant_with_warmup
    description: learning rate scheduler to use.

  precision:
    type: string
    enum:
      - "32"
      - "16"
    default: "32"
    optional: true
    description: Apply mixed precision training. This can reduce memory footprint by performing operations in half-precision.

  seed:
    type: integer
    default: 42
    optional: true
    description: Random seed that will be set at the beginning of training

  enable_full_determinism:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: Ensure reproducible behavior during distributed training

  dataloader_num_workers:
    type: integer
    default: 0
    optional: true
    description: Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.

  ignore_mismatched_sizes:
    type: string
    enum:
      - "true"
      - "false"
    default: "true"
    optional: true
    description: Whether or not to raise an error if some of the weights from the checkpoint do not have the same size as the weights of the model

  max_grad_norm:
    type: number
    default: 1.0
    optional: true
    description: Maximum gradient norm (for gradient clipping)

  evaluation_strategy:
    type: string
    default: epoch
    optional: true
    enum:
      - epoch
      - steps
    description: The evaluation strategy to adopt during training

  evaluation_steps_interval:
    type: number
    default: 0.0
    optional: true
    description: The evaluation steps in fraction of an epoch steps to adopt during training. Overwrites evaluation_steps if not 0.

  eval_steps:
    type: integer
    default: 500
    optional: true
    description: Number of update steps between two evals if evaluation_strategy='steps'

  logging_strategy:
    type: string
    default: steps
    optional: true
    enum:
      - epoch
      - steps
    description: The logging strategy to adopt during training.

  logging_steps:
    type: integer
    default: 10
    optional: true
    description: Number of update steps between two logs if logging_strategy='steps'

  metric_for_best_model:
    type: string
    default: loss
    optional: true
    enum:
      - loss
    description: Specify the metric to use to compare two different models

  resume_from_checkpoint:
    type: string
    default: "false"
    optional: true
    enum:
      - "true"
      - "false"
    description: Loads Optimizer, Scheduler and Trainer state for finetuning if true

  save_total_limit:
    type: integer
    default: -1
    optional: true
    description: If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir. If the value is -1 saves all checkpoints

  # Early Stopping Parameters
  apply_early_stopping:
    type: string
    default: "false"
    optional: true
    enum:
      - "true"
      - "false"
    description: Enable early stopping

  early_stopping_patience:
    type: integer
    default: 1
    optional: true
    description: Stop training when the specified metric worsens for early_stopping_patience evaluation calls

  early_stopping_threshold:
    type: number
    default: 0.0
    optional: true
    description: Denotes how much the specified metric must improve to satisfy early stopping conditions

  # Deepspeed Parameters
  apply_deepspeed:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: If set to true, will enable deepspeed for training

  deepspeed:
    type: uri_file
    optional: true
    description: Deepspeed config to be used for finetuning
    mode: rw_mount

  deepspeed_stage:
    type: string
    optional: true
    default: "2"
    enum:
       - "2"
       - "3"
    description: This parameter configures which DEFAULT deepspeed config to be used - stage2 or stage3. The default choice is stage2. Note that, this parameter is ONLY applicable when user doesn't pass any config information via deepspeed port.

  # ORT Parameters
  apply_ort:
    type: string
    enum:
      - "true"
      - "false"
    default: "false"
    optional: true
    description: If set to true, will use the ONNXRunTime training

  # Validation parameters
  system_properties:
    type: string
    optional: true
    description: Validation parameters propagated from pipeline.

  # Model Evaluation parameters
  evaluation_config:
    type: uri_file
    optional: true
    description: Additional parameters for Computing Metrics. Special characters like \ and ' are invalid in the parameter value.

  evaluation_config_params:
    type: string
    optional: true
    description: Additional parameters as JSON serielized string

  # Compute parameters
  compute_model_import:
    type: string
    optional: true
    default: serverless
    description: >-
      compute to be used for model_import eg. provide 'FT-Cluster' if
      your compute is named 'FT-Cluster'. Special characters like \ and ' are invalid in the parameter value.
      If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used
  compute_preprocess:
    type: string
    optional: true
    default: serverless
    description: >-
      compute to be used for preprocess eg. provide 'FT-Cluster' if your
      compute is named 'FT-Cluster'. Special characters like \ and ' are invalid in the parameter value.
      If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used
  compute_finetune:
    type: string
    optional: true
    default: serverless
    description: >-
      compute to be used for finetune eg. provide 'FT-Cluster' if your
      compute is named 'FT-Cluster'. Special characters like \ and ' are invalid in the parameter value.
      If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used
  compute_model_evaluation:
    type: string
    optional: true
    default: serverless
    description: >-
      compute to be used for model_eavaluation eg. provide 'FT-Cluster' if your
      compute is named 'FT-Cluster'. Special characters like \ and ' are invalid in the parameter value.
      If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used

outputs:
  pytorch_model_folder:
    type: uri_folder
    description: output folder containing _best_ model as defined by _metric_for_best_model_. Along with the best model, output folder contains checkpoints saved after every evaluation which is defined by the _evaluation_strategy_. Each checkpoint contains the model weight(s), config, tokenizer, optimzer, scheduler and random number states.
    mode: rw_mount

  mlflow_model_folder:
    type: mlflow_model
    description: output folder containing _best_ finetuned model in mlflow format.
    mode: rw_mount

  evaluation_result:
    type: uri_folder
    description: Test Data Evaluation Results

jobs:
  ft_nlp_common_validation:
    type: command
    component: azureml:ft_nlp_common_validation:0.0.38
    compute: '${{parent.inputs.compute_model_import}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_model_import}}'
    inputs:
      mlflow_model_path: '${{parent.inputs.mlflow_model_path}}'
      compute_finetune: '${{parent.inputs.compute_finetune}}'
      compute_model_import: '${{parent.inputs.compute_model_import}}'
      compute_data_preprocess: '${{parent.inputs.compute_preprocess}}'
      task_name: 'text-generation'
      num_nodes_finetune: '${{parent.inputs.num_nodes_finetune}}'
      number_of_gpu_to_use_finetuning: '${{parent.inputs.number_of_gpu_to_use_finetuning}}'
      train_file_path: '${{parent.inputs.train_file_path}}'
      validation_file_path: '${{parent.inputs.validation_file_path}}'
      test_file_path: '${{parent.inputs.test_file_path}}'
      train_mltable_path: '${{parent.inputs.train_mltable_path}}'
      validation_mltable_path: '${{parent.inputs.validation_mltable_path}}'
      test_mltable_path: '${{parent.inputs.test_mltable_path}}'
      user_column_names: '${{parent.inputs.text_key}},${{parent.inputs.ground_truth_key}}'
      system_properties: '${{parent.inputs.system_properties}}'
      num_train_epochs: '${{parent.inputs.num_train_epochs}}'
      max_steps: '${{parent.inputs.max_steps}}'
      per_device_train_batch_size: '${{parent.inputs.per_device_train_batch_size}}'
      per_device_eval_batch_size: '${{parent.inputs.per_device_eval_batch_size}}'
      learning_rate: '${{parent.inputs.learning_rate}}'
      adam_beta1: '${{parent.inputs.adam_beta1}}'
      adam_beta2: '${{parent.inputs.adam_beta2}}'
      adam_epsilon: '${{parent.inputs.adam_epsilon}}'
      apply_deepspeed: '${{parent.inputs.apply_deepspeed}}'
      precision: '${{parent.inputs.precision}}'
      deepspeed_stage: '${{parent.inputs.deepspeed_stage}}'
      apply_ort: '${{parent.inputs.apply_ort}}'
      apply_lora: '${{parent.inputs.apply_lora}}'
      ignore_mismatched_sizes: '${{parent.inputs.ignore_mismatched_sizes}}'
      max_seq_length: '${{parent.inputs.max_seq_length}}'
      auto_find_batch_size: '${{parent.inputs.auto_find_batch_size}}'
  text_generation_model_import:
    type: command
    component: azureml:text_generation_model_import:0.0.38
    compute: '${{parent.inputs.compute_model_import}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_model_import}}'
    inputs:
      huggingface_id: '${{parent.inputs.huggingface_id}}'
      pytorch_model_path: '${{parent.inputs.pytorch_model_path}}'
      mlflow_model_path: '${{parent.inputs.mlflow_model_path}}'
      validation_output: '${{parent.jobs.ft_nlp_common_validation.outputs.validation_info}}'
      system_properties: '${{parent.inputs.system_properties}}'
  text_generation_datapreprocess:
    type: command
    component: azureml:text_generation_datapreprocess:0.0.38
    compute: '${{parent.inputs.compute_preprocess}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_preprocess}}'
    inputs:
      text_key: '${{parent.inputs.text_key}}'
      ground_truth_key: '${{parent.inputs.ground_truth_key}}'
      batch_size: '${{parent.inputs.batch_size}}'
      pad_to_max_length: '${{parent.inputs.pad_to_max_length}}'
      max_seq_length: '${{parent.inputs.max_seq_length}}'
      train_file_path: '${{parent.inputs.train_file_path}}'
      validation_file_path: '${{parent.inputs.validation_file_path}}'
      test_file_path: '${{parent.inputs.test_file_path}}'
      train_mltable_path: '${{parent.inputs.train_mltable_path}}'
      validation_mltable_path: '${{parent.inputs.validation_mltable_path}}'
      test_mltable_path: '${{parent.inputs.test_mltable_path}}'
      model_selector_output: '${{parent.jobs.text_generation_model_import.outputs.output_dir}}'
      system_properties: '${{parent.inputs.system_properties}}'
  text_generation_finetune:
    type: command
    component: azureml:text_generation_finetune:0.0.38
    compute: '${{parent.inputs.compute_finetune}}'
    distribution:
      type: pytorch
      process_count_per_instance: '${{parent.inputs.number_of_gpu_to_use_finetuning}}'
    resources:
      instance_count: '${{parent.inputs.num_nodes_finetune}}'
      instance_type: '${{parent.inputs.instance_type_finetune}}'
      shm_size: '${{parent.inputs.shm_size_finetune}}'
    inputs:
      apply_lora: '${{parent.inputs.apply_lora}}'
      merge_lora_weights: '${{parent.inputs.merge_lora_weights}}'
      lora_alpha: '${{parent.inputs.lora_alpha}}'
      lora_r: '${{parent.inputs.lora_r}}'
      lora_dropout: '${{parent.inputs.lora_dropout}}'
      num_train_epochs: '${{parent.inputs.num_train_epochs}}'
      max_steps: '${{parent.inputs.max_steps}}'
      per_device_train_batch_size: '${{parent.inputs.per_device_train_batch_size}}'
      per_device_eval_batch_size: '${{parent.inputs.per_device_eval_batch_size}}'
      auto_find_batch_size: '${{parent.inputs.auto_find_batch_size}}'
      optim: '${{parent.inputs.optim}}'
      learning_rate: '${{parent.inputs.learning_rate}}'
      warmup_steps: '${{parent.inputs.warmup_steps}}'
      weight_decay: '${{parent.inputs.weight_decay}}'
      adam_beta1: '${{parent.inputs.adam_beta1}}'
      adam_beta2: '${{parent.inputs.adam_beta2}}'
      adam_epsilon: '${{parent.inputs.adam_epsilon}}'
      gradient_accumulation_steps: '${{parent.inputs.gradient_accumulation_steps}}'
      eval_accumulation_steps: '${{parent.inputs.eval_accumulation_steps}}'
      lr_scheduler_type: '${{parent.inputs.lr_scheduler_type}}'
      precision: '${{parent.inputs.precision}}'
      seed: '${{parent.inputs.seed}}'
      enable_full_determinism: '${{parent.inputs.enable_full_determinism}}'
      dataloader_num_workers: '${{parent.inputs.dataloader_num_workers}}'
      ignore_mismatched_sizes: '${{parent.inputs.ignore_mismatched_sizes}}'
      max_grad_norm: '${{parent.inputs.max_grad_norm}}'
      evaluation_strategy: '${{parent.inputs.evaluation_strategy}}'
      evaluation_steps_interval: '${{parent.inputs.evaluation_steps_interval}}'
      eval_steps: '${{parent.inputs.eval_steps}}'
      logging_strategy: '${{parent.inputs.logging_strategy}}'
      logging_steps: '${{parent.inputs.logging_steps}}'
      metric_for_best_model: '${{parent.inputs.metric_for_best_model}}'
      resume_from_checkpoint: '${{parent.inputs.resume_from_checkpoint}}'
      save_total_limit: '${{parent.inputs.save_total_limit}}'
      apply_early_stopping: '${{parent.inputs.apply_early_stopping}}'
      early_stopping_patience: '${{parent.inputs.early_stopping_patience}}'
      early_stopping_threshold: '${{parent.inputs.early_stopping_threshold}}'
      apply_deepspeed: '${{parent.inputs.apply_deepspeed}}'
      deepspeed: '${{parent.inputs.deepspeed}}'
      deepspeed_stage: '${{parent.inputs.deepspeed_stage}}'
      apply_ort: '${{parent.inputs.apply_ort}}'
      preprocess_output: '${{parent.jobs.text_generation_datapreprocess.outputs.output_dir}}'
      model_selector_output: '${{parent.jobs.text_generation_model_import.outputs.output_dir}}'
      system_properties: '${{parent.inputs.system_properties}}'
    outputs:
      pytorch_model_folder: '${{parent.outputs.pytorch_model_folder}}'
  ft_nlp_model_converter:
    type: command
    component: azureml:ft_nlp_model_converter:0.0.38
    compute: '${{parent.inputs.compute_finetune}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_finetune}}'
    inputs:
      model_path: '${{parent.jobs.text_generation_finetune.outputs.pytorch_model_folder}}'
      system_properties: '${{parent.inputs.system_properties}}'
    outputs:
      mlflow_model_folder: '${{parent.outputs.mlflow_model_folder}}'
  model_prediction:
    type: command
    component: azureml:model_prediction:0.0.21
    compute: '${{parent.inputs.compute_model_evaluation}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_model_evaluation}}'
    inputs:
      task: text-generation
      test_data: '${{parent.jobs.text_generation_datapreprocess.outputs.output_dir}}'
      label_column_name: '${{parent.inputs.ground_truth_key}}'
      input_column_names: '${{parent.inputs.text_key}}'
      batch_size: '${{parent.inputs.per_device_train_batch_size}}'
      device: auto
      mlflow_model: '${{parent.jobs.ft_nlp_model_converter.outputs.mlflow_model_folder}}'
      evaluation_config: '${{parent.inputs.evaluation_config}}'
      evaluation_config_params: '${{parent.inputs.evaluation_config_params}}'
  compute_metrics:
    type: command
    component: azureml:compute_metrics:0.0.21
    compute: '${{parent.inputs.compute_model_evaluation}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_model_evaluation}}'
    inputs:
      task: text-generation
      ground_truth: '${{parent.jobs.model_prediction.outputs.ground_truth}}'
      ground_truth_column_name: '${{parent.inputs.ground_truth_key}}'
      prediction: '${{parent.jobs.model_prediction.outputs.predictions}}'
      prediction_column_name: predictions
      prediction_probabilities: '${{parent.jobs.model_prediction.outputs.prediction_probabilities}}'
      evaluation_config: '${{parent.inputs.evaluation_config}}'
      evaluation_config_params: '${{parent.inputs.evaluation_config_params}}'
    outputs:
      evaluation_result: '${{parent.outputs.evaluation_result}}'
