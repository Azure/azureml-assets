$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
name: pipeline_draft_model
version: 0.0.1
type: pipeline
display_name: Pipeline Draft Model
description: Pipeline to train draft model for Speculative Decoding.


inputs:
  instance_type_model_import:
    type: string
    optional: true
    default: Standard_d12_v2
    description: Instance type to be used for model_import component in case of serverless compute, eg. standard_d12_v2. The parameter compute_model_import must be set to 'serverless' for instance_type to be used
  instance_type_draft_model_training:
    type: string
    optional: true
    default: Standard_nc24rs_v3
    description: Instance type to be used for draft_model_training component in case of serverless compute, eg. standard_nc24rs_v3. The parameter compute_draft_training must be set to 'serverless' for instance_type to be used
  num_nodes_draft_model_training:
    type: integer
    min: 1
    default: 1
    optional: true
    description: number of nodes to be used for draft model training (used for distributed training)
  number_of_gpu_to_use_draft_model_training:
    type: integer
    min: 1
    default: 1
    optional: true
    description: number of gpus to be used per node for draft model training, should be equal to number of gpu per node in the compute SKU used for training

  # Model Import parameters
  huggingface_id:
    type: string
    description: The string can be any valid Hugging Face id from the [Hugging Face models webpage](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads). Models from Hugging Face are subject to third party license terms available on the Hugging Face model details page. It is your responsibility to comply with the model's license terms.
    optional: true

  pytorch_model_path:
    type: custom_model
    optional: true
    description: Pytorch model asset path. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  mlflow_model_path:
    type: mlflow_model
    optional: true
    description: MLflow model asset path. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  # Dataset parameters
  dataset_train_split:
    type: uri_file
    optional: true
    description: Path to the training dataset in JSONL format
    mode: rw_mount

  dataset_validation_split:
    type: uri_file
    optional: true
    description: Path to the validation dataset in JSONL format
    mode: rw_mount

  # Eagle3 Training parameters
  draft_model_config:
    type: uri_file
    optional: true
    description: Path to draft model configuration JSON file. If not provided, will be auto-generated from target model.
    mode: ro_mount

  num_epochs:
    type: integer
    min: 1
    default: 2
    optional: true
    description: Number of training epochs for draft model.

  training_batch_size:
    type: integer
    min: 1
    default: 2
    optional: true
    description: Batch size for draft model training.

  learning_rate:
    type: number
    default: 0.0001
    optional: true
    description: Learning rate for draft model training.

  max_length:
    type: integer
    default: 2048
    optional: true
    description: Maximum sequence length for draft model training.

  warmup_ratio:
    type: number
    default: 0.015
    optional: true
    description: Warmup ratio for learning rate scheduler.

  max_grad_norm:
    type: number
    default: 0.5
    optional: true
    description: Maximum gradient norm for gradient clipping.

  ttt_length:
    type: integer
    default: 7
    optional: true
    description: The length for Test-Time Training (TTT).

  chat_template:
    type: string
    enum:
    - "llama3"
    - "qwen"
    - "gpt-oss"
    - "deepseek-r1-distill"
    default: llama3
    optional: true
    description: Chat template to use for formatting conversations. Supported templates - qwen, llama3, gpt-oss, deepseek-r1-distill.

  attention_backend:
    type: string
    enum:
    - "flex_attention"
    - "sdpa"
    default: "flex_attention"
    optional: true
    description: Attention implementation backend to use.

  tp_size:
    type: integer
    default: 1
    optional: true
    description: Tensor parallelism size

  dp_size:
    type: integer
    default: 1
    optional: true
    description: Data parallelism size

  draft_global_batch_size:
    type: integer
    default: 8
    optional: true
    description: Global batch size for draft model training

  draft_micro_batch_size:
    type: integer
    default: 1
    optional: true
    description: Micro batch size for draft model

  draft_accumulation_steps:
    type: integer
    default: 1
    optional: true
    description: Gradient accumulation steps for draft model

  log_steps:
    type: integer
    default: 50
    optional: true
    description: Log training metrics every N steps

  eval_interval:
    type: integer
    default: 1
    optional: true
    description: Evaluation interval in epochs

  save_interval:
    type: integer
    default: 1
    optional: true
    description: Checkpoint save interval in epochs

  seed:
    type: integer
    default: 0
    optional: true
    description: Random seed for reproducibility

  total_steps:
    type: integer
    optional: true
    description: Total training steps. If not provided, will be calculated as num_epochs * steps_per_epoch

  dist_timeout:
    type: integer
    default: 20
    optional: true
    description: Timeout for collective communication in minutes

  resume:
    type: string
    enum:
    - "true"
    - "false"
    default: "false"
    optional: true
    description: Whether to resume training from the last checkpoint

  resume_from_checkpoint:
    type: uri_folder
    optional: true
    description: Path to a checkpoint directory to resume training from. Used when resume is true and no checkpoints exist in output folder, or when resume is false to initialize from a pretrained draft model checkpoint.
    mode: ro_mount

  build_dataset_num_proc:
    type: integer
    min: 1
    max: 128
    default: 96
    optional: true
    description: Number of processes to use for building the dataset. Recommended to set same as number of CPU cores available. If this is too high, one may loose performance due to context switching.

  # Compute parameters
  compute_model_import:
    type: string
    optional: true
    default: serverless
    description: Compute to be used for model import.
  compute_draft_model_training:
    type: string
    optional: true
    default: serverless
    description: Compute to be used for draft_model_training.

outputs:
  output_model_path:
    type: uri_folder
    description: Output folder containing trained draft model checkpoints.
    mode: rw_mount

jobs:
  draft_model_import:
    type: command
    component: azureml:model_import_lite:0.0.1
    compute: '${{parent.inputs.compute_model_import}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_model_import}}'
    inputs:
      huggingface_id: '${{parent.inputs.huggingface_id}}'
      pytorch_model_path: '${{parent.inputs.pytorch_model_path}}'
      mlflow_model_path: '${{parent.inputs.mlflow_model_path}}'
  component_draft_model_trainer:
    type: command
    component: azureml:draft_model_trainer:0.0.1
    compute: '${{parent.inputs.compute_draft_model_training}}'
    distribution:
      type: pytorch
      process_count_per_instance: '${{parent.inputs.number_of_gpu_to_use_draft_model_training}}'
    resources:
      instance_count: '${{parent.inputs.num_nodes_draft_model_training}}'
      instance_type: '${{parent.inputs.instance_type_draft_model_training}}'
      shm_size: '${{parent.inputs.shm_size_draft_model_training}}'
    inputs:
      dataset_train_split: '${{parent.inputs.dataset_train_split}}'
      dataset_validation_split: '${{parent.inputs.dataset_validation_split}}'
      target_model_path: '${{parent.jobs.draft_model_import.outputs.output_dir}}'
      draft_model_config: '${{parent.inputs.draft_model_config}}'
      num_epochs: '${{parent.inputs.num_epochs}}'
      batch_size: '${{parent.inputs.training_batch_size}}'
      learning_rate: '${{parent.inputs.learning_rate}}'
      max_length: '${{parent.inputs.max_length}}'
      warmup_ratio: '${{parent.inputs.warmup_ratio}}'
      max_grad_norm: '${{parent.inputs.max_grad_norm}}'
      ttt_length: '${{parent.inputs.ttt_length}}'
      chat_template: '${{parent.inputs.chat_template}}'
      attention_backend: '${{parent.inputs.attention_backend}}'
      tp_size: '${{parent.inputs.tp_size}}'
      dp_size: '${{parent.inputs.dp_size}}'
      draft_global_batch_size: '${{parent.inputs.draft_global_batch_size}}'
      draft_micro_batch_size: '${{parent.inputs.draft_micro_batch_size}}'
      draft_accumulation_steps: '${{parent.inputs.draft_accumulation_steps}}'
      log_steps: '${{parent.inputs.log_steps}}'
      eval_interval: '${{parent.inputs.eval_interval}}'
      save_interval: '${{parent.inputs.save_interval}}'
      seed: '${{parent.inputs.seed}}'
      total_steps: '${{parent.inputs.total_steps}}'
      dist_timeout: '${{parent.inputs.dist_timeout}}'
      resume: '${{parent.inputs.resume}}'
      resume_from_checkpoint: '${{parent.inputs.resume_from_checkpoint}}'
      build_dataset_num_proc: '${{parent.inputs.build_dataset_num_proc}}'
    outputs:
      output_model_path: '${{parent.outputs.output_model_path}}'
