$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: draft_model_trainer
version: 0.0.1
type: command

is_deterministic: true

display_name: Component Draft Model Trainer
description: Component to train draft model for Speculative Decoding.

environment: azureml://registries/test_centralus/environments/acft-draft-model-training/versions/12

code: ../../src/speculative_decoding_draft_trainer/

distribution:
  type: pytorch

inputs:

  dataset_train_split:
    type: uri_file
    optional: true
    description: Path to the training dataset in JSONL format.
    mode: rw_mount

  dataset_validation_split:
    type: uri_file
    optional: true
    description: Path to the validation dataset in JSONL format.
    mode: rw_mount

  target_model_path:
    type: uri_folder
    optional: false
    description: Path to the target model for speculative decoding.
    mode: rw_mount

  draft_model_config:
    type: uri_file
    optional: true
    description: Path to draft model configuration JSON file. If not provided, will be auto-generated from target model.
    mode: ro_mount

  num_epochs:
    type: integer
    default: 2
    optional: true
    description: Number of training epochs for draft model.

  batch_size:
    type: integer
    min: 1
    default: 2
    optional: true
    description: Batch size for draft model training.

  learning_rate:
    type: number
    default: 0.0001
    optional: true
    description: Learning rate for training.

  max_length:
    type: integer
    default: 2048
    optional: true
    description: Maximum sequence length for draft model training.

  warmup_ratio:
    type: number
    default: 0.015
    optional: true
    description: Warmup ratio for learning rate scheduler.

  max_grad_norm:
    type: number
    default: 0.5
    optional: true
    description: Maximum gradient norm for gradient clipping.

  ttt_length:
    type: integer
    default: 7
    optional: true
    description: The length for Training-Time Test (TTT).

  chat_template:
    type: string
    enum:
    - "llama3"
    - "qwen"
    - "gpt-oss"
    - "deepseek-r1-distill"
    default: llama3
    optional: true
    description: Chat template to use for formatting conversations. Supported templates - qwen, llama3, gpt-oss, deepseek-r1-distill.

  attention_backend:
    type: string
    enum:
    - "flex_attention"
    - "sdpa"
    default: "flex_attention"
    optional: true
    description: Attention implementation backend to use.

  tp_size:
    type: integer
    default: 1
    optional: true
    description: Tensor parallelism size

  dp_size:
    type: integer
    default: 1
    optional: true
    description: Data parallelism size

  draft_global_batch_size:
    type: integer
    default: 8
    optional: true
    description: Global batch size for draft model training

  draft_micro_batch_size:
    type: integer
    default: 1
    optional: true
    description: Micro batch size for draft model

  draft_accumulation_steps:
    type: integer
    default: 1
    optional: true
    description: Gradient accumulation steps for draft model

  log_steps:
    type: integer
    default: 50
    optional: true
    description: Log training metrics every N steps

  eval_interval:
    type: integer
    default: 1
    optional: true
    description: Evaluation interval in epochs

  save_interval:
    type: integer
    default: 1
    optional: true
    description: Checkpoint save interval in epochs

  seed:
    type: integer
    default: 0
    optional: true
    description: Random seed for reproducibility

  total_steps:
    type: integer
    default: -1
    optional: true
    description: Total training steps. Set to -1 or 0 to auto-calculate based on num_epochs and dataset size. If set to a positive value, will override num_epochs.

  dist_timeout:
    type: integer
    default: 20
    optional: true
    description: Timeout for collective communication in minutes

  resume:
    type: boolean
    default: false
    optional: true
    description: Whether to resume training from the last checkpoint

  resume_from_checkpoint:
    type: uri_folder
    optional: true
    description: Path to a checkpoint directory to resume training from. Used when resume is true and no checkpoints exist in output folder, or when resume is false to initialize from a pretrained draft model checkpoint.
    mode: ro_mount

  build_dataset_num_proc:
    type: integer
    min: 1
    max: 128
    default: 96
    optional: true
    description: Number of processes to use for building the dataset. Recommended to set same as number of CPU cores available. If this is too high, one may loose performance due to context switching.

outputs:
  output_model_path:
    type: uri_folder
    description: Output folder containing trained Eagle3 draft model checkpoints
    mode: rw_mount

command: >-
  python train_draft_model.py
  $[[--dataset_train_split '${{inputs.dataset_train_split}}']]
  $[[--dataset_validation_split '${{inputs.dataset_validation_split}}']]
  --target_model_path '${{inputs.target_model_path}}'
  $[[--draft_model_config '${{inputs.draft_model_config}}']]
  --output_dir '${{outputs.output_model_path}}'
  $[[--num_epochs '${{inputs.num_epochs}}']]
  $[[--batch_size '${{inputs.batch_size}}']]
  $[[--learning_rate '${{inputs.learning_rate}}']]
  $[[--max_length '${{inputs.max_length}}']]
  $[[--warmup_ratio '${{inputs.warmup_ratio}}']]
  $[[--max_grad_norm '${{inputs.max_grad_norm}}']]
  $[[--ttt_length '${{inputs.ttt_length}}']]
  $[[--chat_template '${{inputs.chat_template}}']]
  $[[--attention_backend '${{inputs.attention_backend}}']]
  $[[--tp_size '${{inputs.tp_size}}']]
  $[[--dp_size '${{inputs.dp_size}}']]
  $[[--draft_global_batch_size '${{inputs.draft_global_batch_size}}']]
  $[[--draft_micro_batch_size '${{inputs.draft_micro_batch_size}}']]
  $[[--draft_accumulation_steps '${{inputs.draft_accumulation_steps}}']]
  $[[--log_steps '${{inputs.log_steps}}']]
  $[[--eval_interval '${{inputs.eval_interval}}']]
  $[[--save_interval '${{inputs.save_interval}}']]
  $[[--seed '${{inputs.seed}}']]
  $[[--total_steps '${{inputs.total_steps}}']]
  $[[--dist_timeout '${{inputs.dist_timeout}}']]
  $[[--resume '${{inputs.resume}}']]
  $[[--resume_from_checkpoint '${{inputs.resume_from_checkpoint}}']]
  --report_to 'azure_ml'
  $[[--build_dataset_num_proc '${{inputs.build_dataset_num_proc}}']]
  --embedding_key 'model.embed_tokens.weight'
  --verbose True
  --profile False

