$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
type: pipeline

name: automl_hts_inference
display_name: AutoML Hierarchical Timeseries Forecasting - Inference
description: Enables inference for hts components.
version: 0.0.12
is_deterministic: false

inputs:
  raw_data:
    type: uri_folder
    description: 'Folder URI with inference data.'
  compute_name:
    type: string
    description: 'Compute name for inference pipeline.'
  max_nodes:
    type: integer
    description: 'Number of nodes in a compute cluster we will run the inference step on.'
  max_concurrency_per_node:
    type: integer
    description: 'Number of processes that will be run concurrently on any given node. This number should not be larger than 1/2 of the number of cores in an individual node in the specified cluster'
  parallel_step_timeout_in_seconds:
    type: integer
    description: 'The PRS step time out settings is seconds.'
    default: 3600
  train_run_id:
    type: string
    optional: True
    description: 'The train run id used for training models that will be used to generate forecasts.'
  train_experiment_name:
    type: string
    optional: True
    description: 'The train experiment used for inference.'
  forecast_mode:
    type: string
    optional: True
    description: 'The forecast mode used for inference. The possible values are `recursive` and `rolling`.'
    default: 'recursive'
    enum: ['recursive', 'rolling']
  forecast_step:
    type: integer
    optional: True
    description: 'The forecast step used for rolling forecast. See more details here: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast?view=azureml-api-2#evaluating-model-accuracy-with-a-rolling-forecast'
    default: 1
  allocation_method:
    type: string
    description: 'Method that allocates forecasts within a hierarchy. Possible values are average_historical_proportions and proportions_of_historical_average'
    default: 'average_historical_proportions'
    enum: ['average_historical_proportions', 'proportions_of_historical_average']
  forecast_level:
    type: string
    description: 'Specify the level of the hierarchy for which you are interested in obtaining the forecast for.'
  optional_train_metadata:
    type: uri_folder
    optional: True
    description: 'Metadata from training run.'
  forecast_quantiles:
    type: string
    optional: True
    description: 'Space separated list of quantiles to get forecasts for forecast quantiles for forecasting jobs. It is applicable only when the forecast_mode is recursive.'

outputs:
  run_output:
    type: uri_folder
    description: 'Folder URI representing the location of the output data'
  evaluation_configs:
    type: uri_file
    description: 'The evaluation configs.'
  evaluation_data:
    type: uri_file
    description: 'The evaluation data.'

jobs:
  automl_hts_inference_setup: 
    type: command
    component: azureml:automl_hts_inference_setup_step:0.0.12
    inputs:
      raw_Data: ${{parent.inputs.raw_data}}
      enable_event_logger:  True
      instance_count:  ${{parent.inputs.max_nodes}}
      max_concurrency_per_instance:  ${{parent.inputs.max_concurrency_per_node}}
      parallel_step_timeout_in_seconds:  ${{parent.inputs.parallel_step_timeout_in_seconds}}
      train_run_id: ${{parent.inputs.train_run_id}}
      train_experiment_name: ${{parent.inputs.train_experiment_name}}
      forecast_mode: ${{parent.inputs.forecast_mode}}
      forecast_step: ${{parent.inputs.forecast_step}}
      forecast_quantiles: ${{parent.inputs.forecast_quantiles}}
      allocation_method: ${{parent.inputs.allocation_method}}
      forecast_level: ${{parent.inputs.forecast_level}}
      optional_train_metadata: ${{parent.inputs.optional_train_metadata}}
    outputs:
      processed_data:
        type: uri_folder
      metadata:
        type: uri_folder
    compute: ${{parent.inputs.compute_name}}
  automl_hts_model_inference: 
    type: parallel
    component: azureml:automl_hts_prs_inference_step:0.0.12
    inputs:
      partitioned_data: ${{parent.jobs.automl_hts_inference_setup.outputs.processed_data}}
      metadata: ${{parent.jobs.automl_hts_inference_setup.outputs.metadata}}
      enable_event_logger:  True
      instance_count:  ${{parent.inputs.max_nodes}}
      max_concurrency_per_instance:  ${{parent.inputs.max_concurrency_per_node}}
      parallel_step_timeout_in_seconds:  ${{parent.inputs.parallel_step_timeout_in_seconds}}
    outputs:
      output_metadata:
        type: uri_folder
      prediction:
        type: uri_folder
      prs_output:
        type: uri_folder
    resources:
      instance_count: ${{parent.inputs.max_nodes}}
    max_concurrency_per_instance:  ${{parent.inputs.max_concurrency_per_node}}
    retry_settings:
      timeout: ${{parent.inputs.parallel_step_timeout_in_seconds}}
    mini_batch_size: "1"
    compute: ${{parent.inputs.compute_name}}
  automl_hts_inference_collect_step: 
    type: command
    component: azureml:automl_hts_inference_collect_step:0.0.12
    inputs:
      setup_metadata: ${{parent.jobs.automl_hts_inference_setup.outputs.metadata}}
      input_metadata: ${{parent.jobs.automl_hts_model_inference.outputs.output_metadata}}
      input_prediction: ${{parent.jobs.automl_hts_model_inference.outputs.prediction}}
      enable_event_logger:  True
    outputs:
      metadata:
        type: uri_folder
        path: ${{parent.outputs.run_output}}
      evaluation_configs:
        type: uri_file
        path: ${{parent.outputs.evaluation_configs}}
      evaluation_data:
        type: uri_folder
        path: ${{parent.outputs.evaluation_data}}
    compute: ${{parent.inputs.compute_name}}
