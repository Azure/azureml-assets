$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json
type: parallel
version: 0.0.2

name: automl_hts_prs_inference_step
display_name: AutoML HTS - Inference Step

inputs:
  partitioned_data: 
    type: uri_folder
    description: 'Partitioned data for inference.'
  metadata:
    type: uri_folder
    description: 'Metadata from setup step.'
  instance_count:
    type: integer
    description: 'Number of nodes in a compute cluster we will run the inference step on.'
  max_concurrency_per_instance:
    type: integer
    description: 'Number of processes that will be run concurrently on any given node. This number should not be larger than 1/2 of the number of cores in an individual node in the specified cluster'
  parallel_step_timeout_in_seconds:
    type: integer
    description: 'The PRS step time out settings is seconds.'
    default: 3600
  enable_event_logger:
    type: boolean
    optional: True
    description: 'Enable event logger.'
    default: true
  
outputs:
  output_metadata:
    mode: mount
    type: uri_folder
    description: "Metadata calculated from the inference run."
  prediction:
    mode: mount
    type: uri_folder
    description: "Predictions calculated from the inference run."
  prs_output:
    mode: mount
    type: uri_folder
    description: "PRS default output which will not be used."

input_data: ${{inputs.partitioned_data}}
output_data: ${{outputs.prs_output}}
mini_batch_size: "1"
resources:
  instance_count: 2
max_concurrency_per_instance: 4

logging_level: "DEBUG"
retry_settings:
  max_retries: 2
  timeout: 3600

task:
  type: run_function
  code: ../../src/inference
  entry_script: inference.py
  environment: azureml://registries/azureml/environments/automl-gpu/versions/2
  program_arguments: >-
    --setup-metadata ${{inputs.metadata}}
    --error_threshold -1
    --progress_update_timeout ${{inputs.parallel_step_timeout_in_seconds}}
    --copy_logs_to_parent True
    --resource_monitor_interval 20
    --output-metadata ${{outputs.output_metadata}}
    --nodes-count ${{inputs.instance_count}}
    --process_count_per_node ${{inputs.max_concurrency_per_instance}}
    --output-prediction ${{outputs.prediction}}
    $[[--enable-event-logger ${{inputs.enable_event_logger}}]]
