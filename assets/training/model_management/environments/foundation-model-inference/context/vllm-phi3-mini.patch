diff --git a/vllm/attention/backends/abstract.py b/vllm/attention/backends/abstract.py
index 106b00cc..4ed8b049 100644
--- a/vllm/attention/backends/abstract.py
+++ b/vllm/attention/backends/abstract.py
@@ -92,6 +92,9 @@ class AttentionMetadata:
     # in block 0, and 1st slot in block 1, respectively.
     slot_mapping: torch.Tensor
 
+    # Number of original input tokens (without any decoding). Some model (phi3-) need this info to decide model settings 
+    num_orig_input_tokens_tensor: Optional[torch.Tensor]
+
     @property
     @abstractmethod
     def prefill_metadata(self) -> Optional["AttentionMetadata"]:
@@ -131,7 +134,7 @@ class AttentionMetadataBuilder(ABC, Generic[T]):
         raise NotImplementedError
 
     @abstractmethod
-    def build(self, seq_lens: List[int], query_lens: List[int],
+    def build(self, seq_lens: List[int], query_lens: List[int], num_orig_input_tokens_list: List[int],
               cuda_graph_pad_size: int, batch_size: int) -> T:
         """Build attention metadata with on-device tensors."""
         raise NotImplementedError
diff --git a/vllm/attention/backends/flash_attn.py b/vllm/attention/backends/flash_attn.py
index b16a204c..8542c906 100644
--- a/vllm/attention/backends/flash_attn.py
+++ b/vllm/attention/backends/flash_attn.py
@@ -157,6 +157,7 @@ class FlashAttentionMetadata(AttentionMetadata):
             slot_mapping=self.slot_mapping[:self.num_prefill_tokens],
             seq_lens=self.seq_lens[:self.num_prefills],
             seq_lens_tensor=self.seq_lens_tensor[:self.num_prefills],
+            num_orig_input_tokens_tensor=self.num_orig_input_tokens_tensor[:self.num_prefills],
             max_query_len=self.max_query_len,
             max_prefill_seq_len=self.max_prefill_seq_len,
             max_decode_seq_len=0,
@@ -185,6 +186,7 @@ class FlashAttentionMetadata(AttentionMetadata):
             slot_mapping=self.slot_mapping[self.num_prefill_tokens:],
             seq_lens=None,
             seq_lens_tensor=self.seq_lens_tensor[self.num_prefills:],
+            num_orig_input_tokens_tensor=self.num_orig_input_tokens_tensor[:self.num_prefills],
             max_query_len=None,
             max_prefill_seq_len=0,
             max_decode_seq_len=self.max_decode_seq_len,
@@ -270,7 +272,7 @@ class FlashAttentionMetadataBuilder(
                                  seq_len, context_len, start_idx,
                                  self.block_size, inter_data.block_tables)
 
-    def build(self, seq_lens: List[int], query_lens: List[int],
+    def build(self, seq_lens: List[int], query_lens: List[int], num_orig_input_tokens_list: List[int],
               cuda_graph_pad_size: int, batch_size: int):
         """Build attention metadata with on-device tensors."""
         for inter_data in self.input_builder.inter_data_list:
@@ -339,6 +341,8 @@ class FlashAttentionMetadataBuilder(
                      dtype=query_start_loc.dtype,
                      out=query_start_loc[1:])
 
+        num_orig_input_tokens_tensor = torch.tensor(num_orig_input_tokens_list, dtype=torch.long, device=device)
+
         slot_mapping_tensor = torch.tensor(self.slot_mapping,
                                            dtype=torch.long,
                                            device=device)
@@ -350,6 +354,7 @@ class FlashAttentionMetadataBuilder(
             num_decode_tokens=num_decode_tokens,
             seq_lens=seq_lens,
             seq_lens_tensor=seq_lens_tensor,
+            num_orig_input_tokens_tensor=num_orig_input_tokens_tensor,
             max_query_len=max_query_len,
             max_prefill_seq_len=max_prefill_seq_len,
             max_decode_seq_len=max_decode_seq_len,
diff --git a/vllm/attention/backends/flashinfer.py b/vllm/attention/backends/flashinfer.py
index 9dac12d3..01d47d6c 100644
--- a/vllm/attention/backends/flashinfer.py
+++ b/vllm/attention/backends/flashinfer.py
@@ -315,7 +315,7 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):
                 last_page_len = self.block_size
             self.paged_kv_last_page_len.append(last_page_len)
 
-    def build(self, seq_lens: List[int], query_lens: List[int],
+    def build(self, seq_lens: List[int], query_lens: List[int], num_orig_input_tokens_list: List[int],
               cuda_graph_pad_size: int, batch_size: int):
         for inter_data in self.input_builder.inter_data_list:
             self._add_seq_group(inter_data,
diff --git a/vllm/attention/backends/utils.py b/vllm/attention/backends/utils.py
index 5877712b..c39d8cc1 100644
--- a/vllm/attention/backends/utils.py
+++ b/vllm/attention/backends/utils.py
@@ -147,7 +147,7 @@ class CommonMetadataBuilder(AttentionMetadataBuilder[TAttentionMetadata]):
                                  seq_len, context_len, start_idx,
                                  self.block_size, inter_data.block_tables)
 
-    def build(self, seq_lens: List[int], query_lens: List[int],
+    def build(self, seq_lens: List[int], query_lens: List[int], num_orig_input_tokens_list: List[int],
               cuda_graph_pad_size: int, batch_size: int):
         for inter_data in self.input_builder.inter_data_list:
             self._add_seq_group(inter_data,
@@ -214,6 +214,8 @@ class CommonMetadataBuilder(AttentionMetadataBuilder[TAttentionMetadata]):
                      dim=0,
                      dtype=query_start_loc.dtype,
                      out=query_start_loc[1:])
+        
+        num_orig_input_tokens_tensor = torch.tensor(num_orig_input_tokens_list, dtype=torch.long, device=device)
 
         slot_mapping_tensor = torch.tensor(self.slot_mapping,
                                            dtype=torch.long,
@@ -226,6 +228,7 @@ class CommonMetadataBuilder(AttentionMetadataBuilder[TAttentionMetadata]):
             num_decode_tokens=num_decode_tokens,
             seq_lens=seq_lens,
             seq_lens_tensor=seq_lens_tensor,
+            num_orig_input_tokens_tensor=num_orig_input_tokens_tensor,
             max_query_len=max_query_len,
             max_prefill_seq_len=max_prefill_seq_len,
             max_decode_seq_len=max_decode_seq_len,
diff --git a/vllm/attention/backends/xformers.py b/vllm/attention/backends/xformers.py
index 1573cd7d..9dbe354e 100644
--- a/vllm/attention/backends/xformers.py
+++ b/vllm/attention/backends/xformers.py
@@ -195,6 +195,8 @@ class XFormersMetadata(AttentionMetadata, PagedAttentionMetadata):
                            self.seq_lens_tensor[:self.num_prefills])
         context_lens_tensor = (None if self.context_lens_tensor is None else
                                self.context_lens_tensor[:self.num_prefills])
+        num_orig_input_tokens_tensor = (None if self.num_orig_input_tokens_tensor is None else
+                               self.num_orig_input_tokens_tensor[:self.num_prefills])
         block_tables = (None if self.block_tables is None else
                         self.block_tables[:self.num_prefills])
 
@@ -206,6 +208,7 @@ class XFormersMetadata(AttentionMetadata, PagedAttentionMetadata):
             slot_mapping=slot_mapping,
             seq_lens=seq_lens,
             seq_lens_tensor=seq_lens_tensor,
+            num_orig_input_tokens_tensor=num_orig_input_tokens_tensor,
             max_query_len=self.max_query_len,
             max_prefill_seq_len=self.max_prefill_seq_len,
             max_decode_seq_len=0,
@@ -240,7 +243,9 @@ class XFormersMetadata(AttentionMetadata, PagedAttentionMetadata):
                            self.seq_lens_tensor[self.num_prefills:])
         block_tables = (None if self.block_tables is None else
                         self.block_tables[self.num_prefills:])
-
+        num_orig_input_tokens_tensor = (None if self.num_orig_input_tokens_tensor is None else
+                               self.num_orig_input_tokens_tensor[:self.num_prefills])
+        
         # Construct & cache decode-phase attention metadata structure
         self._cached_decode_metadata = XFormersMetadata(
             num_prefills=0,
@@ -248,6 +253,7 @@ class XFormersMetadata(AttentionMetadata, PagedAttentionMetadata):
             num_decode_tokens=self.num_decode_tokens,
             slot_mapping=slot_mapping,
             seq_lens_tensor=seq_lens_tensor,
+            num_orig_input_tokens_tensor=num_orig_input_tokens_tensor,
             max_prefill_seq_len=0,
             max_decode_seq_len=self.max_decode_seq_len,
             block_tables=block_tables,
diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py
index 3f9573f5..e3bbbfa4 100644
--- a/vllm/model_executor/layers/rotary_embedding.py
+++ b/vllm/model_executor/layers/rotary_embedding.py
@@ -541,7 +541,7 @@ class Phi3LongRoPEScaledRotaryEmbedding(nn.Module):
                 math.log(self.original_max_position_embeddings))
 
         short_cache = self._compute_cos_sin_cache(
-            original_max_position_embeddings, short_factor, short_mscale)
+            max_position_embeddings, short_factor, short_mscale)
         short_cache = short_cache.to(dtype)
         self.register_buffer("short_cos_sin_cache",
                              short_cache,
@@ -586,13 +586,17 @@ class Phi3LongRoPEScaledRotaryEmbedding(nn.Module):
         query: torch.Tensor,
         key: torch.Tensor,
         offsets: Optional[torch.Tensor] = None,
+        *,
+        num_orig_input_tokens_tensor: torch.Tensor = None,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         query = query.view(*query.shape[:-1], -1, self.head_size)
         key = key.view(*key.shape[:-1], -1, self.head_size)
 
         k = self.original_max_position_embeddings
-        long_prompt_offset = (torch.any(positions > k).float() *
-                              torch.full_like(positions, k)).long()
+        long_prompt_offset_0 = (torch.any(positions > k).float() * torch.full_like(positions, k)).long()
+        long_prompt_offset_1 = torch.where(num_orig_input_tokens_tensor <= k, torch.zeros_like(num_orig_input_tokens_tensor), torch.full_like(num_orig_input_tokens_tensor, self.max_position_embeddings))
+        # print(f"max_seq_tokens_tensor[0]: {max_seq_tokens_tensor[0]}")
+        long_prompt_offset = torch.where(num_orig_input_tokens_tensor < 0, long_prompt_offset_0, long_prompt_offset_1)
         idx = (torch.add(positions, long_prompt_offset)
                if long_prompt_offset is not None else positions)
         self.long_short_cos_sin_cache: torch.Tensor = (
diff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py
index 2052c443..6626151b 100644
--- a/vllm/model_executor/models/llama.py
+++ b/vllm/model_executor/models/llama.py
@@ -171,7 +171,7 @@ class LlamaAttention(nn.Module):
     ) -> torch.Tensor:
         qkv, _ = self.qkv_proj(hidden_states)
         q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
-        q, k = self.rotary_emb(positions, q, k)
+        q, k = self.rotary_emb(positions, q, k, num_orig_input_tokens_tensor=attn_metadata.num_orig_input_tokens_tensor)
         attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
         output, _ = self.o_proj(attn_output)
         return output
diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index e63be184..49f46423 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -195,6 +195,9 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
         query_lens: List[int] = field(default_factory=list)
         # The number of tokens that are already computed.
         context_lens: List[int] = field(default_factory=list)
+        # Number of original input tokens of each sequence
+        num_orig_input_tokens_list: List[int] = field(default_factory=list)
+
         # The current sliding window block.
         curr_sliding_window_blocks: List[int] = field(default_factory=list)
 
@@ -219,6 +222,7 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
 
             self.input_tokens = [[] for _ in range(self.n_seqs)]
             self.input_positions = [[] for _ in range(self.n_seqs)]
+            self.num_orig_input_tokens_list = [[] for _ in range(self.n_seqs)]
             self.seq_lens = [0] * self.n_seqs
             self.orig_seq_lens = [0] * self.n_seqs
             self.query_lens = [0] * self.n_seqs
@@ -312,6 +316,8 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
         inter_data.context_lens[seq_idx] = context_len
         inter_data.input_tokens[seq_idx] = tokens
         inter_data.input_positions[seq_idx] = list(range(context_len, seq_len))
+        inter_data.num_orig_input_tokens_list[seq_idx] = [seq_data.get_prompt_len()] * (seq_len - context_len)
+        
         inter_data.query_lens[
             seq_idx] = seq_len - context_len if inter_data.is_prompt else 1
 
@@ -474,6 +480,11 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
             flatten_2d_lists(inter_data.input_positions)
             for inter_data in self.inter_data_list
         ])
+        num_orig_input_tokens_list = flatten_2d_lists([
+            flatten_2d_lists(inter_data.num_orig_input_tokens_list)
+            for inter_data in self.inter_data_list
+        ])
+        
         seq_lens = []
         max_decode_seq_len = 0
         for inter_data in self.inter_data_list:
@@ -509,6 +520,8 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
         # Tokens and positions.
         input_tokens.extend([0] * cuda_graph_pad_size)
         input_positions.extend([0] * cuda_graph_pad_size)
+        num_orig_input_tokens_list.extend([0] * cuda_graph_pad_size)
+
         input_tokens_tensor = torch.tensor(input_tokens,
                                            dtype=torch.long,
                                            device=self.runner.device)
@@ -519,9 +532,10 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
         # Sequence and query lengths.
         seq_lens.extend([1] * cuda_graph_pad_size)
 
+
         # Attention metadata.
         attn_metadata = self.attn_metadata_builder.build(
-            seq_lens, query_lens, cuda_graph_pad_size, batch_size)
+            seq_lens, query_lens, num_orig_input_tokens_list, cuda_graph_pad_size, batch_size)
 
         # LoRA data.
         lora_requests = set()
@@ -995,6 +1009,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
         slot_mapping = torch.empty(max_batch_size, dtype=torch.long).cuda()
         slot_mapping.fill_(_PAD_SLOT_ID)
         seq_lens = torch.ones(max_batch_size, dtype=torch.int32).cuda()
+        num_orig_input_tokens_tensor = torch.zeros(max_batch_size, dtype=torch.int32).cuda()
         block_tables = torch.from_numpy(self.graph_block_tables).cuda()
         intermediate_inputs = None
         if not get_pp_group().is_first_rank:
@@ -1102,6 +1117,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
                             slot_mapping=slot_mapping[:batch_size],
                             seq_lens=None,
                             seq_lens_tensor=seq_lens[:batch_size],
+                            num_orig_input_tokens_tensor=num_orig_input_tokens_tensor[:batch_size],
                             max_query_len=None,
                             max_prefill_seq_len=0,
                             max_decode_seq_len=self.max_seq_len_to_capture,
@@ -1452,6 +1468,7 @@ class CUDAGraphRunner:
                 "seq_lens_tensor":
                 attn_metadata.decode_metadata.seq_lens_tensor,
                 "block_tables": attn_metadata.decode_metadata.block_tables,
+                "num_orig_input_tokens_tensor": attn_metadata.num_orig_input_tokens_tensor,
                 **kwargs,
             }
         if intermediate_inputs is not None:
@@ -1487,6 +1504,9 @@ class CUDAGraphRunner:
                 non_blocking=True)
             self.input_buffers["block_tables"].copy_(
                 attn_metadata.decode_metadata.block_tables, non_blocking=True)
+            self.input_buffers["num_orig_input_tokens_tensor"].copy_(
+                attn_metadata.num_orig_input_tokens_tensor, non_blocking=True)
+        
         if "seqlen_agnostic_capture_inputs" in self.input_buffers:
             self.model.copy_inputs_before_cuda_graphs(self.input_buffers,
                                                       **kwargs)
