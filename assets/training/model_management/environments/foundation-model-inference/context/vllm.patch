diff --git a/CMakeLists.txt b/CMakeLists.txt
index 1845151..5dc9b82 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -18,6 +18,9 @@ set(PYTHON_SUPPORTED_VERSIONS "3.8" "3.9" "3.10" "3.11")
 # Supported NVIDIA architectures.
 set(CUDA_SUPPORTED_ARCHS "7.0;7.5;8.0;8.6;8.9;9.0")
 
+# Set the path to the CUDA compiler if using a custom one.
+# set(CMAKE_CUDA_COMPILER "/home/user/miniconda3/envs/environment/bin/nvcc")
+
 # Supported AMD GPU architectures.
 set(HIP_SUPPORTED_ARCHS "gfx906;gfx908;gfx90a;gfx940;gfx941;gfx942;gfx1030;gfx1100")
 
diff --git a/SECURITY.md b/SECURITY.md
new file mode 100644
index 0000000..b3c89ef
--- /dev/null
+++ b/SECURITY.md
@@ -0,0 +1,41 @@
+<!-- BEGIN MICROSOFT SECURITY.MD V0.0.9 BLOCK -->
+
+## Security
+
+Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet) and [Xamarin](https://github.com/xamarin).
+
+If you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/security.md/definition), please report it to us as described below.
+
+## Reporting Security Issues
+
+**Please do not report security vulnerabilities through public GitHub issues.**
+
+Instead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/security.md/msrc/create-report).
+
+If you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/security.md/msrc/pgp).
+
+You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc). 
+
+Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:
+
+  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)
+  * Full paths of source file(s) related to the manifestation of the issue
+  * The location of the affected source code (tag/branch/commit or direct URL)
+  * Any special configuration required to reproduce the issue
+  * Step-by-step instructions to reproduce the issue
+  * Proof-of-concept or exploit code (if possible)
+  * Impact of the issue, including how an attacker might exploit the issue
+
+This information will help us triage your report more quickly.
+
+If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/security.md/msrc/bounty) page for more details about our active programs.
+
+## Preferred Languages
+
+We prefer all communications to be in English.
+
+## Policy
+
+Microsoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/security.md/cvd).
+
+<!-- END MICROSOFT SECURITY.MD BLOCK -->
diff --git a/docker/README.md b/docker/README.md
new file mode 100644
index 0000000..cfd2c8c
--- /dev/null
+++ b/docker/README.md
@@ -0,0 +1,14 @@
+## Build
+To build the docker image go the parent folder of the repository and run (assuming the repository folder is called vllm):
+
+```bash
+docker build -t vllm:latest -f vllm/docker/phyagi_vllm.Dockerfile .
+```
+
+## Serve
+
+```bash
+docker run -p 8000:8000 --gpus all --ipc=host --ulimit memlock=-1 -v /local_model:/model vllm:latest python -m vllm.entrypoints.openai.api_server --model /model --trust-remote-code
+```
+
+Change /local_model to the model location.
\ No newline at end of file
diff --git a/docker/phyagi_vllm.Dockerfile b/docker/phyagi_vllm.Dockerfile
new file mode 100644
index 0000000..10a3fb9
--- /dev/null
+++ b/docker/phyagi_vllm.Dockerfile
@@ -0,0 +1,5 @@
+# Use the specified base image
+FROM nvcr.io/nvidia/pytorch:23.10-py3
+
+COPY vllm /phyagi-vllm
+RUN cd /phyagi-vllm && pip install .
\ No newline at end of file
diff --git a/docker/phyagi_vllm_ml.Dockerfile b/docker/phyagi_vllm_ml.Dockerfile
new file mode 100644
index 0000000..6a6196f
--- /dev/null
+++ b/docker/phyagi_vllm_ml.Dockerfile
@@ -0,0 +1,8 @@
+# Use the specified base image
+FROM nvcr.io/nvidia/pytorch:23.10-py3
+
+COPY vllm /phyagi-vllm
+COPY phi-3-3_8b_RC1_30 /phi-3-3_8b_RC1_30
+RUN cd /phyagi-vllm && pip install .
+
+CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--model", "/phi-3-3_8b_RC1_30", "--trust-remote-code"]
\ No newline at end of file
diff --git a/examples/phi_mini_longcontext_example.py b/examples/phi_mini_longcontext_example.py
new file mode 100644
index 0000000..ddecfc3
--- /dev/null
+++ b/examples/phi_mini_longcontext_example.py
@@ -0,0 +1,50 @@
+from vllm import LLM, SamplingParams
+
+prompts = [
+    "Hello, my name is",
+    "The president of the United States is",
+    "The capital of France is",
+    "The future of AI is",
+]
+sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
+
+llm = LLM(model="<model path>",trust_remote_code=True)
+
+outputs = llm.generate(prompts, sampling_params)
+
+# Print the outputs.
+for output in outputs:
+    prompt = output.prompt
+    generated_text = output.outputs[0].text
+    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
+
+
+
+# example amlt_run.yaml
+# description: vllm_phi_debug
+
+# target:
+#   service: sing
+#   name: aims-sing-east-us2
+#   workspace_name: aims-sing-res-eu-WS
+
+# environment:
+#   image: pytorch/pytorch:2.1.2-cuda12.1-cudnn8-devel
+
+# code:
+#   local_dir: ./
+
+# storage:
+#   output:
+#     storage_account_name: genaimsra
+#     container_name: models
+#     mount_dir: /genaimsra/models
+
+# jobs:
+#   - name: debug
+#     sku: G1
+#     command:
+#       - pip install -e .
+#       - python ./test.py
+#     sla_tier: Premium
+#     priority: high
\ No newline at end of file
diff --git a/examples/vllm_phimini_long_prompt.py b/examples/vllm_phimini_long_prompt.py
new file mode 100644
index 0000000..943e58b
--- /dev/null
+++ b/examples/vllm_phimini_long_prompt.py
@@ -0,0 +1,59 @@
+from vllm import LLM, SamplingParams
+import time
+#from transformers import AutoTokenizer
+import random
+import sys
+
+model_path=sys.argv[1]
+prompt_len=int(sys.argv[2])
+
+#model_path='workspace/phyagi-vllm/ckpts/Phi-mini-RC1_44_1L_copy'
+#prompt_len=8192
+
+#Taken from ROPE repo
+def generate_prompt(n_garbage):
+    """Generates a text file and inserts an execute line at a random position."""
+    # n_garbage 15090
+    n_garbage_prefix = random.randint(0, n_garbage) # 10476
+    n_garbage_suffix = n_garbage - n_garbage_prefix # 4614
+
+    task_description = "There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there."
+    garbage = "The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again."
+    garbage_inf = " ".join([garbage] * 10000)
+    assert len(garbage_inf) >= n_garbage
+    garbage_prefix = garbage_inf[:n_garbage_prefix]
+    garbage_suffix = garbage_inf[:n_garbage_suffix]
+    pass_key = random.randint(1, 50000)
+    information_line = f"The pass key is {pass_key}. Remember it. {pass_key} is the pass key."
+    final_question = "What is the pass key? The pass key is"
+    lines = [
+        task_description,
+        garbage_prefix,
+        information_line,
+        garbage_suffix,
+        final_question
+    ]
+    return "\n".join(lines), pass_key
+
+
+
+# Create a sampling params object.
+sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
+
+prompts = generate_prompt(prompt_len)
+# Create an LLM.
+start_time = time.time()
+llm = LLM(model=model_path,trust_remote_code=True, tensor_parallel_size=1,enforce_eager=True)
+
+# Generate texts from the prompts. The output is a list of RequestOutput objects
+outputs = llm.generate(prompts[0],sampling_params)
+print("Prompt len ", len(prompts[0]), "Time(s): ", time.time()-start_time)
+# Print the outputs.
+#'''
+for i, output in enumerate(outputs):
+    if i< 4:   
+        prompt = output.prompt
+        generated_text = output.outputs[0].text
+        #print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
+        print(f"Generated text {i} : {generated_text!r}")
+#'''
diff --git a/vllm/config.py b/vllm/config.py
index 5a29620..afe46e2 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -201,7 +201,14 @@ class ModelConfig:
         if (hasattr(self.hf_text_config, "use_sliding_window")
                 and not self.hf_text_config.use_sliding_window):
             return None
-        return getattr(self.hf_text_config, "sliding_window", None)
+        
+        sliding_window = getattr(self.hf_text_config, "sliding_window", None)
+        if sliding_window is not None and sliding_window % 16 != 0:
+            rounded = round(sliding_window / 16)
+            sliding_window_rounded = int(rounded * 16)
+            logger.warning(f"slinding_window is not multiple of 16, rounding from {sliding_window} to {sliding_window_rounded}.")
+            sliding_window = sliding_window_rounded
+        return sliding_window
 
     def get_vocab_size(self) -> int:
         return self.hf_text_config.vocab_size
@@ -988,12 +995,13 @@ def _get_and_verify_max_len(
 
     rope_scaling = getattr(hf_config, "rope_scaling", None)
     if rope_scaling is not None:
-        assert "factor" in rope_scaling
-        scaling_factor = rope_scaling["factor"]
-        if rope_scaling["type"] == "yarn":
-            derived_max_model_len = rope_scaling[
-                "original_max_position_embeddings"]
-        derived_max_model_len *= scaling_factor
+        if rope_scaling["type"] != "longrope":
+            assert "factor" in rope_scaling
+            scaling_factor = rope_scaling["factor"]
+            if rope_scaling["type"] == "yarn":
+                derived_max_model_len = rope_scaling[
+                    "original_max_position_embeddings"]
+            derived_max_model_len *= scaling_factor
 
     if max_model_len is None:
         max_model_len = int(derived_max_model_len)
diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py
index 6519781..219abbc 100644
--- a/vllm/model_executor/layers/rotary_embedding.py
+++ b/vllm/model_executor/layers/rotary_embedding.py
@@ -337,6 +337,124 @@ class YaRNScalingRotaryEmbedding(RotaryEmbedding):
         return cache
 
 
+class PhiLongScaledRotaryEmbedding(nn.Module):
+
+    def __init__(
+        self,
+        head_size: int,
+        rotary_dim: int,
+        max_position_embeddings: int,
+        original_max_position_embeddings: int,
+        base: int,
+        is_neox_style: bool,
+        short_factor: List[float],
+        long_factor: List[float],
+        short_mscale: float = 1.1,
+        long_mscale: float = 1.225,
+    ):
+        super().__init__()
+
+        self.head_size = head_size
+        self.rotary_dim = rotary_dim
+        self.base = base
+        self.is_neox_style = is_neox_style
+
+        self.max_position_embeddings = max_position_embeddings
+        self.original_max_position_embeddings = original_max_position_embeddings
+        
+        self.short_factor = short_factor
+        self.long_factor = long_factor
+        self.short_mscale = short_mscale
+        self.long_mscale = long_mscale
+
+        short_cache = self._compute_cos_sin_cache(
+            original_max_position_embeddings, short_factor, short_mscale)
+        short_cache = short_cache.to(torch.get_default_dtype())
+        self.register_buffer("short_cos_sin_cache", short_cache, persistent=False)
+
+        long_cache = self._compute_cos_sin_cache(
+            max_position_embeddings, long_factor, long_mscale)
+        long_cache = long_cache.to(torch.get_default_dtype())
+        self.register_buffer("long_cos_sin_cache", long_cache, persistent=False)
+
+        long_short_cache = torch.cat([self.short_cos_sin_cache, 
+                           self.long_cos_sin_cache], dim=0)
+        self.register_buffer("long_short_cos_sin_cache", long_short_cache, persistent=False)
+
+
+    def _compute_inv_freq(self, rescale_factors: List[float]) -> torch.Tensor:
+        rescale_factors = torch.tensor(rescale_factors, dtype=torch.float32)
+        inv_freq = 1.0 / (rescale_factors * (self.base **(torch.arange(
+            0, self.rotary_dim, 2, dtype=torch.float) / self.rotary_dim)))
+        return inv_freq
+
+    def _compute_cos_sin_cache(
+        self,
+        max_position_embeddings: int,
+        rescale_factors: List[float],
+        mscale: float,
+    ) -> torch.Tensor:
+        inv_freq = self._compute_inv_freq(rescale_factors)
+        t = torch.arange(max_position_embeddings, dtype=torch.float)
+        freqs = torch.einsum("i,j -> ij", t, inv_freq)
+        cos = (freqs.cos() * mscale)
+        sin = (freqs.sin() * mscale)
+        cache = torch.cat((cos, sin), dim=-1)
+        return cache
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        query: torch.Tensor,
+        key: torch.Tensor,
+        offsets: Optional[torch.Tensor] = None,
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        # TODO: CUDA kernels for multiple caches
+        query = query.view(*query.shape[:-1], -1, self.head_size)
+        key = key.view(*key.shape[:-1], -1, self.head_size)
+
+        query_rot = query[..., :self.rotary_dim]
+        key_rot = key[..., :self.rotary_dim]
+        if self.rotary_dim < self.head_size:
+            query_pass = query[..., self.rotary_dim:]
+            key_pass = key[..., self.rotary_dim:]
+        
+        # LongRoPE switch logic
+        #For long prompt, offset position by original_max_position_embeddings to index long_cos_sin_cache properly
+        k = self.original_max_position_embeddings
+        long_prompt_offset = (torch.any(positions > k).float() * torch.full_like(positions, k)).long()
+        idx = torch.add(positions, long_prompt_offset) if long_prompt_offset is not None else positions
+        self.long_short_cos_sin_cache = self.long_short_cos_sin_cache.to(idx.device)
+        idx = torch.add(idx, offsets) if offsets is not None else idx
+
+        
+        cos_sin = torch.index_select(self.long_short_cos_sin_cache, 0, idx)
+
+        cos, sin = cos_sin.chunk(2, dim=-1)
+        if self.is_neox_style:
+            # NOTE(woosuk): Here we assume that the positions tensor has the
+            # shape [batch_size, seq_len].
+            cos = cos.repeat(1, 1, 2).unsqueeze(-2)
+            sin = sin.repeat(1, 1, 2).unsqueeze(-2)
+        else:
+            cos = cos.repeat_interleave(2, dim=-1).unsqueeze(-2)
+            sin = sin.repeat_interleave(2, dim=-1).unsqueeze(-2)
+
+        rotate_fn = _rotate_neox if self.is_neox_style else _rotate_gptj
+        query_rot = query_rot * cos + rotate_fn(query_rot) * sin
+        key_rot = key_rot * cos + rotate_fn(key_rot) * sin
+
+        if self.rotary_dim < self.head_size:
+            query = torch.cat((query_rot, query_pass), dim=-1)
+            key = torch.cat((key_rot, key_pass), dim=-1)
+        else:
+            query = query_rot
+            key = key_rot
+        query = query.flatten(0, -2)
+        key = key.flatten(0, -2)
+        return query, key
+
+
 _ROPE_DICT: Dict[Tuple, RotaryEmbedding] = {}
 
 
@@ -349,16 +467,17 @@ def get_rope(
     rope_scaling: Optional[Dict[str, Any]] = None,
 ) -> RotaryEmbedding:
     key = (head_size, rotary_dim, max_position, base, is_neox_style,
-           tuple(rope_scaling.items()) if rope_scaling is not None else None)
+           (v for v in rope_scaling.items() if type(v) is not list)
+           if rope_scaling is not None else None)
     if key in _ROPE_DICT:
         return _ROPE_DICT[key]
-
     if rope_scaling is None:
         rotary_emb = RotaryEmbedding(head_size, rotary_dim, max_position, base,
                                      is_neox_style)
     else:
         scaling_type = rope_scaling["type"]
-        scaling_factor = rope_scaling["factor"]
+        if scaling_type != "longrope":
+            scaling_factor = rope_scaling["factor"]
         if scaling_type == "linear":
             rotary_emb = LinearScalingRotaryEmbedding(head_size, rotary_dim,
                                                       max_position, base,
@@ -382,6 +501,23 @@ def get_rope(
                                                     base, is_neox_style,
                                                     scaling_factor,
                                                     **extra_kwargs)
+        elif scaling_type == "longrope":
+            short_factor = rope_scaling["short_factor"]
+            long_factor = rope_scaling["long_factor"]
+            original_max_position = rope_scaling[
+                "original_max_position_embeddings"]
+            extra_kwargs = {
+                k: v
+                for k, v in rope_scaling.items()
+                if k in ("short_mscale", "long_mscale")
+            }
+            rotary_emb = PhiLongScaledRotaryEmbedding(head_size, rotary_dim,
+                                                      max_position,
+                                                      original_max_position,
+                                                      base, is_neox_style,
+                                                      short_factor,
+                                                      long_factor,
+                                                      **extra_kwargs)
         else:
             raise ValueError(f"Unknown RoPE scaling type {scaling_type}")
     _ROPE_DICT[key] = rotary_emb
diff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py
index 17fc970..861b64b 100755
--- a/vllm/model_executor/models/__init__.py
+++ b/vllm/model_executor/models/__init__.py
@@ -46,6 +46,10 @@ _MODELS = {
     "OPTForCausalLM": ("opt", "OPTForCausalLM"),
     "OrionForCausalLM": ("orion", "OrionForCausalLM"),
     "PhiForCausalLM": ("phi", "PhiForCausalLM"),
+    "Phi3ForCausalLM": ("llama", "LlamaForCausalLM"),
+    "Phi3MiniForCausalLM": ("llama", "LlamaForCausalLM"),
+    "Phi3MediumForCausalLM": ("llama", "LlamaForCausalLM"),
+    "PhiLongRoPEForCausalLM": ("llama", "LlamaForCausalLM"),
     "QWenLMHeadModel": ("qwen", "QWenLMHeadModel"),
     "Qwen2ForCausalLM": ("qwen2", "Qwen2ForCausalLM"),
     "Qwen2MoeForCausalLM": ("qwen2_moe", "Qwen2MoeForCausalLM"),
diff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py
index 016e3b0..65a5deb 100644
--- a/vllm/model_executor/models/llama.py
+++ b/vllm/model_executor/models/llama.py
@@ -180,6 +180,9 @@ class LlamaDecoderLayer(nn.Module):
         self.hidden_size = config.hidden_size
         rope_theta = getattr(config, "rope_theta", 10000)
         rope_scaling = getattr(config, "rope_scaling", None)
+        if getattr(config, "original_max_position_embeddings", None):
+            rope_scaling["original_max_position_embeddings"] = config.original_max_position_embeddings
+
         max_position_embeddings = getattr(config, "max_position_embeddings",
                                           8192)
         sliding_window = getattr(config, "sliding_window", None)
@@ -378,12 +381,18 @@ class LlamaForCausalLM(nn.Module):
     def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
         stacked_params_mapping = [
             # (param_name, shard_name, shard_id)
-            ("qkv_proj", "q_proj", "q"),
-            ("qkv_proj", "k_proj", "k"),
-            ("qkv_proj", "v_proj", "v"),
-            ("gate_up_proj", "gate_proj", 0),
-            ("gate_up_proj", "up_proj", 1),
+            (".qkv_proj", ".q_proj", "q"),
+            (".qkv_proj", ".k_proj", "k"),
+            (".qkv_proj", ".v_proj", "v"),
+            (".gate_up_proj", ".gate_proj", 0),
+            (".gate_up_proj", ".up_proj", 1),
         ]
+
+        phi3_param_mapping = {"fc1": "gate_up_proj",
+                              "fc2": "down_proj",
+                              "dense": "o_proj",
+                              "Wqkv": "qkv_proj",
+                              "final_layernorm": "norm"}
         params_dict = dict(self.named_parameters())
         for name, loaded_weight in weights:
             if "rotary_emb.inv_freq" in name:
@@ -408,6 +417,16 @@ class LlamaForCausalLM(nn.Module):
                 # Skip loading extra bias for GPTQ models.
                 if name.endswith(".bias") and name not in params_dict:
                     continue
+
+                if "fc1" in name:
+                    y, gate = loaded_weight.chunk(2, 0)
+                    loaded_weight = torch.concatenate((gate, y), dim=0)
+
+                for key, value in phi3_param_mapping.items():
+                    if key in name:
+                        name = name.replace(key, value)
+                        break
+
                 param = params_dict[name]
                 weight_loader = getattr(param, "weight_loader",
                                         default_weight_loader)
