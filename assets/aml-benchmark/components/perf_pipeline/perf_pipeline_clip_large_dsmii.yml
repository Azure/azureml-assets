$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json
type: pipeline
name: clip_large_pipeline_dsmii
experiment_name: pipeline_generator_large_dsmii
description: benchmark_clip
jobs:
  endpoint:
    type: pipeline
    component: azureml:batch_benchmark_inference:0.0.5.vision_oss
    inputs:
      input_dataset:
        type: uri_file
        path: azureml:10_samples_large_b64:1
      model_type: vision_oss
      batch_input_pattern: '{
        "input_data": {
          "columns": [
            "image",
            "text"
          ],
          "index": [0],
          "data": [
            ["###<image>", "###<text>"]
          ]
        },
        "params": {}
      }'
      endpoint_url: https://fm-inference-test-clip-large.eastus.inference.ml.azure.com/score
      is_performance_test: False
      deployment_name: openai-clip-vit-large-patch14-2
      connections_name: clip-large-dsmii
      handle_response_failure: use_fallback
      ensure_ascii: false
      initial_worker_count: 1
      max_worker_count: 1
      instance_count: 1
      max_concurrency_per_instance: 1
      debug_mode: true
    outputs:
      predictions:
        type: uri_file
        path: azureml://datastores/${{default_datastore}}/paths/azureml/${{name}}/${{output_name}}.jsonl
      performance_metadata:
        type: uri_file
        path: azureml://datastores/${{default_datastore}}/paths/azureml/${{name}}/${{output_name}}.jsonl
      ground_truth:
        type: uri_file
        path: azureml://datastores/${{default_datastore}}/paths/azureml/${{name}}/${{output_name}}.jsonl
  performance:
    type: command
    compute: azureml:serverless
    component: azureml://registries/azureml/components/compute_performance_metrics/labels/latest
    limits:
      timeout: 900
    inputs:
      performance_data:
        type: uri_folder
        path: ${{parent.jobs.endpoint.outputs.performance_metadata}}
      percentiles: 50,90,99
      batch_size_column_name: batch_size
      start_time_column_name: start_time_iso
      end_time_column_name: end_time_iso
      is_batch_inference_result: true
    outputs:
      performance_result:
        type: uri_file
        path: azureml://datastores/${{default_datastore}}/paths/azureml/${{name}}/${{output_name}}.json
  aggregator:
    type: command
    compute: azureml:serverless
    component: azureml://registries/azureml/components/benchmark_result_aggregator/labels/latest
    limits:
      timeout: 900
    inputs:
      performance_metrics:
        type: uri_folder
        path: ${{parent.jobs.performance.outputs.performance_result}}
    outputs:
      benchmark_result:
        type: uri_file
        path: azureml://datastores/${{default_datastore}}/paths/azureml/${{name}}/${{output_name}}.json
tags:
  workflow: llm_benchmark
  run_type: private
properties:
  _azureml.evaluation_run: Benchmark
settings:
  continue_on_step_failure: true
  force_rerun: false
  default_compute: azureml:serverless
