$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
type: command

name: benchmark_result_aggregator
display_name: Benchmark result aggregator
description: Aggregate quality metrics, performance metrics and all of the metadata from the pipeline. Also add them to the root run.
<<<<<<< HEAD
version: 0.0.5
=======
version: 0.0.8
>>>>>>> 7a54b91f3a492ed00e3033a99450bbc4df36a0fa
is_deterministic: false

inputs:
  quality_metrics:
    type: uri_folder
    description: The quality metrics in json format.
    optional: True
  performance_metrics:
    type: uri_folder
    description: The performance metrics in json format.
    optional: True

outputs:
  benchmark_result:
    type: uri_file
    description: The json file with all of the aggregated results.

code: ../src
<<<<<<< HEAD
environment: azureml://registries/azureml/environments/model-evaluation/versions/20
=======
environment: azureml://registries/azureml/environments/model-evaluation/versions/25
>>>>>>> 7a54b91f3a492ed00e3033a99450bbc4df36a0fa
command: >-
  python -m aml_benchmark.benchmark_result_aggregator.main 
  $[[--quality_metrics_path ${{inputs.quality_metrics}}]] 
  $[[--performance_metrics_path ${{inputs.performance_metrics}}]] 
  --output_dataset_path ${{outputs.benchmark_result}}
