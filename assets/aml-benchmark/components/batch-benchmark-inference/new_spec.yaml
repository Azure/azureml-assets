$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
type: pipeline

name: endpoint_batch_inference
display_name: Endpoint Batch Inference
description: Components for batch endpoint inference
version: 0.0.1
is_deterministic: false

inputs:
  input_dataset: 
    type: uri_folder
    description: Input jsonl dataset that contains prompt.
    optional: False
  batch_input_pattern:
    type: string
    description: The string for the batch input pattern.
    optional: False
  online_endpoint_url:
    type: string
    optional: False
  debug_mode:
    type: boolean
    optional: False
    default: False
  additional_headers:
    type: string
    optional: True
    description: A stringified json expressing additional headers to be added to each request.
  ensure_ascii:
    type: boolean
    optional: False
    default: False
    description: If ensure_ascii is true, the output is guaranteed to have all incoming non-ASCII characters escaped. If ensure_ascii is false, these characters will be output as-is. More defailted information can be found at https://docs.python.org/3/library/json.html
  max_retry_time_interval:
    type: integer
    optional: True
    description: The maximum time (in seconds) spent retrying a payload. If unspecified, payloads are retried unlimited times.
  initial_worker_count:
    type: integer
    optional: False
    default: 5
  max_worker_count:
    type: integer
    optional: False
    default: 200
    description: Overrides initial_worker_count if necessary
  instance_count:
    type: integer
    default: 1
    description: 'Number of nodes in a compute cluster we will run the train step on.'
  max_concurrency_per_instance:
    type: integer
    default: 1
    description: 'Number of processes that will be run concurrently on any given node. This number should not be larger than 1/2 of the number of cores in an individual node in the specified cluster'
  ground_truth_input:
    type: uri_folder
    optional: True
  metadata_key:
    type: string
    optional: True
    default: 'request_metadata'
  label_key:
    type: string
    optional: False
  n_samples:
    type: integer
    description: The number of top samples send to endpoint.
    optional: True
outputs:
  prediction_data:
    type: uri_file
  perf_data:
    type: uri_file
  predict_ground_truth_data:
    type: uri_file
  

jobs:
  # Preparer
  batch_endpoint_preparer: 
    type: command
    component: ../batch-endpoint-preparer/spec.yaml
    inputs:
      input_dataset: ${{parent.inputs.input_dataset}}
      model_type: 'llama'
      batch_input_pattern: ${{parent.inputs.batch_input_pattern}}
      n_samples: ${{parent.inputs.n_samples}}
    outputs:
      formatted_data:
        type: mltable
  # Inference
  real_point_batch_inference: 
    type: parallel
    component: ../batch-benchmark-score/spec.yaml
    inputs:
      online_endpoint_url: ${{parent.inputs.online_endpoint_url}}
      debug_mode: ${{parent.inputs.debug_mode}}
      additional_headers: ${{parent.inputs.additional_headers}}
      ensure_ascii: ${{parent.inputs.ensure_ascii}}
      max_retry_time_interval: ${{parent.inputs.max_retry_time_interval}}
      initial_worker_count: ${{parent.inputs.initial_worker_count}}
      max_worker_count: ${{parent.inputs.max_worker_count}}
      data_input_table: ${{parent.jobs.batch_endpoint_preparer.outputs.formatted_data}}
    outputs:
      job_out_path:
        type: uri_file
      mini_batch_results_out_directory:
        type: uri_folder
      metrics_out_directory:
        type: uri_folder
    resources:
      instance_count: ${{parent.inputs.instance_count}}
    max_concurrency_per_instance:  ${{parent.inputs.max_concurrency_per_instance}}
    mini_batch_size: "3072"
    retry_settings:
      timeout: 6000
      max_retries: 10
  # Reformat
  batch_output_formatter: 
    type: command
    component: ../batch-output-formatter/spec.yaml
    inputs:
      batch_inference_output: ${{parent.jobs.real_point_batch_inference.outputs.mini_batch_results_out_directory}}
      ground_truth_input: ${{parent.inputs.ground_truth_input}}
      model_type: 'llama'
      metadata_key: ${{parent.inputs.metadata_key}}
      label_key: ${{parent.inputs.label_key}}
    outputs:
      prediction_data:
        type: uri_file
        path: ${{parent.outputs.prediction_data}}
      perf_data:
        type: uri_file
        path: ${{parent.outputs.perf_data}}
      predict_ground_truth_data:
        type: uri_file
        path: ${{parent.outputs.predict_ground_truth_data}}
