name: batch_output_formatter
version: 0.0.1
display_name: Batch Output Formatter
is_deterministic: True
type: command
description: Output Formatter for batch inference output
inputs:
  batch_inference_output:
    type: uri_folder
    description: The raw batch inference output.
    optional: False
  metadata_key:
    type: string
    optional: True
    description: The metadata key that stores ground truth `label_key` in the response. If using azureml llama model and pass `_batch_request_metadata`, then this one can left empty or using `request_metadata`.
  label_key:
    type: string
    optional: False
    description: The key that stores the ground truth value.
  data_id_key:
    type: string
    optional: True
  model_type:
    type: string
    optional: False
    description: The endpoint model type.
    enum:
      - oss
      - oai
  ground_truth_input:
    type: uri_folder
    description: The raw batch inference output.
    optional: True
  delete_managed_resources:
    type: boolean
    optional: True
    default: True
    description: If true, the managed resources created during the run will be deleted.
  deployment_metadata_dir:
    type: uri_folder
    optional: True
    description: The deployment metadata directory that contains deployment details.
  handle_response_failure:
    type: string
    optional: False
    description: The way that the formatter handles the failed response.
    enum:
      - use_fallback
      - neglect
    default: use_fallback
  fallback_value:
    description: The fallback value that can be used when request payload failed.
    type: string
    optional: True
  is_performance_test:
    type: boolean
    default: False
    description: If true, the performance test will be run.
outputs:
  predictions:
    type: uri_file
  performance_metadata:
    type: uri_file
  ground_truth:
    type: uri_file
code: ../src
environment: azureml://registries/azureml/environments/model-evaluation/labels/latest

resources:
  instance_count: 1

command: >-
  python -m batch_output_formatter.main
  --batch_inference_output ${{inputs.batch_inference_output}}
  --prediction_data ${{outputs.predictions}}
  --perf_data ${{outputs.performance_metadata}}
  --predict_ground_truth_data ${{outputs.ground_truth}}
  --label_key ${{inputs.label_key}}
  --model_type ${{inputs.model_type}}
  --handle_response_failure ${{inputs.handle_response_failure}}
  --is_performance_test ${{inputs.is_performance_test}}
  $[[--metadata_key ${{inputs.metadata_key}}]]
  $[[--data_id_key ${{inputs.data_id_key}}]]
  $[[--fallback_value ${{inputs.fallback_value}}]]
  $[[--ground_truth_input ${{inputs.ground_truth_input}}]]
  $[[--delete_managed_resources ${{inputs.delete_managed_resources}}]]
  $[[--deployment_metadata_dir ${{inputs.deployment_metadata_dir}}]]
