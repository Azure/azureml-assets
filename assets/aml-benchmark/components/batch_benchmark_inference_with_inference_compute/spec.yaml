$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
type: pipeline

name: batch_benchmark_inference_with_inference_compute
display_name: Batch Benchmark Inference (With Inference compute Support)
description: Components for batch endpoint inference with inference compute support.
version: 0.0.7

inputs:
  input_dataset:
    type: uri_folder
    description: Input jsonl dataset that contains prompt.  For the performance test, this one will be neglected.
    optional: True
  model_type:
    type: string
    description: Type of model's input and output contract. Can be one of ('oai', 'oss', 'vision_oss')
    optional: False
    enum:
        - oai
        - oss
        - vision_oss
  inference_compute:
    type: string
    description: Compute to be used for inferencing.
    optional: False
  batch_input_pattern:
    type: string
    description: >- 
      The string for the batch input pattern. The input should be the payload format with substitution
      for the key for the value put in the `###<key>`. For example, one can use the following format for
      a llama text-gen model with a input dataset has `prompt` for the payload
      and `_batch_request_metadata` storing the corresponding ground truth.
      {
        "input_data":
        {
          "input_string": ["###<prompt>"],
          "parameters":
          {
            "temperature": 0.6,
            "max_new_tokens": 100,
            "do_sample": true
          }
        },
        "_batch_request_metadata": ###<_batch_request_metadata>
      }

      For AOAI chat completion model, the following pattern can be used,
      {
          "messages": ###<prompt>,
            "temperature": 0.7,
            "top_p": 0.95,
            "frequency_penalty": 0,
            "presence_penalty": 0,
            "max_tokens": 800,
            "stop": null
      }
    optional: False
  endpoint_url:
    type: string
    optional: False
    description: The URL of the endpoint.
  is_performance_test:
    type: boolean
    default: False
    description: If true, the performance test will be run and the input dataset will be neglected.
  use_tiktoken:
    type: boolean
    default: False
    description: If true, `cl100k_base` encoder is used from tiktoken to calculate token count; overrides any other token count calculation.
    optional: True
  authentication_type:
    type: string
    optional: False
    description: Authentication type for endpoint- azureml_workspace_connection or managed_identity.
    default: azureml_workspace_connection
    enum:
      - azureml_workspace_connection
      - managed_identity
  deployment_name:
    type: string
    optional: True
    description: The deployment name. Only needed for managed OSS deployment.
  connections_name:
    type: string
    optional: True
    description: Connections name for the endpoint. Only required if authentication_type is "azureml_workspace_connection".
  label_column_name:
    type: string
    optional: True
    description: The label column name.
  additional_columns:
    type: string
    optional: True
    description: The name(s) for additional columns that could be helpful to calculate some metrics, separated by comma (",").
  n_samples:
    type: integer
    description: The number of top samples send to endpoint. When performance test is enabled, this will be the number of repeated samples send to the endpoint.
    optional: True
  handle_response_failure:
    type: string
    optional: False
    description: The way that the formatter handles the failed response. 'use_fallback' will replace them with fallback_value and 'neglect' will drop those rows.
    enum:
      - use_fallback
      - neglect
    default: use_fallback
  fallback_value:
    description: The fallback value that can be used when request payload failed. If not provided, the fallback value will be an empty string.
    type: string
    optional: True
  min_endpoint_success_ratio:
    type: number
    description: The minimum value of (successful_requests / total_requests) required for classifying inference as successful. If (successful_requests / total_requests) < min_endpoint_success_ratio, the experiment will be marked as failed. By default it is 0. (0 means all requests are allowed to fail while 1 means no request should fail.)
    min: 0
    max: 1
    default: 0
    optional: False
  additional_headers:
    type: string
    optional: True
    description: A stringified json expressing additional headers to be added to each request.
  ensure_ascii:
    type: boolean
    optional: False
    default: False
    description: If ensure_ascii is true, the output is guaranteed to have all incoming non-ASCII characters escaped. If ensure_ascii is false, these characters will be output as-is. More detailed information can be found at https://docs.python.org/3/library/json.html
  max_retry_time_interval:
    type: integer
    optional: True
    description: The maximum time (in seconds) spent retrying a payload. If unspecified, payloads are retried unlimited times.
  mini_batch_size:
    type: string
    optional: true
    default: 100KB
    description: The mini batch size for parallel run.
  endpoint_config_file:
    type: uri_file
    optional: True
    description: The endpoint config file.
  initial_worker_count:
    type: integer
    optional: False
    default: 5
    description: The initial number of workers to use for scoring.
  max_worker_count:
    type: integer
    optional: False
    default: 200
    description: Overrides initial_worker_count if necessary
  instance_count:
    type: integer
    default: 1
    description: 'Number of nodes in a compute cluster we will run the train step on.'
  max_concurrency_per_instance:
    type: integer
    default: 1
    description: Number of processes that will be run concurrently on any given node. This number should not be larger than 1/2 of the number of cores in an individual node in the specified cluster.
  debug_mode:
    type: boolean
    optional: False
    default: False
    description: Enable debug mode will print all the debug logs in the score step.
  app_insights_connection_string:
    type: string
    optional: True
    description: Application insights connection string where the batch score component will log metrics and logs.
outputs:
  predictions:
    type: uri_file
    description: The prediction data.
  performance_metadata:
    type: uri_file
    description: The performance data.
  ground_truth:
    type: uri_file
    description: The ground truth data that has a one-to-one mapping with the prediction data.
  successful_requests:
    type: uri_file
    description: The successful requests.
  failed_requests:
    type: uri_file
    description: The failed requests.
  unsafe_content_blocked_requests:
    type: uri_file
    description: The unsafe requests that were blocked due to Responsible AI concerns.
jobs:
  # Preparer
  batch_inference_preparer: 
    type: command
    component: azureml:batch_inference_preparer:0.0.13
    inputs:
      input_dataset: ${{parent.inputs.input_dataset}}
      model_type: ${{parent.inputs.model_type}}
      batch_input_pattern: ${{parent.inputs.batch_input_pattern}}
      is_performance_test: ${{parent.inputs.is_performance_test}}
      n_samples: ${{parent.inputs.n_samples}}
      endpoint_url: ${{parent.inputs.endpoint_url}}
      label_column_name: ${{parent.inputs.label_column_name}}
      additional_columns: ${{parent.inputs.additional_columns}}
    outputs:
      formatted_data:
        type: mltable
      ground_truth_metadata:
        type: uri_folder
  # Config generation
  config_generation:
    type: command
    component: azureml:batch_benchmark_config_generator:0.0.7
    inputs:
      scoring_url: ${{parent.inputs.endpoint_url}}
      connection_name: ${{parent.inputs.connections_name}}
      deployment_name: ${{parent.inputs.deployment_name}}
      authentication_type: ${{parent.inputs.authentication_type}}
      debug_mode: ${{parent.inputs.debug_mode}}
      additional_headers: ${{parent.inputs.additional_headers}}
      ensure_ascii: ${{parent.inputs.ensure_ascii}}
      max_retry_time_interval: ${{parent.inputs.max_retry_time_interval}}
      initial_worker_count: ${{parent.inputs.initial_worker_count}}
      max_worker_count: ${{parent.inputs.max_worker_count}}
      configuration_file: ${{parent.inputs.endpoint_config_file}}
      model_type: ${{parent.inputs.model_type}}
      app_insights_connection_string: ${{parent.inputs.app_insights_connection_string}}
    outputs:
      batch_score_config:
        type: uri_file
        path: azureml://datastores/${{default_datastore}}/paths/azureml/${{name}}/${{output_name}}.json
  # Inference
  endpoint_batch_score:
    type: parallel
    component: azureml://registries/azureml/components/batch_score_llm/versions/1.1.9
    inputs:
      async_mode: False
      data_input_table: ${{parent.jobs.batch_inference_preparer.outputs.formatted_data}}
      configuration_file: ${{parent.jobs.config_generation.outputs.batch_score_config}}
    outputs:
      job_output_path:
        type: uri_file
      mini_batch_results_output_directory:
        type: uri_folder
    resources:
      instance_count: ${{parent.inputs.instance_count}}
    max_concurrency_per_instance:  ${{parent.inputs.max_concurrency_per_instance}}
    mini_batch_size: ${{parent.inputs.mini_batch_size}}
    retry_settings:
      timeout: 6000
      max_retries: 10
    compute: ${{parent.inputs.inference_compute}}
    environment_variables:
      BATCH_SCORE_INITIAL_REQUEST_TIMEOUT: '180'
      BATCH_SCORE_DELAY_AFTER_SUCCESSFUL_REQUEST: 'False'
      BATCH_SCORE_MAX_REQUEST_TIMEOUT: '300'
      
  # Reformat
  batch_output_formatter: 
    type: command
    component: azureml:batch_output_formatter:0.0.13
    inputs:
      model_type: ${{parent.inputs.model_type}}
      batch_inference_output: ${{parent.jobs.endpoint_batch_score.outputs.mini_batch_results_output_directory}}
      label_column_name: ${{parent.inputs.label_column_name}}
      additional_columns: ${{parent.inputs.additional_columns}}
      ground_truth_input: ${{parent.jobs.batch_inference_preparer.outputs.ground_truth_metadata}}
      fallback_value: ${{parent.inputs.fallback_value}}
      handle_response_failure: ${{parent.inputs.handle_response_failure}}
      is_performance_test: ${{parent.inputs.is_performance_test}}
      use_tiktoken: ${{parent.inputs.use_tiktoken}}
      endpoint_url: ${{parent.inputs.endpoint_url}}
      min_endpoint_success_ratio: ${{parent.inputs.min_endpoint_success_ratio}}
    outputs:
      predictions:
        type: uri_file
        path: ${{parent.outputs.predictions}}
      performance_metadata:
        type: uri_file
        path: ${{parent.outputs.performance_metadata}}
      ground_truth:
        type: uri_file
        path: ${{parent.outputs.ground_truth}}
      successful_requests:
        type: uri_file
        path: ${{parent.outputs.successful_requests}}
      failed_requests:
        type: uri_file
        path: ${{parent.outputs.failed_requests}}
      unsafe_content_blocked_requests:
        type: uri_file
        path: ${{parent.outputs.unsafe_content_blocked_requests}}
