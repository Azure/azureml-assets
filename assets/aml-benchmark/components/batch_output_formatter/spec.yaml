name: batch_output_formatter
version: 0.0.15
display_name: Batch Output Formatter
is_deterministic: True
type: command
description: Output Formatter for batch inference output
inputs:
  model_type:
    type: string
    description: Type of model. Can be one of ('oai', 'oss', 'vision_oss', 'claude')
    optional: True
  batch_inference_output:
    type: uri_folder
    description: The raw batch inference output.
    optional: False
  label_column_name:
    type: string
    optional: True
    description: The label column name.
  additional_columns:
    type: string
    optional: True
    description: Name(s) of additional column(s) that could be useful to compute metrics, separated by comma (",").
  endpoint_url:
    type: string
    optional: True
  ground_truth_input:
    type: uri_folder
    description: The raw batch inference output.
    optional: True
  handle_response_failure:
    type: string
    optional: False
    description: The way that the formatter handles the failed response.
    enum:
      - use_fallback
      - neglect
    default: use_fallback
  fallback_value:
    description: The fallback value that can be used when request payload failed.
    type: string
    optional: True
  min_endpoint_success_ratio:
    description: The minimum value of (successful_requests / total_requests) required for classifying inference as successful. If (successful_requests / total_requests) < min_endpoint_success_ratio, the experiment will be marked as failed. By default it is 0. (0 means all requests are allowed to fail while 1 means no request should fail.)
    type: number
    min: 0
    max: 1
    default: 0
    optional: False
  is_performance_test:
    type: boolean
    default: False
    description: If true, the performance test will be run.
    optional: False
  use_tiktoken:
    type: boolean
    default: False
    description: If true, `cl100k_base` encoder is used from tiktoken to calculate token count; overrides any other token count calculation.
    optional: True
outputs:
  predictions:
    type: uri_file
  performance_metadata:
    type: uri_file
  ground_truth:
    type: uri_file
  successful_requests:
    type: uri_file
  failed_requests:
    type: uri_file
  unsafe_content_blocked_requests:
    type: uri_file
code: ../src
environment: azureml://registries/azureml/environments/evaluation/labels/latest

resources:
  instance_count: 1

command: >-
  python -m aml_benchmark.batch_output_formatter.main
  $[[--model_type ${{inputs.model_type}}]]
  --batch_inference_output ${{inputs.batch_inference_output}}
  --prediction_data ${{outputs.predictions}}
  --min_endpoint_success_ratio ${{inputs.min_endpoint_success_ratio}}
  --perf_data ${{outputs.performance_metadata}}
  --successful_requests_data ${{outputs.successful_requests}}
  --failed_requests_data ${{outputs.failed_requests}}
  --blocked_requests_data ${{outputs.unsafe_content_blocked_requests}}
  --predict_ground_truth_data ${{outputs.ground_truth}}
  $[[--endpoint_url ${{inputs.endpoint_url}}]]
  $[[--label_key ${{inputs.label_column_name}}]]
  $[[--additional_columns ${{inputs.additional_columns}}]]
  --handle_response_failure ${{inputs.handle_response_failure}}
  --is_performance_test ${{inputs.is_performance_test}}
  $[[--use_tiktoken ${{inputs.use_tiktoken}}]]
  $[[--fallback_value ${{inputs.fallback_value}}]]
  $[[--ground_truth_input ${{inputs.ground_truth_input}}]]
