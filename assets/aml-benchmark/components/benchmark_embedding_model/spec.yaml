$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
type: command

name: benchmark_embedding_model
display_name: Benchmark Embedding Model
description: Component for benchmarking an embedding model via MTEB.
version: 0.0.1

inputs:
  endpoint_url:
    type: string
    optional: True
    description: For AOAI, the base endpoint url. For OAI, this will be ignored. For OSS_MaaS, the target url.
  deployment_type:
    type: string
    optional: False
    description: The deployment type.
    enum:
      - AOAI
      - OAI
      - OSS_MaaS
  deployment_name:
    type: string
    optional: True
    description: For AOAI, the deployment name. For OAI, the model name. For OSS_MaaS, this wil be ignored. 
  connections_name:
    type: string
    optional: False
    description: Used for authenticating endpoint.
  tasks:
    type: string
    optional: True
    description: Comma separated string denoting the tasks to benchmark the model on.
  task_types:
    type: string
    optional: True
    description: >-
      Comma separated string denoting the task type to benchmark the model on. Choose from
      the following task types: BitextMining, Classification, Clustering, PairClassification, Reranking,
      Retrieval, STS, Summarization.
  task_langs:
    type: string
    optional: True
    description: Comma separated string denoting the task languages to benchmark the model on.
  preset:
    type: string
    optional: False
    description: Choose from one of the presets for benchmarking.
    enum:
      - None
      - mteb_main_en
    default: None

outputs:
  metrics:
    type: uri_folder
    description: Directory where the benchmark metrics will be saved.

code: ../src
environment: azureml://registries/azureml-preview-test1/environments/benchmark-embedding-model/versions/0.1
command: >-
  python -m aml_benchmark.benchmark_embedding_model.main
  $[[--endpoint_url ${{inputs.endpoint_url}}]]
  --deployment_type ${{inputs.deployment_type}}
  $[[--deployment_name ${{inputs.deployment_name}}]]
  --connections_name '${{inputs.connections_name}}'
  $[[--tasks '${{inputs.tasks}}']]
  $[[--task_types '${{inputs.task_types}}']]
  $[[--task_langs '${{inputs.task_langs}}']]
  --preset ${{inputs.preset}}
  --output_metrics_dir ${{outputs.metrics}}
