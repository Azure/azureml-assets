$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
type: command

name: batch_endpoint_preparer
display_name: Prepare Batch Inference
description: Prepare the jsonl file and endpoint for batch inference component.
version: 0.0.1

inputs:
  input_dataset: 
    type: uri_folder
    description: Input jsonl dataset that contains prompt.
    optional: False
  batch_input_pattern:
    type: string
    description: >- 
      The string for the batch input pattern. The input should be the payload format with substitution
      for the key for the value put in the `###<key>`. For example, one can use the following format for
      a llama text-gen model with a input dataset has `prompt` for the payload
      and `_batch_request_metadata` storing the corresponding ground truth.
      {"input_data": 
        {
          "input_string": ["###<prompt>"],
          "parameters":
          {
            "temperature": 0.6,
            "max_new_tokens": 100,
            "do_sample": true
          }
        },
        "_batch_request_metadata": ###<_batch_request_metadata>
      }
    optional: False
outputs:
  formatted_data:
    type: mltable
    description: Path to the folder where the payload will be stored.

code: ../src
environment: azureml://registries/azureml/environments/model-evaluation/labels/latest
command: >-
  python -m batch_inference_preparer.main
  --input_dataset ${{inputs.input_dataset}}
  --model_type llama
  --batch_input_pattern '${{inputs.batch_input_pattern}}'
  --formatted_data ${{outputs.formatted_data}}
