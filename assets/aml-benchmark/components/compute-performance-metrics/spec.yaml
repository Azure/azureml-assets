$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
type: command

name: compute_performance_metrics
display_name: Compute Performance Metrics
description: Performs performance metric post processing using data from a model inference run.
version: 0.0.1

inputs:
  performance_data: 
    type: uri_folder
    description: Data outputted by model inferencing that contains performance data.
    optional: False
  percentiles:
    type: string
    description: Comma-separated list of percentiles of latency to be calculated.
    optional: True
    default: "50,90,99"
  batch_size_column_name:
    type: string
    description: The name of the column that contains the batch size information. Ex. "batch_size"
    optional: False
  start_time_column_name:
    type: string
    description: The name of the column that contains the start timestamp in ISO 8601 format. Ex. "start_time_iso"
    optional: False
  end_time_column_name:
    type: string
    description: The name of the column that contains the end timestamp in ISO 8601 format. Ex. "end_time_iso"
    optional: False
  input_token_count_column_name:
    type: string
    description: The name of the column that contains the input token count information. Ex. "input_token_count"
    optional: True
  output_token_count_column_name:
    type: string
    description: The name of the column that contains the output token count information. Ex. "output_token_count"
    optional: True
  input_char_count_column_name:
    type: string
    description: The name of the column that contains the input character count information. Ex. "input_char_count"
    optional: True
  output_char_count_column_name:
    type: string
    description: The name of the column that contains the output character count information. Ex. "output_char_count"
    optional: True
  is_batch_inference_result:
    type: boolean
    description: If True, we will use the time between the first and last request to calculate the tokens per second and request per second. If False, we will use individual request time to calculate the tokens per second and request per second.
    optional: False
    default: True

outputs:
  performance_result:
    type: uri_file
    description: Path to the file where the calculated performance metric results will be stored.

code: ../src
environment: azureml://registries/azureml/environments/model-evaluation/labels/latest
command: >-
  python -m aml_benchmark.perf_metrics.main
  --performance_data ${{inputs.performance_data}}
  --batch_size_column_name ${{inputs.batch_size_column_name}}
  --start_time_column_name ${{inputs.start_time_column_name}}
  --end_time_column_name ${{inputs.end_time_column_name}}
  --performance_result ${{outputs.performance_result}}
  --is_batch_inference_result ${{inputs.is_batch_inference_result}}
  $[[--percentiles ${{inputs.percentiles}}]]
  $[[--input_token_count_column_name ${{inputs.input_token_count_column_name}}]]
  $[[--output_token_count_column_name ${{inputs.output_token_count_column_name}}]]
  $[[--input_char_count_column_name ${{inputs.input_char_count_column_name}}]]
  $[[--output_char_count_column_name ${{inputs.output_char_count_column_name}}]]