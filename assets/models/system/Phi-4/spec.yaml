$schema: https://azuremlschemas.azureedge.net/latest/model.schema.json

name: Phi-4
path: ./

properties:
  inference-min-sku-spec: 24|1|220|64
  inference-recommended-sku: Standard_NC24ads_A100_v4, Standard_NC48ads_A100_v4, Standard_NC96ads_A100_v4, Standard_ND96asr_v4, Standard_ND96amsr_A100_v4
  languages: en
  SharedComputeCapacityEnabled: true

tags:
  author: Microsoft
  SharedComputeCapacityEnabled: ""
  disable-batch: true
  freePlayground: "false"
  Preview: ""
  Featured: ""
  languages: "en,ar,bn,cs,da,de,el,es,fa,fi,fr,gu,ha,he,hi,hu,id,it,ja,jv,kn,ko,ml,mr,nl,no,or,pa,pl,ps,pt,ro,ru,sv,sw,ta,te,th,tl,tr,uk,ur,vi,yo,zh"
  keywords: "Conversation"
  inference_supported_envs:
    - vllm
  license: MSRLA
  task: chat-completion
  displayName: "Phi-4"
  summary: "Phi-4 14B, a highly capable model for low latency scenarios."
  textContextWindow: 16384
  maxOutputTokens: 16384
  inputModalities: "text"
  outputModalities: "text"
  evaluation: "evaluation.md"
  trainingDataDate: "June 2024"
  notes: "notes.md"
  hiddenlayerscanned: ""
  maas-inference: "true"
  inference_compute_allow_list:
    [
      Standard_NC24ads_A100_v4,
      Standard_NC48ads_A100_v4,
      Standard_NC96ads_A100_v4,
      Standard_ND96asr_v4,
      Standard_ND96amsr_A100_v4
    ]
  benchmark: "quality"

version: 1
