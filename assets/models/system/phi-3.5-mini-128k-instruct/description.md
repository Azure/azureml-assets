Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.

### Resources
ğŸ¡ [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>
ğŸ“° [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>
ğŸ“– [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>
ğŸ‘©â€ğŸ³ [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>

### Model Architecture
Phi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini. It is a text-only model best suited for prompts using chat format.

### Training Data
Phi-3.5-mini is a static model trained on an offline dataset with 3.4T tokens and a cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.