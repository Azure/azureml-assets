BERT-Large-Cased is a variant of the BERT (Bidirectional Encoder Representations from Transformers) model, developed by Google. Unlike the "bert-base-uncased" version, "bert-large-cased" uses a larger architecture with more parameters, making it more powerful but also computationally more intensive. The "cased" tokenization means that the model retains the original case of the text during preprocessing.

BERT-Large-Cased is particularly well-suited for complex natural language processing tasks that require a deeper understanding of context and semantics. It provides even better performance and higher accuracy compared to "bert-base-uncased" but may require more computational resources for training and inference.

Due to its larger size and increased computational requirements, BERT-Large-Cased is often used in scenarios where high-quality language representations are crucial and the computational cost can be justified. It is commonly employed in research, large-scale language understanding tasks, and applications demanding advanced NLP capabilities.
<br>Please Note: This model accepts masks in `[mask]` format. See Sample input for reference.Â 
> The above summary was generated using ChatGPT. Review the <a href="https://huggingface.co/bert-large-cased" target="_blank">original model card</a> to understand the data used to train the model, evaluation metrics, license, intended uses, limitations and bias before using the model.

BERT-Large, Cased (Whole Word Masking): 24-layer, 1024-hidden, 16-heads, 340M parameters

|      Model                             | SQUAD 1.1 F1/EM  | Multi NLI Accuracy  |
|----------------------------------------|------------------|---------------------|
|BERT-Large, Cased (Original)            |   91.5/84.8      |  86.09              |
|BERT-Large, Cased (Whole Word Masking)  |   92.9/86.7      | 86.46               |


### Inference samples

Inference type|Python sample (Notebook)|CLI with YAML
|--|--|--|
Real time|<a href="https://aka.ms/azureml-infer-online-sdk-fill-mask" target="_blank">fill-mask-online-endpoint.ipynb</a>|<a href="https://aka.ms/azureml-infer-online-cli-fill-mask" target="_blank">fill-mask-online-endpoint.sh</a>
Batch |<a href="https://aka.ms/azureml-infer-batch-sdk-fill-mask" target="_blank">fill-mask-batch-endpoint.ipynb</a>| coming soon


### Finetuning samples

Task|Use case|Dataset|Python sample (Notebook)|CLI with YAML
|--|--|--|--|--|
Text Classification|Emotion Detection|<a href="https://huggingface.co/datasets/dair-ai/emotion" target="_blank">Emotion</a>|<a href="https://aka.ms/azureml-ft-sdk-emotion-detection" target="_blank">emotion-detection.ipynb</a>|<a href="https://aka.ms/azureml-ft-cli-emotion-detection" target="_blank">emotion-detection.sh</a>
Token Classification|Named Entity Recognition|<a href="https://huggingface.co/datasets/conll2003" target="_blank">Conll2003</a>|<a href="https://aka.ms/azureml-ft-sdk-token-classification" target="_blank">named-entity-recognition.ipynb</a>|<a href="https://aka.ms/azureml-ft-cli-token-classification" target="_blank">named-entity-recognition.sh</a>
Question Answering|Extractive Q&A|<a href="https://huggingface.co/datasets/squad" target="_blank">SQUAD (Wikipedia)</a>|<a href="https://aka.ms/azureml-ft-sdk-extractive-qa" target="_blank">extractive-qa.ipynb</a>|<a href="https://aka.ms/azureml-ft-cli-extractive-qa" target="_blank">extractive-qa.sh</a>


### Model Evaluation

Task|Use case|Python sample (Notebook)|CLI with YAML
|--|--|--|--|
Fill Mask|Fill Mask|<a href="https://huggingface.co/datasets/rcds/wikipedia-for-mask-filling" target="_blank">rcds/wikipedia-for-mask-filling</a>|<a href="https://aka.ms/azureml-eval-sdk-fill-mask/" target="_blank">evaluate-model-fill-mask.ipynb</a>|<a href="https://aka.ms/azureml-eval-cli-fill-mask/" target="_blank">evaluate-model-fill-mask.yml</a>


### Sample inputs and outputs (for real-time inference)

#### Sample input
```json
{
    "inputs": {
        "input_string": ["Paris is the [MASK] of France.", "Today is a [MASK] day!"]
    }
}
```

#### Sample output
```json
[
    {
        "0": "capital"
    },
    {
        "0": "beautiful"
    }
]
```
