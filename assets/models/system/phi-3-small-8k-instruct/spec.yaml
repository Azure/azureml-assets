$schema: https://azuremlschemas.azureedge.net/latest/model.schema.json
name: Phi-3-small-8k-instruct
path: ./
properties:
  SharedComputeCapacityEnabled: true
  languages: en
  inference-min-sku-spec: 24|1|220|64
  inference-recommended-sku: Standard_NC24ads_A100_v4, Standard_NC48ads_A100_v4, Standard_NC96ads_A100_v4, Standard_ND96asr_v4, Standard_ND96amsr_A100_v4
  finetuning-tasks: chat-completion
  finetune-min-sku-spec: 96|4|880|256
  finetune-recommended-sku: Standard_NC96ads_A100_v4, Standard_ND96asr_v4, Standard_ND96amsr_A100_v4
tags:
  Featured: ""
  Preview: ""
  huggingface_model_id: ""
  license: mit
  disable-batch: "true"
  task: chat-completion
  author: microsoft
  SharedComputeCapacityEnabled : ""
  hiddenlayerscanned : ""
  inference_compute_allow_list:
    [ 
      Standard_NC24ads_A100_v4,
      Standard_NC48ads_A100_v4, 
      Standard_ND96asr_v4,
      Standard_ND96amsr_A100_v4,
      Standard_NC96ads_A100_v4
    ]
  finetune_compute_allow_list:
    [ 
      Standard_ND96asr_v4,
      Standard_ND96amsr_A100_v4,
      Standard_NC96ads_A100_v4
    ]
  inference_supported_envs:
    - vllm
  model_specific_defaults:
    apply_deepspeed: "true"
    deepspeed_stage: 3
    apply_lora: "false"
    apply_ort: "false"
    precision: 16
    ignore_mismatched_sizes: "false"
    num_train_epochs: 1
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 4
    learning_rate: 5e-6
    lr_scheduler_type: "cosine"
    logging_strategy: "steps"
    logging_steps: 10
    save_total_limit: 1
    max_seq_length: 4096
version: 1
