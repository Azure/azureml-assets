flavors:
  python_function:
    env:
      conda: conda.yaml
      virtualenv: python_env.yaml
    loader_module: mlflow.transformers
    model_binary: model
    python_version: 3.8.18
  transformers:
    code: null
    components:
    - tokenizer
    framework: pt
    instance_type: TextGenerationPipeline
    model_binary: model
    pipeline_model_type: LlamaForCausalLM
    task: text-generation
    tokenizer_type: LlamaTokenizerFast
    transformers_version: 4.35.2
metadata:
  base_model_name: microsoft/Orca-2-7b
  base_model_task: text-generation
  is_acft_model: true
  is_finetuned_model: false
  azureml.base_image: mcr.microsoft.com/azureml/curated/mlflow-model-inference:1
mlflow_version: 2.8.1
model_size_bytes: 26956165610
model_uuid: 8911b994c877417e84842221b8acd04c
signature:
  inputs: '[{"type": "string"}]'
  outputs: '[{"type": "string"}]'
  params: '[{"name": "top_p", "type": "float", "default": 0.9, "shape": null}, {"name": "temperature", "type": "float", "default": 0.2, "shape": null}, {"name": "max_new_tokens", "type": "integer", "default": 50, "shape": null}, {"name": "do_sample", "type": "boolean", "default": true, "shape": null}, {"name": "return_full_text", "type": "boolean", "default": true, "shape": null}]'
utc_time_created: '2023-12-08 11:36:30.800706'
