flavors:
  python_function:
    env:
      conda: conda.yaml
      virtualenv: python_env.yaml
    loader_module: mlflow.transformers
    model_binary: model
    python_version: 3.8.18
  transformers:
    code: null
    components:
    - tokenizer
    framework: pt
    instance_type: InstructionTextGenerationPipeline
    model_binary: model
    pipeline_model_type: GPTNeoXForCausalLM
    task: text-generation
    tokenizer_type: GPTNeoXTokenizerFast
    transformers_version: 4.35.2
metadata:
  base_model_name: databricks/dolly-v2-12b
  base_model_task: text-generation
  is_acft_model: true
  is_finetuned_model: false
  azureml.base_image: mcr.microsoft.com/azureml/curated/mlflow-model-inference:1
mlflow_version: 2.8.1
model_size_bytes: 47369790595
model_uuid: f7383acbad6f48069c76424fe00f52fd
signature:
  inputs: '[{"type": "string"}]'
  outputs: '[{"type": "string"}]'
  params: '[{"name": "top_p", "type": "float", "default": 0.9, "shape": null}, {"name": "temperature", "type": "float", "default": 0.2, "shape": null}, {"name": "max_new_tokens", "type": "integer", "default": 50, "shape": null}, {"name": "do_sample", "type": "boolean", "default": true, "shape": null}, {"name": "return_full_text", "type": "boolean", "default": true, "shape": null}]'
utc_time_created: '2023-12-07 04:44:00.270496'
