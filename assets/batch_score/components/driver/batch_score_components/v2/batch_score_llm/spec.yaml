$schema: http://azureml/sdk-2-0/ParallelComponent.json
type: parallel

name: batch_score_llm
version: 0.0.1
display_name: Batch Score Large Language Models
is_deterministic: False

inputs:
  # Predefined arguments for parallel job: https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-job-parallel?source=recommendations#predefined-arguments-for-parallel-job
  resume_from:
    type: string
    optional: True
    description: The pipeline run id to resume from
  append_row_safe_output:
    type: boolean
    default: True
    description: Enable PRS safe append row configuration that is needed when dealing with large outputs with Unicode characters.

  # Custom arguments
  data_input_table:
    type: mltable
    optional: False
    description: The data to be split and scored in parallel.
  api_type:
    type: string
    optional: False
    description: Specifies the API type used for scoring.
    default: completion
    enum:
      - completion
      - chat_completion
      - embeddings
      - vesta
      - vesta_chat_completion
  scoring_url:
    type: string
    optional: False
    description: Url used for scoring input data.
  authentication_type:
    type: string
    optional: True
    description: Specifies the authentication type to use for scoring.
    default: managed_identity
    enum:
      - azureml_workspace_connection
      - managed_identity
  connection_name:
    type: string
    optional: True
    description: Specifies the connection name containing the api-key for scoring. This is required for authentication type "azureml_workspace_connection".
  debug_mode:
    type: boolean
    default: False
  additional_properties:
    type: string
    optional: True
    description: A stringified json expressing additional properties to be added to each request body at the top level.
  additional_headers:
    type: string
    optional: True
    description: A stringified json expressing additional headers to be added to each request.
  configuration_file:
    type: uri_file
    optional: True
    description: A json file containing configuration values for the batch score component.
  tally_failed_requests:
    type: boolean
    default: False
    description: Determines if failed requests will be outputted. Enabling this will count failed requests towards error_threshold.
  tally_exclusions:
    type: string
    default: none
    description: >-
      Configures which failed requests will be excluded from tallying. Only applicable when tally_failed_requests is enabled. Delimit with "|" when specifying multiple values.
        - "none": None of the failed requests will be excluded from tallying.
        - "bad_request_to_model": 400 model status code will be excluded from tallying.
  segment_large_requests:
    type: string
    optional: True
    enum:
      - disabled
      - enabled
  segment_max_token_size:
    type: integer
    default: 600
  app_insights_connection_string:
    type: string
    optional: True
    description: An application insights connection string. If provided, batch component will emit metrics and logs to this application insight instance
  ensure_ascii:
    type: boolean
    default: False
    description: If ensure_ascii is True, the output is guaranteed to have all incoming non-ASCII characters escaped. If ensure_ascii is False, these characters will be output as-is. More defailted information can be found at https://docs.python.org/3/library/json.html
  output_behavior:
    type: string
    optional: False
    default: append_row
    enum:
    - append_row
    - summary_only
  max_retry_time_interval:
    type: integer
    optional: True
    description: The maximum time (in seconds) spent retrying a payload. If unspecified, payloads are retried unlimited times.

  # Parallel configuration
  initial_worker_count:
    type: integer
    default: 5
  max_worker_count:
    type: integer
    description: Overrides initial_worker_count if necessary
    default: 200

  # Partial results configuration
  save_mini_batch_results:
    type: string
    optional: False
    default: disabled
    enum:
      - disabled
      - enabled

  async_mode:
    type: boolean
    default: false
    description: Whether to use PRS mini-batch streaming feature, which allows each PRS processor to process multiple mini-batches at a time.

outputs:
  job_out_path:
    type: uri_file
  mini_batch_results_out_directory:
    type: uri_folder
  metrics_out_directory:
    type: uri_folder

max_concurrency_per_instance: 1
resources:
  instance_count: 1
mini_batch_size: 3kb
mini_batch_error_threshold: 5
logging_level: "DEBUG"
retry_settings:
  max_retries: 2
  timeout: 60

input_data: ${{inputs.data_input_table}}

code: ../../../src

task:
  type: run_function
  entry_script: batch_score.main
  program_arguments: >-
    $[[--resume_from ${{inputs.resume_from}}]]
    --append_row_safe_output ${{inputs.append_row_safe_output}}
    --debug_mode ${{inputs.debug_mode}}
    --api_type ${{inputs.api_type}}
    --scoring_url ${{inputs.scoring_url}}
    $[[--authentication_type ${{inputs.authentication_type}}]]
    $[[--connection_name ${{inputs.connection_name}}]]    
    $[[--additional_properties ${{inputs.additional_properties}}]]
    $[[--additional_headers ${{inputs.additional_headers}}]]
    $[[--configuration_file ${{inputs.configuration_file}}]]
    --metrics_out_directory ${{outputs.metrics_out_directory}}
    --tally_failed_requests ${{inputs.tally_failed_requests}}
    --tally_exclusions ${{inputs.tally_exclusions}}
    $[[--segment_large_requests ${{inputs.segment_large_requests}}]]
    --segment_max_token_size ${{inputs.segment_max_token_size}}
    $[[--app_insights_connection_string ${{inputs.app_insights_connection_string}}]]
    --ensure_ascii ${{inputs.ensure_ascii}}
    --output_behavior ${{inputs.output_behavior}}
    --initial_worker_count ${{inputs.initial_worker_count}}
    --max_worker_count ${{inputs.max_worker_count}}
    $[[--max_retry_time_interval  ${{inputs.max_retry_time_interval}}]]
    --save_mini_batch_results ${{inputs.save_mini_batch_results}}
    --mini_batch_results_out_directory ${{outputs.mini_batch_results_out_directory}}
    --amlbi_async_mode ${{inputs.async_mode}}
  environment: azureml://registries/azureml/environments/batch-score-llm/versions/0.0.1
  append_row_to: ${{outputs.job_out_path}}
