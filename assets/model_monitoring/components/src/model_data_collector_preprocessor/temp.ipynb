{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import bisect\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pyspark.sql import DataFrame, Row, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampNTZType, ArrayType, DataType\n",
    "from pyspark.sql.functions import from_json, to_json, udf, collect_list\n",
    "from typing import Dict, Iterator, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_log_schema = StructType(\n",
    "        [\n",
    "            StructField(\"trace_id\", StringType(), False),\n",
    "            StructField(\"span_id\", StringType(), False),\n",
    "            StructField(\"parent_id\", StringType(), False),\n",
    "            StructField(\"user_id\", StringType(), True),\n",
    "            StructField(\"session_id\", StringType(), True),\n",
    "            StructField(\"span_type\", StringType(), False),\n",
    "            StructField(\"start_time\", TimestampNTZType(), False),\n",
    "            StructField(\"end_time\", TimestampNTZType(), False),\n",
    "            StructField(\"name\", StringType(), False),\n",
    "            StructField(\"status\", StringType(), False),\n",
    "            StructField(\"framework\", StringType(), False),\n",
    "            StructField(\"input\", StringType(), False),\n",
    "            StructField(\"output\", StringType(), False),\n",
    "            StructField(\"attributes\", StringType(), False),\n",
    "            StructField(\"events\", ArrayType(DataType()), False),\n",
    "            StructField(\"links\", ArrayType(DataType()), False),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark() -> SparkSession:\n",
    "    \"\"\"Get or create spark session.\"\"\"\n",
    "    spark = SparkSession.builder.appName(\"AccessParquetFiles\").getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+--------------------+----------------+--------------------+------------+--------------------+-----------+--------------------+--------------------+--------------------+\n",
      "|               agent|     collectdatatype|       contentrange|       correlationid|                data| datacontenttype|                  id|modelversion|              source|specversion|                time|                type|          xrequestid|\n",
      "+--------------------+--------------------+-------------------+--------------------+--------------------+----------------+--------------------+------------+--------------------+-----------+--------------------+--------------------+--------------------+\n",
      "|azureml-ai-monito...|pandas.core.frame...|    bytes 0-867/868|ee3c0e82-7edf-480...|[{{NULL, promptfl...|application/json|6fee5e87-e702-40f...|     default|/subscriptions/96...|        1.0|2024-02-05T15:00:22Z|azureml.inference...|ee3c0e82-7edf-480...|\n",
      "|azureml-ai-monito...|pandas.core.frame...|  bytes 0-2125/2126|0683a146-03f1-4aa...|[{{, promptflow, ...|application/json|59ab58f7-cbf6-4f4...|     default|/subscriptions/96...|        1.0|2024-02-05T15:00:17Z|azureml.inference...|0683a146-03f1-4aa...|\n",
      "|azureml-ai-monito...|pandas.core.frame...|  bytes 0-9664/9665|a8d25f2e-2089-4c4...|[{{, promptflow, ...|application/json|37730a53-a3fe-425...|     default|/subscriptions/96...|        1.0|2024-02-05T15:00:22Z|azureml.inference...|a8d25f2e-2089-4c4...|\n",
      "|azureml-ai-monito...|pandas.core.frame...|  bytes 0-5386/5387|566b7a59-9c2a-475...|[{{, promptflow, ...|application/json|7945cce6-a17f-471...|     default|/subscriptions/96...|        1.0|2024-02-05T15:00:22Z|azureml.inference...|566b7a59-9c2a-475...|\n",
      "|azureml-ai-monito...|pandas.core.frame...|  bytes 0-7205/7206|c8f691e6-376b-4ad...|[{{, promptflow, ...|application/json|b55484ce-43a0-4f6...|     default|/subscriptions/96...|        1.0|2024-02-05T15:00:22Z|azureml.inference...|c8f691e6-376b-4ad...|\n",
      "|azureml-ai-monito...|pandas.core.frame...|  bytes 0-6179/6180|b1ffd289-d325-477...|[{{, promptflow, ...|application/json|bcf9fd6a-694b-43f...|     default|/subscriptions/96...|        1.0|2024-02-05T15:00:22Z|azureml.inference...|b1ffd289-d325-477...|\n",
      "|azureml-ai-monito...|pandas.core.frame...|  bytes 0-9201/9202|81134217-0100-463...|[{{, promptflow, ...|application/json|43d2493b-33d2-4ca...|     default|/subscriptions/96...|        1.0|2024-02-05T15:00:22Z|azureml.inference...|81134217-0100-463...|\n",
      "|azureml-ai-monito...|pandas.core.frame...|  bytes 0-6141/6142|1c16dc7d-c10e-4d4...|[{{NULL, NULL, NU...|application/json|9be18d14-3fac-494...|     default|/subscriptions/96...|        1.0|2024-02-05T15:00:22Z|azureml.inference...|1c16dc7d-c10e-4d4...|\n",
      "|azureml-ai-monito...|pandas.core.frame...|  bytes 0-1428/1429|b7fdb4b5-4904-45d...|[{{, promptflow, ...|application/json|d8fa94f7-0d57-478...|     default|/subscriptions/96...|        1.0|2024-02-05T15:00:17Z|azureml.inference...|b7fdb4b5-4904-45d...|\n",
      "|azureml-ai-monito...|pandas.core.frame...|bytes 0-49333/49334|ba98f534-0803-40f...|[{{, promptflow, ...|application/json|939b0ed2-e8b5-481...|     default|/subscriptions/96...|        1.0|2024-02-05T15:00:22Z|azureml.inference...|ba98f534-0803-40f...|\n",
      "+--------------------+--------------------+-------------------+--------------------+--------------------+----------------+--------------------+------------+--------------------+-----------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"./sample_preprocessed_span.jsonl\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(data=[Row(attributes=Row(flow_id=None, framework='promptflow', function=None, inputs='{\\n  \"question\": \"what\\'s Attention?\",\\n  \"chat_history\": []\\n}', line_run_id=None, node_name=None, output='{\\n  \"output\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\"\\n}', retrieval.documents=None, retrieval.query=None, root_run_id=None, span_type='Flow'), end_time='2024-02-05T15:00:18.791564Z', events=[], input='{\\n  \"question\": \"what\\'s Attention?\",\\n  \"chat_history\": []\\n}', links=[], name='promptflow.flow', output='{\\n  \"output\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\"\\n}', parent_id=None, session_id='', span_id='0x7fd179134fb9e709', span_type='SpanKind.INTERNAL', start_time='2024-02-05T15:00:13.789782Z', status='OK', trace_id='0xccf497116a5a912b29e55229bbac0ba8', user_id='')]), Row(data=[Row(attributes=Row(flow_id='', framework='promptflow', function='openai.resources.chat.completions.Completions.create', inputs='{\\n  \"model\": \"gpt-35-turbo\",\\n  \"messages\": [\\n    {\\n      \"role\": \"system\",\\n      \"content\": \"Given the following conversation history and the users next question,rephrase the question to be a stand alone question.\\\\nIf the conversation is irrelevant or empty, just restate the original question.\\\\nDo not add more details than necessary to the question.\\\\nconversation:\\\\n\\\\n chat history:\"\\n    },\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"Follow up Input: what\\'s Attention? \\\\nStandalone Question:\"\\n    }\\n  ],\\n  \"temperature\": 1.0,\\n  \"top_p\": 1.0,\\n  \"n\": 1,\\n  \"stream\": false,\\n  \"stop\": null,\\n  \"max_tokens\": null,\\n  \"presence_penalty\": 0.0,\\n  \"frequency_penalty\": 0.0,\\n  \"logit_bias\": {},\\n  \"user\": \"\",\\n  \"response_format\": null\\n}', line_run_id='6663adb5-bec1-4e57-b3fc-88b2ce82eb11', node_name='modify_query_with_history', output='{\\n  \"id\": \"chatcmpl-8oukAwNXR2qyf3yQzLGoZ8o2M8Ka5\",\\n  \"choices\": [\\n    {\\n      \"finish_reason\": \"stop\",\\n      \"index\": 0,\\n      \"logprobs\": null,\\n      \"message\": {\\n        \"content\": \"What is the meaning of Attention?\",\\n        \"role\": \"assistant\",\\n        \"function_call\": null,\\n        \"tool_calls\": null\\n      }\\n    }\\n  ],\\n  \"created\": 1707145214,\\n  \"model\": \"gpt-35-turbo\",\\n  \"object\": \"chat.completion\",\\n  \"system_fingerprint\": null,\\n  \"usage\": {\\n    \"completion_tokens\": 7,\\n    \"prompt_tokens\": 78,\\n    \"total_tokens\": 85\\n  }\\n}', retrieval.documents=None, retrieval.query=None, root_run_id='', span_type='LLM'), end_time='2024-02-05T15:00:14.436365Z', events=[], input='{\\n  \"model\": \"gpt-35-turbo\",\\n  \"messages\": [\\n    {\\n      \"role\": \"system\",\\n      \"content\": \"Given the following conversation history and the users next question,rephrase the question to be a stand alone question.\\\\nIf the conversation is irrelevant or empty, just restate the original question.\\\\nDo not add more details than necessary to the question.\\\\nconversation:\\\\n\\\\n chat history:\"\\n    },\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"Follow up Input: what\\'s Attention? \\\\nStandalone Question:\"\\n    }\\n  ],\\n  \"temperature\": 1.0,\\n  \"top_p\": 1.0,\\n  \"n\": 1,\\n  \"stream\": false,\\n  \"stop\": null,\\n  \"max_tokens\": null,\\n  \"presence_penalty\": 0.0,\\n  \"frequency_penalty\": 0.0,\\n  \"logit_bias\": {},\\n  \"user\": \"\",\\n  \"response_format\": null\\n}', links=[], name='openai.resources.chat.completions.Completions.create', output='{\\n  \"id\": \"chatcmpl-8oukAwNXR2qyf3yQzLGoZ8o2M8Ka5\",\\n  \"choices\": [\\n    {\\n      \"finish_reason\": \"stop\",\\n      \"index\": 0,\\n      \"logprobs\": null,\\n      \"message\": {\\n        \"content\": \"What is the meaning of Attention?\",\\n        \"role\": \"assistant\",\\n        \"function_call\": null,\\n        \"tool_calls\": null\\n      }\\n    }\\n  ],\\n  \"created\": 1707145214,\\n  \"model\": \"gpt-35-turbo\",\\n  \"object\": \"chat.completion\",\\n  \"system_fingerprint\": null,\\n  \"usage\": {\\n    \"completion_tokens\": 7,\\n    \"prompt_tokens\": 78,\\n    \"total_tokens\": 85\\n  }\\n}', parent_id='0x21d2ebaaad77bedf', session_id=None, span_id='0x6275bb0c2a2594e0', span_type='SpanKind.INTERNAL', start_time='2024-02-05T15:00:13.796476Z', status='OK', trace_id='0xccf497116a5a912b29e55229bbac0ba8', user_id=None)]), Row(data=[Row(attributes=Row(flow_id='', framework='promptflow', function='generate_prompt_context', inputs='{\\n  \"search_result\": [\\n    {\\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\",\\n      \"metadata\": {\\n        \"source_doc_id\": \"1706.03762.pdf13\",\\n        \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\\n        \"mtime\": null,\\n        \"page_number\": 13,\\n        \"stats\": {\\n          \"tiktokens\": 304,\\n          \"chars\": 835,\\n          \"lines\": 113\\n        },\\n        \"source\": {\\n          \"filename\": \"1706.03762.pdf\",\\n          \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n          \"mtime\": 1706775620.0\\n        }\\n      },\\n      \"score\": 0.39159566164016724\\n    },\\n    {\\n      \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\",\\n      \"metadata\": {\\n        \"source_doc_id\": \"1706.03762.pdf12\",\\n        \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\\n        \"mtime\": null,\\n        \"page_number\": 12,\\n        \"stats\": {\\n          \"tiktokens\": 254,\\n          \"chars\": 833,\\n          \"lines\": 73\\n        },\\n        \"source\": {\\n          \"filename\": \"1706.03762.pdf\",\\n          \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n          \"mtime\": 1706775620.0\\n        }\\n      },\\n      \"score\": 0.39190346002578735\\n    },\\n    {\\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\",\\n      \"metadata\": {\\n        \"source_doc_id\": \"1706.03762.pdf14\",\\n        \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\\n        \"mtime\": null,\\n        \"page_number\": 14,\\n        \"stats\": {\\n          \"tiktokens\": 291,\\n          \"chars\": 838,\\n          \"lines\": 113\\n        },\\n        \"source\": {\\n          \"filename\": \"1706.03762.pdf\",\\n          \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n          \"mtime\": 1706775620.0\\n        }\\n      },\\n      \"score\": 0.42808181047439575\\n    }\\n  ]\\n}', line_run_id='6663adb5-bec1-4e57-b3fc-88b2ce82eb11', node_name='generate_prompt_context', output='\"Content: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\"', retrieval.documents=None, retrieval.query=None, root_run_id='', span_type='Tool'), end_time='2024-02-05T15:00:17.672230Z', events=[], input='{\\n  \"search_result\": [\\n    {\\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\",\\n      \"metadata\": {\\n        \"source_doc_id\": \"1706.03762.pdf13\",\\n        \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\\n        \"mtime\": null,\\n        \"page_number\": 13,\\n        \"stats\": {\\n          \"tiktokens\": 304,\\n          \"chars\": 835,\\n          \"lines\": 113\\n        },\\n        \"source\": {\\n          \"filename\": \"1706.03762.pdf\",\\n          \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n          \"mtime\": 1706775620.0\\n        }\\n      },\\n      \"score\": 0.39159566164016724\\n    },\\n    {\\n      \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\",\\n      \"metadata\": {\\n        \"source_doc_id\": \"1706.03762.pdf12\",\\n        \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\\n        \"mtime\": null,\\n        \"page_number\": 12,\\n        \"stats\": {\\n          \"tiktokens\": 254,\\n          \"chars\": 833,\\n          \"lines\": 73\\n        },\\n        \"source\": {\\n          \"filename\": \"1706.03762.pdf\",\\n          \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n          \"mtime\": 1706775620.0\\n        }\\n      },\\n      \"score\": 0.39190346002578735\\n    },\\n    {\\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\",\\n      \"metadata\": {\\n        \"source_doc_id\": \"1706.03762.pdf14\",\\n        \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\\n        \"mtime\": null,\\n        \"page_number\": 14,\\n        \"stats\": {\\n          \"tiktokens\": 291,\\n          \"chars\": 838,\\n          \"lines\": 113\\n        },\\n        \"source\": {\\n          \"filename\": \"1706.03762.pdf\",\\n          \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n          \"mtime\": 1706775620.0\\n        }\\n      },\\n      \"score\": 0.42808181047439575\\n    }\\n  ]\\n}', links=[], name='generate_prompt_context', output='\"Content: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\"', parent_id='0x7fd179134fb9e709', session_id=None, span_id='0x3d101a004d88e6d0', span_type='SpanKind.INTERNAL', start_time='2024-02-05T15:00:17.668465Z', status='OK', trace_id='0xccf497116a5a912b29e55229bbac0ba8', user_id=None)]), Row(data=[Row(attributes=Row(flow_id='', framework='promptflow', function='AzureOpenAI.chat', inputs='{\\n  \"prompt\": \"{{prompt_text}}\",\\n  \"deployment_name\": \"gpt-35-turbo\",\\n  \"temperature\": 0.0,\\n  \"top_p\": 1.0,\\n  \"max_tokens\": 1000,\\n  \"presence_penalty\": 0.0,\\n  \"frequency_penalty\": 0.0,\\n  \"prompt_text\": \"system: \\\\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\\\nPlease add citation after each sentence when possible in a form \\\\\"(Source: citation)\\\\\".\\\\n\\\\n user: \\\\n Content: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf \\\\n\\\\n chat history: \\\\nuser: what\\'s Attention? \\\\nassistant:\",\\n  \"stream\": false\\n}', line_run_id='6663adb5-bec1-4e57-b3fc-88b2ce82eb11', node_name='answer_the_question_with_context', output='\"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\"', retrieval.documents=None, retrieval.query=None, root_run_id='', span_type='Tool'), end_time='2024-02-05T15:00:18.788089Z', events=[], input='{\\n  \"prompt\": \"{{prompt_text}}\",\\n  \"deployment_name\": \"gpt-35-turbo\",\\n  \"temperature\": 0.0,\\n  \"top_p\": 1.0,\\n  \"max_tokens\": 1000,\\n  \"presence_penalty\": 0.0,\\n  \"frequency_penalty\": 0.0,\\n  \"prompt_text\": \"system: \\\\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\\\nPlease add citation after each sentence when possible in a form \\\\\"(Source: citation)\\\\\".\\\\n\\\\n user: \\\\n Content: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf \\\\n\\\\n chat history: \\\\nuser: what\\'s Attention? \\\\nassistant:\",\\n  \"stream\": false\\n}', links=[], name='answer_the_question_with_context', output='\"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\"', parent_id='0x7fd179134fb9e709', session_id=None, span_id='0xcda5d0e89f4747df', span_type='SpanKind.INTERNAL', start_time='2024-02-05T15:00:17.680046Z', status='OK', trace_id='0xccf497116a5a912b29e55229bbac0ba8', user_id=None)]), Row(data=[Row(attributes=Row(flow_id='', framework='promptflow', function='search', inputs='{\\n  \"mlindex_content\": \"embeddings:\\\\n  api_base: https://promptflow-ci-weu.openai.azure.com/\\\\n  api_type: azure\\\\n  api_version: 2023-03-15-preview\\\\n  batch_size: \\'16\\'\\\\n  connection:\\\\n    id: \\\\n      /subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/connections/azure_open_ai_connection\\\\n  connection_type: workspace_connection\\\\n  deployment: text-embedding-ada-003\\\\n  dimension: 1536\\\\n  file_format_version: \\'2\\'\\\\n  kind: open_ai\\\\n  model: text-embedding-ada-002\\\\n  schema_version: \\'2\\'\\\\nindex:\\\\n  engine: langchain.vectorstores.FAISS\\\\n  kind: faiss\\\\n  method: FlatL2\\\\n  path: \\\\n    azureml://subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourcegroups/promptflow/workspaces/pf-xp/datastores/workspaceblobstore/paths/azureml/f7ee0381-06fc-4566-820f-86ba174db987/index/\\\\nself:\\\\n  path: \\\\n    azureml://subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourcegroups/promptflow/workspaces/pf-xp/datastores/workspaceblobstore/paths/azureml/f7ee0381-06fc-4566-820f-86ba174db987/index/\\\\n  asset_id: \\\\n    azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/serene-honey-h9lrkyczzk/versions/1\\\\n\",\\n  \"queries\": \"What is the meaning of Attention?\",\\n  \"query_type\": \"Vector\",\\n  \"top_k\": 3\\n}', line_run_id='6663adb5-bec1-4e57-b3fc-88b2ce82eb11', node_name='vector_lookup', output='[\\n  {\\n    \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\",\\n    \"metadata\": {\\n      \"source_doc_id\": \"1706.03762.pdf13\",\\n      \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\\n      \"mtime\": null,\\n      \"page_number\": 13,\\n      \"stats\": {\\n        \"tiktokens\": 304,\\n        \"chars\": 835,\\n        \"lines\": 113\\n      },\\n      \"source\": {\\n        \"filename\": \"1706.03762.pdf\",\\n        \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n        \"mtime\": 1706775620.0\\n      }\\n    },\\n    \"score\": 0.39159566164016724\\n  },\\n  {\\n    \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\",\\n    \"metadata\": {\\n      \"source_doc_id\": \"1706.03762.pdf12\",\\n      \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\\n      \"mtime\": null,\\n      \"page_number\": 12,\\n      \"stats\": {\\n        \"tiktokens\": 254,\\n        \"chars\": 833,\\n        \"lines\": 73\\n      },\\n      \"source\": {\\n        \"filename\": \"1706.03762.pdf\",\\n        \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n        \"mtime\": 1706775620.0\\n      }\\n    },\\n    \"score\": 0.39190346002578735\\n  },\\n  {\\n    \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\",\\n    \"metadata\": {\\n      \"source_doc_id\": \"1706.03762.pdf14\",\\n      \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\\n      \"mtime\": null,\\n      \"page_number\": 14,\\n      \"stats\": {\\n        \"tiktokens\": 291,\\n        \"chars\": 838,\\n        \"lines\": 113\\n      },\\n      \"source\": {\\n        \"filename\": \"1706.03762.pdf\",\\n        \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n        \"mtime\": 1706775620.0\\n      }\\n    },\\n    \"score\": 0.42808181047439575\\n  }\\n]', retrieval.documents=None, retrieval.query=None, root_run_id='', span_type='Tool'), end_time='2024-02-05T15:00:17.625517Z', events=[], input='{\\n  \"mlindex_content\": \"embeddings:\\\\n  api_base: https://promptflow-ci-weu.openai.azure.com/\\\\n  api_type: azure\\\\n  api_version: 2023-03-15-preview\\\\n  batch_size: \\'16\\'\\\\n  connection:\\\\n    id: \\\\n      /subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/connections/azure_open_ai_connection\\\\n  connection_type: workspace_connection\\\\n  deployment: text-embedding-ada-003\\\\n  dimension: 1536\\\\n  file_format_version: \\'2\\'\\\\n  kind: open_ai\\\\n  model: text-embedding-ada-002\\\\n  schema_version: \\'2\\'\\\\nindex:\\\\n  engine: langchain.vectorstores.FAISS\\\\n  kind: faiss\\\\n  method: FlatL2\\\\n  path: \\\\n    azureml://subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourcegroups/promptflow/workspaces/pf-xp/datastores/workspaceblobstore/paths/azureml/f7ee0381-06fc-4566-820f-86ba174db987/index/\\\\nself:\\\\n  path: \\\\n    azureml://subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourcegroups/promptflow/workspaces/pf-xp/datastores/workspaceblobstore/paths/azureml/f7ee0381-06fc-4566-820f-86ba174db987/index/\\\\n  asset_id: \\\\n    azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/serene-honey-h9lrkyczzk/versions/1\\\\n\",\\n  \"queries\": \"What is the meaning of Attention?\",\\n  \"query_type\": \"Vector\",\\n  \"top_k\": 3\\n}', links=[], name='vector_lookup', output='[\\n  {\\n    \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\",\\n    \"metadata\": {\\n      \"source_doc_id\": \"1706.03762.pdf13\",\\n      \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\\n      \"mtime\": null,\\n      \"page_number\": 13,\\n      \"stats\": {\\n        \"tiktokens\": 304,\\n        \"chars\": 835,\\n        \"lines\": 113\\n      },\\n      \"source\": {\\n        \"filename\": \"1706.03762.pdf\",\\n        \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n        \"mtime\": 1706775620.0\\n      }\\n    },\\n    \"score\": 0.39159566164016724\\n  },\\n  {\\n    \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\",\\n    \"metadata\": {\\n      \"source_doc_id\": \"1706.03762.pdf12\",\\n      \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\\n      \"mtime\": null,\\n      \"page_number\": 12,\\n      \"stats\": {\\n        \"tiktokens\": 254,\\n        \"chars\": 833,\\n        \"lines\": 73\\n      },\\n      \"source\": {\\n        \"filename\": \"1706.03762.pdf\",\\n        \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n        \"mtime\": 1706775620.0\\n      }\\n    },\\n    \"score\": 0.39190346002578735\\n  },\\n  {\\n    \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\",\\n    \"metadata\": {\\n      \"source_doc_id\": \"1706.03762.pdf14\",\\n      \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\\n      \"mtime\": null,\\n      \"page_number\": 14,\\n      \"stats\": {\\n        \"tiktokens\": 291,\\n        \"chars\": 838,\\n        \"lines\": 113\\n      },\\n      \"source\": {\\n        \"filename\": \"1706.03762.pdf\",\\n        \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n        \"mtime\": 1706775620.0\\n      }\\n    },\\n    \"score\": 0.42808181047439575\\n  }\\n]', parent_id='0x7fd179134fb9e709', session_id=None, span_id='0xd25a6f7c89885b7f', span_type='SpanKind.INTERNAL', start_time='2024-02-05T15:00:14.441572Z', status='OK', trace_id='0xccf497116a5a912b29e55229bbac0ba8', user_id=None)]), Row(data=[Row(attributes=Row(flow_id='', framework='promptflow', function='openai.resources.chat.completions.Completions.create', inputs='{\\n  \"model\": \"gpt-35-turbo\",\\n  \"messages\": [\\n    {\\n      \"role\": \"system\",\\n      \"content\": \"You are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\\\nPlease add citation after each sentence when possible in a form \\\\\"(Source: citation)\\\\\".\"\\n    },\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"Content: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf \\\\n\\\\n chat history: \\\\nuser: what\\'s Attention? \\\\nassistant:\"\\n    }\\n  ],\\n  \"temperature\": 0.0,\\n  \"top_p\": 1.0,\\n  \"n\": 1,\\n  \"stream\": false,\\n  \"stop\": null,\\n  \"max_tokens\": 1000,\\n  \"presence_penalty\": 0.0,\\n  \"frequency_penalty\": 0.0,\\n  \"logit_bias\": {},\\n  \"user\": \"\",\\n  \"response_format\": null\\n}', line_run_id='6663adb5-bec1-4e57-b3fc-88b2ce82eb11', node_name='answer_the_question_with_context', output='{\\n  \"id\": \"chatcmpl-8oukE8DGO1ZnKNpbqZMpApVzxy0bX\",\\n  \"choices\": [\\n    {\\n      \"finish_reason\": \"stop\",\\n      \"index\": 0,\\n      \"logprobs\": null,\\n      \"message\": {\\n        \"content\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\",\\n        \"role\": \"assistant\",\\n        \"function_call\": null,\\n        \"tool_calls\": null\\n      }\\n    }\\n  ],\\n  \"created\": 1707145218,\\n  \"model\": \"gpt-35-turbo\",\\n  \"object\": \"chat.completion\",\\n  \"system_fingerprint\": null,\\n  \"usage\": {\\n    \"completion_tokens\": 58,\\n    \"prompt_tokens\": 1110,\\n    \"total_tokens\": 1168\\n  }\\n}', retrieval.documents=None, retrieval.query=None, root_run_id='', span_type='LLM'), end_time='2024-02-05T15:00:18.787224Z', events=[], input='{\\n  \"model\": \"gpt-35-turbo\",\\n  \"messages\": [\\n    {\\n      \"role\": \"system\",\\n      \"content\": \"You are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\\\nPlease add citation after each sentence when possible in a form \\\\\"(Source: citation)\\\\\".\"\\n    },\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"Content: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf \\\\n\\\\n chat history: \\\\nuser: what\\'s Attention? \\\\nassistant:\"\\n    }\\n  ],\\n  \"temperature\": 0.0,\\n  \"top_p\": 1.0,\\n  \"n\": 1,\\n  \"stream\": false,\\n  \"stop\": null,\\n  \"max_tokens\": 1000,\\n  \"presence_penalty\": 0.0,\\n  \"frequency_penalty\": 0.0,\\n  \"logit_bias\": {},\\n  \"user\": \"\",\\n  \"response_format\": null\\n}', links=[], name='openai.resources.chat.completions.Completions.create', output='{\\n  \"id\": \"chatcmpl-8oukE8DGO1ZnKNpbqZMpApVzxy0bX\",\\n  \"choices\": [\\n    {\\n      \"finish_reason\": \"stop\",\\n      \"index\": 0,\\n      \"logprobs\": null,\\n      \"message\": {\\n        \"content\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\",\\n        \"role\": \"assistant\",\\n        \"function_call\": null,\\n        \"tool_calls\": null\\n      }\\n    }\\n  ],\\n  \"created\": 1707145218,\\n  \"model\": \"gpt-35-turbo\",\\n  \"object\": \"chat.completion\",\\n  \"system_fingerprint\": null,\\n  \"usage\": {\\n    \"completion_tokens\": 58,\\n    \"prompt_tokens\": 1110,\\n    \"total_tokens\": 1168\\n  }\\n}', parent_id='0xcda5d0e89f4747df', session_id=None, span_id='0x52355639a5440992', span_type='SpanKind.INTERNAL', start_time='2024-02-05T15:00:17.682481Z', status='OK', trace_id='0xccf497116a5a912b29e55229bbac0ba8', user_id=None)]), Row(data=[Row(attributes=Row(flow_id='', framework='promptflow', function='render_template_jinja2', inputs='{\\n  \"template\": \"system: \\\\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\\\nPlease add citation after each sentence when possible in a form \\\\\"(Source: citation)\\\\\".\\\\n\\\\n user: \\\\n {{contexts}} \\\\n\\\\n chat history: \\\\n{% for item in chat_history %} user: \\\\n{{ item.inputs.question }} \\\\nassistant: \\\\n{{ item.outputs.output }} \\\\n{% endfor %}\\\\nuser: {{question}} \\\\nassistant:\",\\n  \"chat_history\": [],\\n  \"contexts\": \"Content: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n  \"question\": \"what\\'s Attention?\"\\n}', line_run_id='6663adb5-bec1-4e57-b3fc-88b2ce82eb11', node_name='Prompt_variants', output='\"system: \\\\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\\\nPlease add citation after each sentence when possible in a form \\\\\"(Source: citation)\\\\\".\\\\n\\\\n user: \\\\n Content: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf \\\\n\\\\n chat history: \\\\nuser: what\\'s Attention? \\\\nassistant:\"', retrieval.documents=None, retrieval.query=None, root_run_id='', span_type='Tool'), end_time='2024-02-05T15:00:17.677929Z', events=[], input='{\\n  \"template\": \"system: \\\\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\\\nPlease add citation after each sentence when possible in a form \\\\\"(Source: citation)\\\\\".\\\\n\\\\n user: \\\\n {{contexts}} \\\\n\\\\n chat history: \\\\n{% for item in chat_history %} user: \\\\n{{ item.inputs.question }} \\\\nassistant: \\\\n{{ item.outputs.output }} \\\\n{% endfor %}\\\\nuser: {{question}} \\\\nassistant:\",\\n  \"chat_history\": [],\\n  \"contexts\": \"Content: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n  \"question\": \"what\\'s Attention?\"\\n}', links=[], name='Prompt_variants', output='\"system: \\\\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\\\nPlease add citation after each sentence when possible in a form \\\\\"(Source: citation)\\\\\".\\\\n\\\\n user: \\\\n Content: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\\\n\\\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\\\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf \\\\n\\\\n chat history: \\\\nuser: what\\'s Attention? \\\\nassistant:\"', parent_id='0x7fd179134fb9e709', session_id=None, span_id='0x1a77eee854345f20', span_type='SpanKind.INTERNAL', start_time='2024-02-05T15:00:17.675380Z', status='OK', trace_id='0xccf497116a5a912b29e55229bbac0ba8', user_id=None)]), Row(data=[Row(attributes=Row(flow_id=None, framework=None, function=None, inputs=None, line_run_id=None, node_name=None, output=None, retrieval.documents='[\\n    {\\n        \"document.content\": \"Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5\\\\nand 6. Note that the attentions are very sharp for this word.\\\\n14\",\\n        \"document.score\": 0.39159566164016724,\\n        \"document.metadata\": {\\n            \"source_doc_id\": \"1706.03762.pdf13\",\\n            \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\\n            \"mtime\": null,\\n            \"page_number\": 13,\\n            \"stats\": {\\n                \"tiktokens\": 304,\\n                \"chars\": 835,\\n                \"lines\": 113\\n            },\\n            \"source\": {\\n                \"filename\": \"1706.03762.pdf\",\\n                \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n                \"mtime\": 1706775620.0\\n            }\\n        }\\n    },\\n    {\\n        \"document.content\": \"Title: 1706.03762.pdfAttention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n<EOS>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\n<pad>\\\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\\\nthe verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019. Attentions here shown only for\\\\nthe word \\\\u2018making\\\\u2019. Different colors represent different heads. Best viewed in color.\\\\n13\",\\n        \"document.score\": 0.39190346002578735,\\n        \"document.metadata\": {\\n            \"source_doc_id\": \"1706.03762.pdf12\",\\n            \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\\n            \"mtime\": null,\\n            \"page_number\": 12,\\n            \"stats\": {\\n                \"tiktokens\": 254,\\n                \"chars\": 833,\\n                \"lines\": 73\\n            },\\n            \"source\": {\\n                \"filename\": \"1706.03762.pdf\",\\n                \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n                \"mtime\": 1706775620.0\\n            }\\n        }\\n    },\\n    {\\n        \"document.content\": \"Title: 1706.03762.pdfInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n<EOS>\\\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\\\n15\",\\n        \"document.score\": 0.42808181047439575,\\n        \"document.metadata\": {\\n            \"source_doc_id\": \"1706.03762.pdf14\",\\n            \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\\n            \"mtime\": null,\\n            \"page_number\": 14,\\n            \"stats\": {\\n                \"tiktokens\": 291,\\n                \"chars\": 838,\\n                \"lines\": 113\\n            },\\n            \"source\": {\\n                \"filename\": \"1706.03762.pdf\",\\n                \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\\n                \"mtime\": 1706775620.0\\n            }\\n        }\\n    }\\n]', retrieval.query='What is the meaning of Attention?', root_run_id=None, span_type='Retrieval'), end_time='2024-02-05T15:00:17.620704Z', events=[], input=None, links=[], name='search', output=None, parent_id='0xd25a6f7c89885b7f', session_id=None, span_id='0x1bb2aa9e6b530b09', span_type='SpanKind.INTERNAL', start_time='2024-02-05T15:00:16.775105Z', status='UNSET', trace_id='0xccf497116a5a912b29e55229bbac0ba8', user_id=None)]), Row(data=[Row(attributes=Row(flow_id='', framework='promptflow', function='AzureOpenAI.chat', inputs='{\\n  \"prompt\": \"system: \\\\nGiven the following conversation history and the users next question,rephrase the question to be a stand alone question.\\\\nIf the conversation is irrelevant or empty, just restate the original question.\\\\nDo not add more details than necessary to the question.\\\\nconversation:\\\\n\\\\n chat history: \\\\n{% for item in chat_history %} user: \\\\n{{ item.inputs.question }} \\\\nassistant: \\\\n{{ item.outputs.output }} \\\\n{% endfor %}\\\\n\\\\nuser:\\\\nFollow up Input: {{question}} \\\\nStandalone Question:\",\\n  \"deployment_name\": \"gpt-35-turbo\",\\n  \"temperature\": 1.0,\\n  \"top_p\": 1.0,\\n  \"presence_penalty\": 0.0,\\n  \"frequency_penalty\": 0.0,\\n  \"chat_history\": [],\\n  \"question\": \"what\\'s Attention?\"\\n}', line_run_id='6663adb5-bec1-4e57-b3fc-88b2ce82eb11', node_name='modify_query_with_history', output='\"What is the meaning of Attention?\"', retrieval.documents=None, retrieval.query=None, root_run_id='', span_type='Tool'), end_time='2024-02-05T15:00:14.437464Z', events=[], input='{\\n  \"prompt\": \"system: \\\\nGiven the following conversation history and the users next question,rephrase the question to be a stand alone question.\\\\nIf the conversation is irrelevant or empty, just restate the original question.\\\\nDo not add more details than necessary to the question.\\\\nconversation:\\\\n\\\\n chat history: \\\\n{% for item in chat_history %} user: \\\\n{{ item.inputs.question }} \\\\nassistant: \\\\n{{ item.outputs.output }} \\\\n{% endfor %}\\\\n\\\\nuser:\\\\nFollow up Input: {{question}} \\\\nStandalone Question:\",\\n  \"deployment_name\": \"gpt-35-turbo\",\\n  \"temperature\": 1.0,\\n  \"top_p\": 1.0,\\n  \"presence_penalty\": 0.0,\\n  \"frequency_penalty\": 0.0,\\n  \"chat_history\": [],\\n  \"question\": \"what\\'s Attention?\"\\n}', links=[], name='modify_query_with_history', output='\"What is the meaning of Attention?\"', parent_id='0x7fd179134fb9e709', session_id=None, span_id='0x21d2ebaaad77bedf', span_type='SpanKind.INTERNAL', start_time='2024-02-05T15:00:13.792668Z', status='OK', trace_id='0xccf497116a5a912b29e55229bbac0ba8', user_id=None)]), Row(data=[Row(attributes=Row(flow_id='', framework='promptflow', function='openai.resources.embeddings.Embeddings.create', inputs='{\\n  \"input\": [\\n    [\\n      3923,\\n      374,\\n      279,\\n      7438,\\n      315,\\n      63120,\\n      30\\n    ]\\n  ],\\n  \"model\": \"text-embedding-ada-002\"\\n}', line_run_id='6663adb5-bec1-4e57-b3fc-88b2ce82eb11', node_name='vector_lookup', output='{\\n  \"data\": [\\n    {\\n      \"embedding\": [\\n        -0.03181939572095871,\\n        0.0023466129787266254,\\n        0.03857223689556122,\\n        -0.014059418812394142,\\n        -0.005179430358111858,\\n        0.014072923921048641,\\n        -0.027767689898610115,\\n        -0.017179232090711594,\\n        -0.01000095997005701,\\n        -0.023594433441758156,\\n        0.0336291566491127,\\n        0.020055942237377167,\\n        -0.013937867246568203,\\n        0.006378060206770897,\\n        0.014329532161355019,\\n        -0.013451662845909595,\\n        0.043164171278476715,\\n        0.010541187599301338,\\n        -0.01359347254037857,\\n        -0.030036645010113716,\\n        -0.02895618975162506,\\n        -7.66025623306632e-05,\\n        0.0011108426842838526,\\n        0.02378351241350174,\\n        -0.007610453758388758,\\n        0.011182707734405994,\\n        0.011094920337200165,\\n        -0.024728910997509956,\\n        -0.006685314234346151,\\n        -0.038842350244522095,\\n        0.023418858647346497,\\n        0.004946457222104073,\\n        0.006573892664164305,\\n        -0.020380079746246338,\\n        -0.017003657296299934,\\n        -0.004686472937464714,\\n        0.004949833732098341,\\n        -0.028605042025446892,\\n        0.020326057448983192,\\n        -0.00028868403751403093,\\n        0.02526913769543171,\\n        0.024013109505176544,\\n        -0.0022486967500299215,\\n        0.00564537663012743,\\n        0.0006626226822845638,\\n        0.006668432150036097,\\n        0.008089905604720116,\\n        -0.028361938893795013,\\n        -0.010784289799630642,\\n        0.014653668738901615,\\n        0.028794120997190475,\\n        0.03271077200770378,\\n        -0.029847566038370132,\\n        -0.02616051211953163,\\n        0.0024951754603534937,\\n        -0.004916069563478231,\\n        -0.004412982612848282,\\n        0.0013429715763777494,\\n        0.013742035254836082,\\n        0.0015455569373443723,\\n        -0.0005807444686070085,\\n        -0.013120773248374462,\\n        -0.010547940619289875,\\n        0.01214161142706871,\\n        -0.0017489863093942404,\\n        0.01996140368282795,\\n        -0.00020986569870729,\\n        0.019515715539455414,\\n        -0.010784289799630642,\\n        -0.007610453758388758,\\n        0.020366573706269264,\\n        0.022595012560486794,\\n        0.025606779381632805,\\n        -0.01546400971710682,\\n        0.026065973564982414,\\n        -0.01551803294569254,\\n        -0.020434102043509483,\\n        -0.008501828648149967,\\n        0.010932852514088154,\\n        0.026808785274624825,\\n        0.005871596746146679,\\n        -0.0004243739531375468,\\n        -0.0043994770385324955,\\n        0.03319697454571724,\\n        -0.016760556027293205,\\n        0.0012771313777193427,\\n        0.007765769027173519,\\n        0.002853075973689556,\\n        -0.000697653042152524,\\n        -0.004453499801456928,\\n        -0.012803389690816402,\\n        -0.030036645010113716,\\n        0.02590390481054783,\\n        0.008792201057076454,\\n        0.0159637201577425,\\n        0.025539251044392586,\\n        -0.0046898494474589825,\\n        0.0365193746984005,\\n        -0.02611999586224556,\\n        -0.022203346714377403,\\n        0.0023753123823553324,\\n        0.01688210666179657,\\n        -0.032224565744400024,\\n        -0.0002006860449910164,\\n        0.007286317180842161,\\n        -0.0017059368547052145,\\n        -0.019178073853254318,\\n        -0.0076172067783772945,\\n        0.002799053443595767,\\n        -0.01794905588030815,\\n        -0.03100905381143093,\\n        0.02657919004559517,\\n        -0.008218209259212017,\\n        -0.04481186345219612,\\n        0.003481090534478426,\\n        0.015153379179537296,\\n        -0.004122610669583082,\\n        -0.0030792963225394487,\\n        -0.03446650877594948,\\n        -0.018786408007144928,\\n        0.0302257239818573,\\n        0.007644217927008867,\\n        0.018529800698161125,\\n        -0.0014062795089557767,\\n        -0.003997683059424162,\\n        0.018988993018865585,\\n        -0.008771942928433418,\\n        -0.01928611844778061,\\n        -0.016341879963874817,\\n        -0.013053244911134243,\\n        0.006364554166793823,\\n        0.030873997136950493,\\n        0.0016147735295817256,\\n        0.013087009079754353,\\n        -0.0053887683898210526,\\n        0.01751687377691269,\\n        0.004666214343160391,\\n        -0.0027146427892148495,\\n        -0.05175378546118736,\\n        -0.019137555733323097,\\n        0.0069689336232841015,\\n        0.024161672219634056,\\n        -0.0064523410983383656,\\n        -0.0075901951640844345,\\n        0.01862434111535549,\\n        0.030522849410772324,\\n        0.02611999586224556,\\n        0.012202386744320393,\\n        0.0072593060322105885,\\n        0.010271074250340462,\\n        0.03092801943421364,\\n        0.006749466527253389,\\n        0.01808411255478859,\\n        -0.002189609222114086,\\n        0.0005111058126203716,\\n        0.017935549840331078,\\n        -0.007144507486373186,\\n        0.0018975487910211086,\\n        0.011500091291964054,\\n        -0.025606779381632805,\\n        0.033467087894678116,\\n        -0.009649812243878841,\\n        0.010041477158665657,\\n        -0.026565684005618095,\\n        0.02686280943453312,\\n        0.045081976801157,\\n        0.0035519953817129135,\\n        0.01152034942060709,\\n        -0.005162548273801804,\\n        -0.002206491306424141,\\n        -0.016112282872200012,\\n        0.023702478036284447,\\n        -0.03138721361756325,\\n        0.017611414194107056,\\n        -0.013829821720719337,\\n        0.018165146932005882,\\n        0.027659643441438675,\\n        0.0012366143055260181,\\n        -0.016085270792245865,\\n        -0.00223856745287776,\\n        -0.016868600621819496,\\n        -0.0010838313028216362,\\n        0.01834072172641754,\\n        0.010892335325479507,\\n        -0.026957347989082336,\\n        -0.002802429720759392,\\n        0.010304838418960571,\\n        -0.006236250512301922,\\n        0.0056420001201331615,\\n        -0.01177020464092493,\\n        0.01546400971710682,\\n        0.03430444002151489,\\n        -0.004075340460985899,\\n        -0.008258726447820663,\\n        -0.6651279926300049,\\n        -0.003524984000250697,\\n        0.006502987816929817,\\n        -0.005297604948282242,\\n        -0.0007613830384798348,\\n        0.004645955748856068,\\n        0.01688210666179657,\\n        0.011000380851328373,\\n        -0.007536172401160002,\\n        0.028226882219314575,\\n        -0.01309376209974289,\\n        -0.007718499284237623,\\n        -0.009474238380789757,\\n        -0.0015143250348046422,\\n        -0.019191579893231392,\\n        -0.017395323142409325,\\n        -0.012384713627398014,\\n        -0.016801072284579277,\\n        -0.012816895730793476,\\n        0.008906999602913857,\\n        -0.03057687170803547,\\n        0.022676046937704086,\\n        -0.019340142607688904,\\n        0.015234413556754589,\\n        0.030522849410772324,\\n        -0.012384713627398014,\\n        -0.000990135595202446,\\n        -0.025282643735408783,\\n        -0.02229788713157177,\\n        0.022135818377137184,\\n        -0.0076982406899333,\\n        -0.01122322492301464,\\n        0.027051888406276703,\\n        0.0230812169611454,\\n        0.029469406232237816,\\n        -0.020474620163440704,\\n        -0.004902563989162445,\\n        0.02718694508075714,\\n        -0.04286704584956169,\\n        0.03379122540354729,\\n        0.008204704150557518,\\n        -0.0033173339907079935,\\n        0.021082375198602676,\\n        -0.021501051262021065,\\n        -0.0043522072955966,\\n        -0.016409408301115036,\\n        0.02474241517484188,\\n        -0.0087044145911932,\\n        0.0005136380787007511,\\n        -0.028578029945492744,\\n        -0.01245899498462677,\\n        0.0074753970839083195,\\n        -0.00895426981151104,\\n        -0.005091643426567316,\\n        0.006286896765232086,\\n        0.002908786991611123,\\n        0.01961025595664978,\\n        0.0027095782570540905,\\n        0.019205084070563316,\\n        0.0023263543844223022,\\n        -0.00943372119218111,\\n        0.045811284333467484,\\n        -0.00444337073713541,\\n        -0.013742035254836082,\\n        -0.02753809280693531,\\n        0.003401744645088911,\\n        0.016720037907361984,\\n        0.03092801943421364,\\n        0.01830020360648632,\\n        -0.034601565450429916,\\n        0.0030573494732379913,\\n        0.029226303100585938,\\n        -0.007549678441137075,\\n        0.0075901951640844345,\\n        0.016301361843943596,\\n        0.003043843898922205,\\n        -0.001223952742293477,\\n        0.013816316612064838,\\n        0.020163988694548607,\\n        0.015126368030905724,\\n        -0.015139873139560223,\\n        -0.008785448037087917,\\n        -0.0037782154977321625,\\n        -0.020150482654571533,\\n        0.02155507355928421,\\n        0.007711746264249086,\\n        -0.0025356924161314964,\\n        -0.006908158306032419,\\n        5.09364836034365e-05,\\n        -0.011405551806092262,\\n        -0.012040318921208382,\\n        -0.0013387510553002357,\\n        0.004966715816408396,\\n        -0.041894637048244476,\\n        0.013937867246568203,\\n        0.01801658421754837,\\n        -0.009001539088785648,\\n        0.008994787000119686,\\n        0.00886648241430521,\\n        0.023702478036284447,\\n        0.0038524968549609184,\\n        -0.010858571156859398,\\n        -0.02092030644416809,\\n        0.002834505867213011,\\n        0.005013985559344292,\\n        -0.008218209259212017,\\n        -0.018462272360920906,\\n        0.0330619178712368,\\n        0.04027395322918892,\\n        -0.008744931779801846,\\n        -0.0026420496869832277,\\n        -0.005675764288753271,\\n        0.008852977305650711,\\n        0.009251394309103489,\\n        -0.002478293376043439,\\n        -0.02605246752500534,\\n        0.02002893202006817,\\n        0.0142484987154603,\\n        0.008630133233964443,\\n        -0.008251973427832127,\\n        0.010541187599301338,\\n        -0.011621642857789993,\\n        0.01840825006365776,\\n        -0.024728910997509956,\\n        -0.010014466010034084,\\n        -0.012296927161514759,\\n        -0.0017979444237425923,\\n        -0.008981280960142612,\\n        -0.03387225791811943,\\n        0.009447227232158184,\\n        0.0176654364913702,\\n        -0.0009572154376655817,\\n        -0.004125986713916063,\\n        -0.01274261437356472,\\n        0.001882354961708188,\\n        0.00665492657572031,\\n        -0.019029511138796806,\\n        -0.03854522481560707,\\n        0.022635528817772865,\\n        -0.03932855650782585,\\n        -0.017935549840331078,\\n        0.019137555733323097,\\n        0.017854515463113785,\\n        -0.01794905588030815,\\n        -0.028226882219314575,\\n        -0.020960824564099312,\\n        -0.0015404922887682915,\\n        0.001764180138707161,\\n        -0.028902167454361916,\\n        -0.0065637631341814995,\\n        0.009636306203901768,\\n        -0.01233744341880083,\\n        -0.02087979018688202,\\n        0.013289595022797585,\\n        -0.0148157374933362,\\n        -0.011466327123343945,\\n        -0.016233833506703377,\\n        -0.014937288127839565,\\n        -0.023634949699044228,\\n        -0.03160330280661583,\\n        0.013141032308340073,\\n        0.015369470231235027,\\n        -0.021136397495865822,\\n        -0.022905642166733742,\\n        -0.024999024346470833,\\n        -0.01688210666179657,\\n        -0.012729108333587646,\\n        0.03967970237135887,\\n        -0.04575726389884949,\\n        -0.025863388553261757,\\n        0.023837534710764885,\\n        -0.012317185290157795,\\n        0.003997683059424162,\\n        0.010210298001766205,\\n        -0.0060843112878501415,\\n        0.025863388553261757,\\n        -0.017138715833425522,\\n        -0.006320660933852196,\\n        0.011993048712611198,\\n        -0.006492858286947012,\\n        0.015653088688850403,\\n        0.0036060181446373463,\\n        -0.02169013023376465,\\n        0.0016586669953539968,\\n        0.009291911497712135,\\n        0.011446068063378334,\\n        0.04491991177201271,\\n        0.013316606171429157,\\n        -0.0022942782379686832,\\n        0.009913173504173756,\\n        -0.004818153101950884,\\n        0.0071850246749818325,\\n        -0.018070606514811516,\\n        0.01003472413867712,\\n        -0.0009344246354885399,\\n        0.0153964813798666,\\n        0.015653088688850403,\\n        0.0055001904256641865,\\n        0.0030944901518523693,\\n        0.03743775933980942,\\n        0.029037224128842354,\\n        0.00429143151268363,\\n        0.01928611844778061,\\n        -0.010500670410692692,\\n        0.009609295055270195,\\n        0.017017163336277008,\\n        -0.0009310481837019324,\\n        -0.025890398770570755,\\n        0.008191198110580444,\\n        -0.0011327894171699882,\\n        0.005797315388917923,\\n        -0.027740677818655968,\\n        -0.009453980252146721,\\n        -0.010730267502367496,\\n        0.015693606808781624,\\n        0.02286512590944767,\\n        -0.0020815636962652206,\\n        0.004818153101950884,\\n        -0.034709613770246506,\\n        0.017705954611301422,\\n        0.0213795006275177,\\n        -0.0056521291844546795,\\n        -0.011412303894758224,\\n        -0.0026369851548224688,\\n        0.005837832577526569,\\n        0.003325775032863021,\\n        0.034034326672554016,\\n        0.04654059186577797,\\n        -0.009258147329092026,\\n        -0.002203115029260516,\\n        -0.018880948424339294,\\n        0.024161672219634056,\\n        0.014950794167816639,\\n        0.013755540363490582,\\n        -0.02013697661459446,\\n        -0.0006900561274960637,\\n        0.01609877683222294,\\n        -0.0067528425715863705,\\n        0.00794809591025114,\\n        -0.025228621438145638,\\n        0.007610453758388758,\\n        0.01670653373003006,\\n        -0.00128557241987437,\\n        -0.014113441109657288,\\n        0.016828084364533424,\\n        0.002250384772196412,\\n        0.03805902227759361,\\n        0.01677406206727028,\\n        0.009960442781448364,\\n        -0.003947036806493998,\\n        -0.0043994770385324955,\\n        -0.008711167611181736,\\n        -0.002922292798757553,\\n        -0.0003201269428245723,\\n        -0.01635538600385189,\\n        0.0037241927348077297,\\n        -0.0025930916890501976,\\n        -0.013458415865898132,\\n        0.011689171195030212,\\n        0.03784292936325073,\\n        0.01943468116223812,\\n        0.022676046937704086,\\n        0.02884814515709877,\\n        -0.0017658683937042952,\\n        -0.006334166508167982,\\n        0.014464588835835457,\\n        0.011736440472304821,\\n        -0.004622321110218763,\\n        -0.004787765443325043,\\n        0.002299343002960086,\\n        0.008535593748092651,\\n        -0.014640163630247116,\\n        0.0021372747141867876,\\n        -0.014140453189611435,\\n        0.01840825006365776,\\n        0.009291911497712135,\\n        -0.019772322848439217,\\n        -0.0008521243580617011,\\n        0.019704794511198997,\\n        0.0174088291823864,\\n        -0.006877770181745291,\\n        -0.027011370286345482,\\n        -0.006641421001404524,\\n        0.0076982406899333,\\n        -0.0059053609147667885,\\n        -0.025998445227742195,\\n        0.013505685143172741,\\n        0.005375262815505266,\\n        -0.016476936638355255,\\n        0.0035823830403387547,\\n        -0.009899667464196682,\\n        -0.009339181706309319,\\n        -0.006320660933852196,\\n        -0.013228818774223328,\\n        0.0018975487910211086,\\n        -0.012101094238460064,\\n        0.011304258368909359,\\n        0.013742035254836082,\\n        0.0142620038241148,\\n        -0.03011767938733101,\\n        -0.002380377147346735,\\n        -0.0060539236292243,\\n        -0.008137175813317299,\\n        -0.008879988454282284,\\n        -0.0003192828444298357,\\n        0.010568198747932911,\\n        -0.0006221056682989001,\\n        -0.012884424068033695,\\n        -0.020717721432447433,\\n        -0.0225409884005785,\\n        0.017476357519626617,\\n        0.007954848930239677,\\n        -0.0030235853046178818,\\n        -0.032791804522275925,\\n        0.005861467681825161,\\n        0.016652509570121765,\\n        -0.0033105812035501003,\\n        0.018961982801556587,\\n        0.03357513248920441,\\n        0.012506265193223953,\\n        -0.002544133458286524,\\n        -0.04046303406357765,\\n        0.02020450495183468,\\n        -0.004372465889900923,\\n        0.0856260433793068,\\n        0.002562703797593713,\\n        -0.02279759757220745,\\n        0.020650193095207214,\\n        -0.01189175620675087,\\n        0.019340142607688904,\\n        -0.01734130084514618,\\n        -0.010777536779642105,\\n        0.02452632412314415,\\n        -0.002885152120143175,\\n        -0.025917410850524902,\\n        -0.013492180034518242,\\n        0.00851533468812704,\\n        -0.004899187479168177,\\n        0.025390688329935074,\\n        0.017827505245804787,\\n        -0.0032312353141605854,\\n        0.001186812063679099,\\n        0.010163028724491596,\\n        -0.0230812169611454,\\n        -0.0020072825718671083,\\n        -0.009393204003572464,\\n        0.016017742455005646,\\n        -0.0025998444762080908,\\n        -0.007144507486373186,\\n        -0.013451662845909595,\\n        -0.0039943065494298935,\\n        0.00806964747607708,\\n        0.022770585492253304,\\n        -0.00948774442076683,\\n        0.01075727865099907,\\n        0.0030759198125451803,\\n        -0.024013109505176544,\\n        -0.0038457440678030252,\\n        -0.015599066391587257,\\n        0.011101673357188702,\\n        0.011621642857789993,\\n        -0.012445488944649696,\\n        0.012870918028056622,\\n        -0.021365994587540627,\\n        0.015153379179537296,\\n        0.03206249698996544,\\n        0.013478673994541168,\\n        -0.016720037907361984,\\n        -0.0028193118050694466,\\n        -0.013296347111463547,\\n        0.001635876134969294,\\n        0.005398897919803858,\\n        -0.009703835472464561,\\n        -0.0028125590179115534,\\n        0.011297506280243397,\\n        -0.005935749039053917,\\n        -0.030603883787989616,\\n        0.009629554115235806,\\n        0.014802231453359127,\\n        0.013647494837641716,\\n        -0.01339764054864645,\\n        -0.002650490729138255,\\n        -0.0028935931622982025,\\n        -0.022743575274944305,\\n        -0.03376421332359314,\\n        -0.009656565263867378,\\n        -0.007164766080677509,\\n        -0.0011716182343661785,\\n        0.0035925123374909163,\\n        -0.03389926999807358,\\n        -0.006462470628321171,\\n        -0.018921464681625366,\\n        -0.005827703513205051,\\n        0.009595789946615696,\\n        0.025390688329935074,\\n        -0.02452632412314415,\\n        -0.04448772966861725,\\n        -0.004747248254716396,\\n        0.026822291314601898,\\n        -0.0008090749615803361,\\n        -0.005034244153648615,\\n        0.0011783710215240717,\\n        -0.024688392877578735,\\n        0.011358281597495079,\\n        -0.009778115898370743,\\n        -0.02566080167889595,\\n        -0.018597329035401344,\\n        -0.009055562317371368,\\n        -0.011648654006421566,\\n        0.005040997173637152,\\n        -0.0031890301033854485,\\n        0.015761135146021843,\\n        -0.0031569539569318295,\\n        0.011594630777835846,\\n        -0.005294228903949261,\\n        0.01893497072160244,\\n        0.04562220722436905,\\n        -0.03614121302962303,\\n        0.007792780641466379,\\n        0.017462851479649544,\\n        -0.003997683059424162,\\n        0.0067798541858792305,\\n        0.009987454861402512,\\n        -0.0225139781832695,\\n        -0.0092716533690691,\\n        -0.026768269017338753,\\n        -0.016436418518424034,\\n        0.012587298639118671,\\n        0.011398798786103725,\\n        0.016328373923897743,\\n        0.00027391218463890254,\\n        0.029280327260494232,\\n        0.018097618594765663,\\n        -0.020933812484145164,\\n        -0.006695443764328957,\\n        -0.04184061288833618,\\n        -0.009048809297382832,\\n        0.004129363223910332,\\n        0.003528360277414322,\\n        0.002262202324345708,\\n        0.0159637201577425,\\n        0.018880948424339294,\\n        -0.0005047749727964401,\\n        -0.005037620663642883,\\n        -0.017084691673517227,\\n        -0.03808603435754776,\\n        0.032089509069919586,\\n        0.022527484223246574,\\n        -0.003980800975114107,\\n        -0.022784091532230377,\\n        0.004348830785602331,\\n        -0.005885102320462465,\\n        -0.011142190545797348,\\n        0.015504526905715466,\\n        -0.013809563592076302,\\n        0.004048329312354326,\\n        -0.02067720517516136,\\n        -0.04297509044408798,\\n        -0.020272033289074898,\\n        -0.000584120920393616,\\n        -0.022149324417114258,\\n        0.020906800404191017,\\n        -0.017125209793448448,\\n        0.00536175724118948,\\n        -0.0036296530161052942,\\n        -0.019907381385564804,\\n        -0.011331270448863506,\\n        -0.01546400971710682,\\n        0.04640553519129753,\\n        -0.030468827113509178,\\n        -0.027848724275827408,\\n        0.016720037907361984,\\n        -0.023000182583928108,\\n        0.028875155374407768,\\n        -0.014586140401661396,\\n        -0.002562703797593713,\\n        -0.03692454472184181,\\n        0.003670169971883297,\\n        0.0022588258143514395,\\n        -0.015545044094324112,\\n        -0.019421175122261047,\\n        -0.022216852754354477,\\n        0.03670845180749893,\\n        0.010169780813157558,\\n        0.012202386744320393,\\n        -0.004801271017640829,\\n        0.003869378939270973,\\n        0.011155696585774422,\\n        -0.006401694845408201,\\n        -0.0010661050910130143,\\n        -0.004784388933330774,\\n        -0.014005395583808422,\\n        -0.0001811661059036851,\\n        0.0048687998205423355,\\n        0.026038961485028267,\\n        -0.016368890181183815,\\n        0.02356742136180401,\\n        0.017989574000239372,\\n        0.025228621438145638,\\n        0.02162260189652443,\\n        0.0005600639269687235,\\n        -0.009946937672793865,\\n        -0.02618752419948578,\\n        -0.013897350057959557,\\n        -0.009690329432487488,\\n        0.009845645166933537,\\n        0.003646535100415349,\\n        -0.003062414238229394,\\n        -0.024999024346470833,\\n        0.001968453638255596,\\n        0.0153964813798666,\\n        0.006638044491410255,\\n        -0.006604280322790146,\\n        0.02349989302456379,\\n        0.011885003186762333,\\n        -0.018070606514811516,\\n        -0.0006478508585132658,\\n        0.007353845983743668,\\n        0.045325081795454025,\\n        -0.0020883167162537575,\\n        -0.011756699532270432,\\n        0.004082093480974436,\\n        0.002177791902795434,\\n        0.01245899498462677,\\n        0.028226882219314575,\\n        -0.0009521508472971618,\\n        -0.015153379179537296,\\n        -0.0033291515428572893,\\n        -0.024202188476920128,\\n        0.012384713627398014,\\n        -0.0008930634357966483,\\n        -0.012891177088022232,\\n        0.002405700273811817,\\n        -0.004710108041763306,\\n        -0.025228621438145638,\\n        -0.008224962279200554,\\n        -0.02502603456377983,\\n        -0.009413463063538074,\\n        0.011776957660913467,\\n        0.0009504626505076885,\\n        -0.005760174710303545,\\n        0.007684735115617514,\\n        -0.015707112848758698,\\n        -0.017395323142409325,\\n        0.012627815827727318,\\n        -0.003528360277414322,\\n        0.03443949669599533,\\n        -0.002047799527645111,\\n        0.02035306766629219,\\n        -0.002650490729138255,\\n        0.023243285715579987,\\n        -0.030873997136950493,\\n        0.009670071303844452,\\n        0.0006254820618778467,\\n        -0.0008719608304090798,\\n        0.01688210666179657,\\n        -0.010588457807898521,\\n        0.0068541355431079865,\\n        -0.01546400971710682,\\n        0.011331270448863506,\\n        0.02056915871798992,\\n        -0.007272811606526375,\\n        -0.023905063048005104,\\n        0.008170939981937408,\\n        0.012114600278437138,\\n        -0.006340919528156519,\\n        -0.009426968172192574,\\n        -0.019299624487757683,\\n        0.005405650474131107,\\n        0.03965269401669502,\\n        0.001683990121819079,\\n        0.007090484723448753,\\n        -0.005631871055811644,\\n        -0.01819215901196003,\\n        -0.014856253750622272,\\n        0.009838892146945,\\n        -0.013208560645580292,\\n        0.014559129253029823,\\n        0.020663699135184288,\\n        -0.017152220010757446,\\n        -0.009980701841413975,\\n        0.023905063048005104,\\n        0.00895426981151104,\\n        0.005885102320462465,\\n        -0.010244062170386314,\\n        -0.005341498646885157,\\n        -0.04086820408701897,\\n        0.01572061888873577,\\n        -0.01038587186485529,\\n        -0.011425809934735298,\\n        0.013525944203138351,\\n        0.005638623610138893,\\n        0.00803588330745697,\\n        0.028469985350966454,\\n        -0.028794120997190475,\\n        -0.03687052056193352,\\n        0.011939026415348053,\\n        -0.006982439663261175,\\n        0.022838113829493523,\\n        0.01038587186485529,\\n        0.009690329432487488,\\n        -0.0046256971545517445,\\n        -0.009028551168739796,\\n        -0.042380839586257935,\\n        0.010392624884843826,\\n        0.010554693639278412,\\n        -0.010615468956530094,\\n        -0.011067909188568592,\\n        0.0087044145911932,\\n        0.011338023468852043,\\n        -0.023905063048005104,\\n        0.007022956386208534,\\n        0.0050680083222687244,\\n        0.010264321230351925,\\n        -0.03927453234791756,\\n        0.004186762496829033,\\n        -0.0002825642586685717,\\n        0.029928598552942276,\\n        0.0072120362892746925,\\n        0.006479352712631226,\\n        0.002410764805972576,\\n        0.014437577687203884,\\n        -0.029577450826764107,\\n        0.001482249004766345,\\n        -0.0041833859868347645,\\n        0.011682418175041676,\\n        0.008414042182266712,\\n        0.016544464975595474,\\n        -0.0023212896194308996,\\n        -0.03673546388745308,\\n        0.02216283045709133,\\n        -0.013519191183149815,\\n        -0.020380079746246338,\\n        -0.013586719520390034,\\n        0.02223035879433155,\\n        0.008204704150557518,\\n        0.011810721829533577,\\n        -0.005979642271995544,\\n        -0.02183869294822216,\\n        0.009426968172192574,\\n        0.009832139126956463,\\n        0.002562703797593713,\\n        -0.0005908737657591701,\\n        -0.007056720554828644,\\n        -0.013843327760696411,\\n        -0.00681361835449934,\\n        0.03862626105546951,\\n        -0.024026615545153618,\\n        0.0035992651246488094,\\n        -0.0015751005848869681,\\n        -0.005898608360439539,\\n        -0.0014273821143433452,\\n        -0.0307929627597332,\\n        -0.003504725405946374,\\n        -0.03230559825897217,\\n        0.017638426274061203,\\n        -0.0020950695034116507,\\n        -0.010649233125150204,\\n        0.03000963293015957,\\n        -0.00400781212374568,\\n        -0.006232874002307653,\\n        0.027659643441438675,\\n        0.019556231796741486,\\n        -0.01035210769623518,\\n        -0.017287276685237885,\\n        0.013586719520390034,\\n        0.01563958451151848,\\n        0.01992088556289673,\\n        0.007022956386208534,\\n        0.014802231453359127,\\n        0.011473080143332481,\\n        -0.01093960553407669,\\n        -0.011000380851328373,\\n        -0.03635730594396591,\\n        -0.016733543947339058,\\n        0.012904682196676731,\\n        0.025188103318214417,\\n        -0.00637130718678236,\\n        -0.02505304664373398,\\n        -0.006648173555731773,\\n        -0.012303679250180721,\\n        -0.008366771973669529,\\n        -0.012621062807738781,\\n        -0.0029476159252226353,\\n        0.01132451742887497,\\n        0.011405551806092262,\\n        -0.003866002429276705,\\n        0.0021541567984968424,\\n        -0.01447809487581253,\\n        -0.011560866609215736,\\n        -0.01476171426475048,\\n        0.010196792893111706,\\n        -0.01830020360648632,\\n        -0.013600225560367107,\\n        0.014788725413382053,\\n        -0.00618560379371047,\\n        -0.03009066730737686,\\n        -0.021393006667494774,\\n        -0.008839471265673637,\\n        -0.023418858647346497,\\n        -0.02439126744866371,\\n        0.006921663880348206,\\n        -0.004537910223007202,\\n        -0.003069167025387287,\\n        -0.012431983835995197,\\n        0.03497972711920738,\\n        0.00257114483974874,\\n        0.02343236468732357,\\n        0.007340339943766594,\\n        -0.0170981977134943,\\n        -0.006921663880348206,\\n        0.0005136380787007511,\\n        -0.007853556424379349,\\n        -0.020758239552378654,\\n        0.0472969114780426,\\n        0.01646343059837818,\\n        -0.01834072172641754,\\n        0.024796439334750175,\\n        -0.020690711215138435,\\n        -0.0104061309248209,\\n        0.020839272066950798,\\n        0.007205283269286156,\\n        -0.0015995795838534832,\\n        0.028307916596531868,\\n        -0.007954848930239677,\\n        -0.0005482464330270886,\\n        0.0008061206317506731,\\n        -0.007813039235770702,\\n        -0.027159933000802994,\\n        -0.002035982208326459,\\n        -0.0029357983730733395,\\n        -0.011939026415348053,\\n        0.009859150275588036,\\n        -0.012951952405273914,\\n        -0.00621261540800333,\\n        0.012641321867704391,\\n        -0.015247918665409088,\\n        -0.013883844949305058,\\n        0.01286416593939066,\\n        -0.004331948701292276,\\n        0.004152998328208923,\\n        -0.0404900461435318,\\n        0.006243003066629171,\\n        -0.0006841474096290767,\\n        -0.02895618975162506,\\n        0.014991311356425285,\\n        -0.01886744238436222,\\n        -0.013688012026250362,\\n        -0.00980512797832489,\\n        -0.0017110015032812953,\\n        -0.0030218970496207476,\\n        -0.019907381385564804,\\n        -0.003005014965310693,\\n        0.030765952542424202,\\n        0.007914331741631031,\\n        0.002878399332985282,\\n        -0.008562604896724224,\\n        0.017017163336277008,\\n        -0.011743193492293358,\\n        0.003190718125551939,\\n        0.22646333277225494,\\n        -0.011216471903026104,\\n        0.006151839625090361,\\n        0.009001539088785648,\\n        -0.010899088345468044,\\n        0.005429285578429699,\\n        0.015328953042626381,\\n        -0.00955527275800705,\\n        0.00781979225575924,\\n        0.020218010991811752,\\n        0.007542925421148539,\\n        -0.0182326752692461,\\n        -0.03217054158449173,\\n        0.01242523081600666,\\n        0.0015075721312314272,\\n        0.006320660933852196,\\n        -0.029334349557757378,\\n        -0.03281881660223007,\\n        -0.002746718702837825,\\n        -0.035763055086135864,\\n        0.013141032308340073,\\n        -0.003474337514489889,\\n        -0.02059617079794407,\\n        -0.02109588123857975,\\n        0.04300210252404213,\\n        0.009224383160471916,\\n        0.004237408749759197,\\n        -0.005550836678594351,\\n        0.023378342390060425,\\n        0.003764709923416376,\\n        -0.02020450495183468,\\n        0.005577848292887211,\\n        -0.0034338205587118864,\\n        0.004568298347294331,\\n        -0.0202450230717659,\\n        0.006631291471421719,\\n        -0.007988613098859787,\\n        -0.0082654794678092,\\n        0.021109387278556824,\\n        0.007596948184072971,\\n        0.0003009235661011189,\\n        0.013249077834188938,\\n        -0.016220327466726303,\\n        0.008643638342618942,\\n        0.013019480742514133,\\n        0.018178652971982956,\\n        -0.0002962809812743217,\\n        -0.03114411048591137,\\n        -0.0038086033891886473,\\n        0.025525745004415512,\\n        -0.019529221579432487,\\n        0.00334434537217021,\\n        0.002144027501344681,\\n        0.037815921008586884,\\n        -0.012884424068033695,\\n        0.01805710233747959,\\n        -0.00022326585894916207,\\n        0.0004566609859466553,\\n        -0.0013632301706820726,\\n        0.031063076108694077,\\n        0.006367930676788092,\\n        0.00592899601906538,\\n        -0.006914910860359669,\\n        0.04845840111374855,\\n        0.008292490616440773,\\n        0.03408835083246231,\\n        -0.016260845586657524,\\n        0.015409987419843674,\\n        0.008292490616440773,\\n        -0.03581707924604416,\\n        -0.003082672832533717,\\n        -0.026822291314601898,\\n        -0.0005174365942366421,\\n        -0.01453211810439825,\\n        -0.018070606514811516,\\n        -0.007907578721642494,\\n        0.013127526268362999,\\n        0.008177693001925945,\\n        0.02048812434077263,\\n        0.016193317249417305,\\n        -0.022676046937704086,\\n        -0.03203548491001129,\\n        -0.012479253113269806,\\n        -0.005344875156879425,\\n        0.002648802474141121,\\n        -0.023418858647346497,\\n        -0.0076644765213131905,\\n        -0.0028328176122158766,\\n        -0.006681937724351883,\\n        -0.008690908551216125,\\n        0.025755342096090317,\\n        -0.024688392877578735,\\n        -0.023972591385245323,\\n        -0.010095500387251377,\\n        0.008137175813317299,\\n        -0.016557971015572548,\\n        0.017044175416231155,\\n        0.014869759790599346,\\n        0.0068541355431079865,\\n        0.013012727722525597,\\n        0.00861662719398737,\\n        0.02441827952861786,\\n        0.012344196438789368,\\n        0.011351528577506542,\\n        -0.01985335722565651,\\n        -0.00024162515182979405,\\n        0.0002707467938307673,\\n        0.023878052830696106,\\n        0.0026910079177469015,\\n        -0.01692262478172779,\\n        0.005000479985028505,\\n        -0.03225157782435417,\\n        0.0023938827216625214,\\n        -0.01985335722565651,\\n        0.010264321230351925,\\n        -0.021176915615797043,\\n        -0.022149324417114258,\\n        0.0007200218387879431,\\n        0.01751687377691269,\\n        -0.02356742136180401,\\n        -0.004882305394858122,\\n        -0.025255631655454636,\\n        0.008360018953680992,\\n        0.02039358578622341,\\n        0.004943080712109804,\\n        -0.024999024346470833,\\n        -0.006050547119230032,\\n        -0.007299823220819235,\\n        0.013168043456971645,\\n        -0.004642579238861799,\\n        0.01801658421754837,\\n        -0.03606018051505089,\\n        0.011966037563979626,\\n        -0.000476497458294034,\\n        -0.0037376985419541597,\\n        -0.010682997293770313,\\n        0.018138134852051735,\\n        -0.011034145019948483,\\n        -0.00955527275800705,\\n        0.01060196291655302,\\n        -0.0024985517375171185,\\n        -0.019772322848439217,\\n        0.006800112780183554,\\n        0.007644217927008867,\\n        0.0006339231040328741,\\n        -0.016841590404510498,\\n        0.009994206950068474,\\n        0.001396150211803615,\\n        -0.01748986355960369,\\n        -0.007016203831881285,\\n        -0.013174796476960182,\\n        -0.005797315388917923,\\n        -0.000915854296181351,\\n        -0.012195633724331856,\\n        0.0032329235691577196,\\n        0.002496863715350628,\\n        -0.0006795048248022795,\\n        -0.018097618594765663,\\n        0.02092030644416809,\\n        -0.0033156457357108593,\\n        -0.026038961485028267,\\n        -0.014491600915789604,\\n        0.010865324176847935,\\n        0.00835326686501503,\\n        -0.013735282234847546,\\n        0.015072344802320004,\\n        -0.17384518682956696,\\n        0.004551416262984276,\\n        0.019529221579432487,\\n        -0.01777348294854164,\\n        0.01758440211415291,\\n        -0.021298466250300407,\\n        -0.003585759550333023,\\n        -0.0009783180430531502,\\n        -0.02159559167921543,\\n        0.02354040928184986,\\n        0.04232681915163994,\\n        -0.03114411048591137,\\n        -0.021987255662679672,\\n        -0.024593854323029518,\\n        -0.0013269336195662618,\\n        0.004180009476840496,\\n        0.007873814553022385,\\n        -0.008751683868467808,\\n        0.014896770939230919,\\n        0.02240593172609806,\\n        0.020380079746246338,\\n        -0.002083251951262355,\\n        0.005000479985028505,\\n        0.0038457440678030252,\\n        -0.0040044356137514114,\\n        -0.002125457162037492,\\n        -0.016841590404510498,\\n        0.037572816014289856,\\n        0.007718499284237623,\\n        -0.009636306203901768,\\n        0.013525944203138351,\\n        0.0022588258143514395,\\n        0.013262582942843437,\\n        0.016341879963874817,\\n        -0.004564921837300062,\\n        0.010325096547603607,\\n        -0.007650970946997404,\\n        -0.029010212048888206,\\n        -0.021298466250300407,\\n        0.015950214117765427,\\n        0.02655217796564102,\\n        0.017287276685237885,\\n        0.011020638979971409,\\n        -0.00978486891835928,\\n        -0.01939416490495205,\\n        -0.00444337073713541,\\n        0.021433522924780846,\\n        -0.004791141953319311,\\n        0.022878631949424744,\\n        -0.04427163675427437,\\n        0.01646343059837818,\\n        0.010581704787909985,\\n        -0.002368559595197439,\\n        -0.018462272360920906,\\n        0.0182326752692461,\\n        0.003511478193104267,\\n        0.00946748536080122,\\n        0.023594433441758156,\\n        -0.014275509864091873,\\n        0.0014383555389940739,\\n        -0.0005305202212184668,\\n        -0.01511286199092865,\\n        0.029469406232237816,\\n        -0.01343140471726656,\\n        0.017395323142409325,\\n        -0.003045532153919339,\\n        0.01447809487581253,\\n        0.00055964186321944,\\n        -0.012330691330134869,\\n        0.006192356813699007,\\n        -0.017476357519626617,\\n        -0.006533375475555658,\\n        -0.009575530886650085,\\n        -0.026228042319417,\\n        -0.018840432167053223,\\n        0.007954848930239677,\\n        -0.027267979457974434,\\n        -0.003572253743186593,\\n        -0.020218010991811752,\\n        0.009231136180460453,\\n        -0.004078716970980167,\\n        0.03795097768306732,\\n        -0.01624733954668045,\\n        0.009143348783254623,\\n        -0.010608715936541557,\\n        -0.0031029311940073967,\\n        0.009028551168739796,\\n        0.01620682328939438,\\n        0.009845645166933537,\\n        -0.02749757654964924,\\n        0.023986097425222397,\\n        -0.03011767938733101,\\n        -0.005040997173637152,\\n        -0.01504533365368843,\\n        0.030522849410772324,\\n        0.01447809487581253,\\n        0.015301941893994808,\\n        0.005587977357208729,\\n        -0.0028615170158445835,\\n        -0.0005701931659132242,\\n        -0.0092716533690691,\\n        -0.01659848727285862,\\n        0.002939174883067608,\\n        -0.008981280960142612,\\n        -0.013485427014529705,\\n        0.012479253113269806,\\n        0.023918569087982178,\\n        -0.006209238898009062,\\n        0.04005786404013634,\\n        -0.0005959383561275899,\\n        -0.02046111412346363,\\n        0.0011024016421288252,\\n        0.022432943806052208,\\n        0.01635538600385189,\\n        0.0024732286110520363,\\n        0.02806481532752514,\\n        0.011081415228545666,\\n        -0.00823171529918909,\\n        0.010959863662719727,\\n        0.01545050460845232,\\n        0.07476747781038284,\\n        0.014086429961025715,\\n        -0.009933431632816792,\\n        0.0028885283973068,\\n        -0.007461891509592533,\\n        -0.03795097768306732,\\n        -0.09972598403692245,\\n        -0.021393006667494774,\\n        0.013039739802479744,\\n        0.006621162407100201,\\n        0.015572055242955685,\\n        -0.001142918597906828,\\n        0.013586719520390034,\\n        -0.010196792893111706,\\n        0.013417898677289486,\\n        0.021136397495865822,\\n        -0.018219169229269028,\\n        -0.017638426274061203,\\n        -0.015288435854017735,\\n        -0.03217054158449173,\\n        -0.015436998568475246,\\n        -0.007542925421148539,\\n        -0.003953789360821247,\\n        -0.019475199282169342,\\n        -0.0048789288848638535,\\n        0.009190618991851807,\\n        -0.004926198627799749,\\n        -0.012634568847715855,\\n        -0.01613929495215416,\\n        -0.011405551806092262,\\n        -0.01459964644163847,\\n        -0.02166312001645565,\\n        -0.014005395583808422,\\n        0.008873235434293747,\\n        0.008724672719836235,\\n        0.006455717608332634,\\n        -0.0165309589356184,\\n        -0.0279297586530447,\\n        0.011148943565785885,\\n        0.0030404673889279366,\\n        0.016260845586657524,\\n        0.01666601561009884,\\n        -0.03954464569687843,\\n        -0.0018435260280966759,\\n        0.00558460084721446,\\n        -0.010163028724491596,\\n        0.0061585926450788975,\\n        0.021987255662679672,\\n        0.009008292108774185,\\n        -0.009177112951874733,\\n        0.030063655227422714,\\n        -0.01819215901196003,\\n        -0.019664278253912926,\\n        0.013492180034518242,\\n        0.005696022883057594,\\n        0.01762492023408413,\\n        -0.011013886891305447,\\n        -0.002336483681574464,\\n        -0.024958506226539612,\\n        -0.01180396880954504,\\n        0.015193896368145943,\\n        -0.018772903829813004,\\n        -0.004163127392530441,\\n        -0.017192738130688667,\\n        -0.008042635396122932,\\n        -0.002027540933340788,\\n        0.00580406840890646,\\n        -0.002078187419101596,\\n        -0.01847577840089798,\\n        0.03344007581472397,\\n        0.03676247596740723,\\n        0.0013092074077576399,\\n        0.0012180439662188292,\\n        -0.008792201057076454,\\n        0.02325678989291191,\\n        -0.005628494545817375,\\n        -0.023580927401781082,\\n        0.02467488683760166,\\n        0.006226120982319117,\\n        -0.010912594385445118,\\n        -0.023770006373524666,\\n        0.0002443685079924762,\\n        -0.015990732237696648,\\n        0.015085850842297077,\\n        -0.01897548884153366,\\n        -0.011020638979971409,\\n        -0.036789488047361374,\\n        -0.009933431632816792,\\n        -0.007826544344425201,\\n        -0.015315447002649307,\\n        -0.0008293334976769984,\\n        -0.012783131562173367,\\n        -0.03271077200770378,\\n        -0.042029693722724915,\\n        0.012330691330134869,\\n        -0.02930733747780323,\\n        -0.012661579996347427,\\n        0.026038961485028267,\\n        0.03606018051505089,\\n        0.013249077834188938,\\n        -0.010750525631010532,\\n        0.015004816465079784,\\n        -0.0024749168660491705,\\n        -0.018664857372641563,\\n        2.4663702788529918e-05,\\n        0.010392624884843826,\\n        0.014275509864091873,\\n        -0.003653287887573242,\\n        -0.03522282838821411,\\n        -0.005581224337220192,\\n        0.006546881049871445,\\n        -0.003381486050784588,\\n        -0.014221486635506153,\\n        -0.013066750951111317,\\n        0.006891276221722364,\\n        -0.005915490444749594,\\n        -0.015693606808781624,\\n        -0.0026791903655976057,\\n        -0.007239047437906265,\\n        0.006837253458797932,\\n        0.008947516791522503,\\n        0.009345934726297855,\\n        -0.014207981526851654,\\n        -0.013444909825921059,\\n        0.025809364393353462,\\n        0.0012138234451413155,\\n        -0.003781592007726431,\\n        0.007468644063919783,\\n        -0.0076644765213131905,\\n        -0.0014408878050744534,\\n        0.014748208224773407,\\n        0.0005887635052204132,\\n        -0.0048316591419279575,\\n        -0.01208758819848299,\\n        0.008684155531227589,\\n        -0.00230103125795722,\\n        -0.028469985350966454,\\n        -0.011594630777835846,\\n        0.019556231796741486,\\n        -0.00863688625395298,\\n        0.004618944600224495,\\n        0.0279567688703537,\\n        -0.002799053443595767,\\n        -0.012296927161514759,\\n        -0.0012433672090992332,\\n        0.019839851185679436,\\n        0.011783710680902004,\\n        -0.005625118035823107,\\n        -0.015261424705386162,\\n        -0.028713086619973183,\\n        0.004186762496829033,\\n        -0.02644413150846958,\\n        0.013444909825921059,\\n        -0.0031198132783174515,\\n        -0.025188103318214417,\\n        -0.00018749690207187086,\\n        0.02718694508075714,\\n        -0.013870338909327984,\\n        0.0148292426019907,\\n        0.014842748641967773,\\n        0.0004252180806361139,\\n        0.006050547119230032,\\n        -0.008738178759813309,\\n        -0.02884814515709877,\\n        0.007293070200830698,\\n        0.008170939981937408,\\n        0.008812460117042065,\\n        -0.016017742455005646,\\n        0.03298088535666466,\\n        0.012634568847715855,\\n        -0.00032899004872888327,\\n        -0.0006440524011850357,\\n        0.03352111205458641,\\n        0.0020680581219494343,\\n        -0.013262582942843437,\\n        0.005554213188588619,\\n        -0.002520498586818576,\\n        -0.03238663449883461,\\n        -0.0017321042250841856,\\n        -0.011216471903026104,\\n        0.009562025777995586,\\n        -0.0026049090083688498,\\n        0.04267796501517296,\\n        0.0037174399476498365,\\n        0.02039358578622341,\\n        0.004679719917476177,\\n        -0.028767110779881477,\\n        0.026781775057315826,\\n        0.009190618991851807,\\n        -0.020717721432447433,\\n        -0.01563958451151848,\\n        0.009285158477723598,\\n        0.04705381020903587,\\n        0.002957745222374797,\\n        -0.018840432167053223,\\n        0.017678942531347275,\\n        -0.004409606568515301,\\n        0.00017958341049961746,\\n        0.008785448037087917,\\n        0.008684155531227589,\\n        0.011939026415348053,\\n        -0.007495655678212643,\\n        0.008758436888456345,\\n        0.013357123360037804,\\n        -0.0028682700358331203,\\n        0.013600225560367107,\\n        0.01812463067471981,\\n        0.01072351448237896,\\n        0.012310432270169258,\\n        0.0028699582908302546,\\n        -0.01950220949947834,\\n        -0.0014839372597634792,\\n        -0.01286416593939066,\\n        -0.0007060941425152123,\\n        -0.008360018953680992,\\n        -0.02208179607987404,\\n        -0.010041477158665657,\\n        0.022838113829493523,\\n        0.023175757378339767,\\n        0.00923788920044899,\\n        0.0267952810972929,\\n        0.01242523081600666,\\n        -0.027200451120734215,\\n        0.011128684505820274,\\n        0.010966616682708263,\\n        -0.0065164933912456036,\\n        -0.014842748641967773,\\n        0.007286317180842161,\\n        -0.002145715756341815,\\n        -0.0007896605529822409,\\n        0.04324520379304886,\\n        -0.016125788912177086,\\n        0.02133898250758648,\\n        -0.006249756086617708,\\n        0.007752263452857733,\\n        -0.033034905791282654,\\n        0.008042635396122932,\\n        0.009305417537689209,\\n        0.01600423827767372,\\n        -0.010568198747932911,\\n        -0.0014366672839969397,\\n        -0.0142484987154603,\\n        0.008056141436100006,\\n        0.000470588740427047,\\n        0.0022115560714155436,\\n        0.05064631998538971,\\n        -0.02244644984602928,\\n        0.03338605538010597,\\n        0.0041462453082203865,\\n        0.0004359804152045399,\\n        -0.027848724275827408,\\n        0.007455138489603996,\\n        0.01488326583057642,\\n        0.011662159115076065,\\n        -0.001961700851097703,\\n        -0.00851533468812704,\\n        0.0005845429259352386,\\n        0.00053220841800794,\\n        0.007083732169121504,\\n        0.016625499352812767,\\n        -0.0165309589356184,\\n        -0.019299624487757683,\\n        0.02399960346519947,\\n        -0.01129075326025486,\\n        0.018003078177571297,\\n        -0.002427646890282631,\\n        -0.005922242999076843,\\n        0.02828090637922287,\\n        -0.007117496337741613,\\n        0.009575530886650085,\\n        -0.01006848830729723,\\n        -0.016557971015572548,\\n        -0.01585567556321621,\\n        0.011533855460584164,\\n        -0.014113441109657288,\\n        -0.020731227472424507,\\n        -0.04116532951593399,\\n        0.03522282838821411,\\n        0.0038761317264288664,\\n        -0.022568000480532646,\\n        -0.0033899270929396152,\\n        -0.0046999785117805,\\n        -0.022554494440555573,\\n        0.0003549462999217212,\\n        0.003869378939270973,\\n        0.019907381385564804,\\n        0.029361359775066376,\\n        0.003609394421800971,\\n        0.03090100921690464,\\n        -0.01488326583057642,\\n        -0.03430444002151489,\\n        -0.010608715936541557,\\n        -0.0013564772671088576,\\n        -0.007529419846832752,\\n        -0.014180969446897507,\\n        -0.020690711215138435\\n      ],\\n      \"index\": 0,\\n      \"object\": \"embedding\"\\n    }\\n  ],\\n  \"model\": \"ada\",\\n  \"object\": \"list\",\\n  \"usage\": {\\n    \"prompt_tokens\": 7,\\n    \"total_tokens\": 7\\n  }\\n}', retrieval.documents=None, retrieval.query=None, root_run_id='', span_type='LLM'), end_time='2024-02-05T15:00:17.619294Z', events=[], input='{\\n  \"input\": [\\n    [\\n      3923,\\n      374,\\n      279,\\n      7438,\\n      315,\\n      63120,\\n      30\\n    ]\\n  ],\\n  \"model\": \"text-embedding-ada-002\"\\n}', links=[], name='openai.resources.embeddings.Embeddings.create', output='{\\n  \"data\": [\\n    {\\n      \"embedding\": [\\n        -0.03181939572095871,\\n        0.0023466129787266254,\\n        0.03857223689556122,\\n        -0.014059418812394142,\\n        -0.005179430358111858,\\n        0.014072923921048641,\\n        -0.027767689898610115,\\n        -0.017179232090711594,\\n        -0.01000095997005701,\\n        -0.023594433441758156,\\n        0.0336291566491127,\\n        0.020055942237377167,\\n        -0.013937867246568203,\\n        0.006378060206770897,\\n        0.014329532161355019,\\n        -0.013451662845909595,\\n        0.043164171278476715,\\n        0.010541187599301338,\\n        -0.01359347254037857,\\n        -0.030036645010113716,\\n        -0.02895618975162506,\\n        -7.66025623306632e-05,\\n        0.0011108426842838526,\\n        0.02378351241350174,\\n        -0.007610453758388758,\\n        0.011182707734405994,\\n        0.011094920337200165,\\n        -0.024728910997509956,\\n        -0.006685314234346151,\\n        -0.038842350244522095,\\n        0.023418858647346497,\\n        0.004946457222104073,\\n        0.006573892664164305,\\n        -0.020380079746246338,\\n        -0.017003657296299934,\\n        -0.004686472937464714,\\n        0.004949833732098341,\\n        -0.028605042025446892,\\n        0.020326057448983192,\\n        -0.00028868403751403093,\\n        0.02526913769543171,\\n        0.024013109505176544,\\n        -0.0022486967500299215,\\n        0.00564537663012743,\\n        0.0006626226822845638,\\n        0.006668432150036097,\\n        0.008089905604720116,\\n        -0.028361938893795013,\\n        -0.010784289799630642,\\n        0.014653668738901615,\\n        0.028794120997190475,\\n        0.03271077200770378,\\n        -0.029847566038370132,\\n        -0.02616051211953163,\\n        0.0024951754603534937,\\n        -0.004916069563478231,\\n        -0.004412982612848282,\\n        0.0013429715763777494,\\n        0.013742035254836082,\\n        0.0015455569373443723,\\n        -0.0005807444686070085,\\n        -0.013120773248374462,\\n        -0.010547940619289875,\\n        0.01214161142706871,\\n        -0.0017489863093942404,\\n        0.01996140368282795,\\n        -0.00020986569870729,\\n        0.019515715539455414,\\n        -0.010784289799630642,\\n        -0.007610453758388758,\\n        0.020366573706269264,\\n        0.022595012560486794,\\n        0.025606779381632805,\\n        -0.01546400971710682,\\n        0.026065973564982414,\\n        -0.01551803294569254,\\n        -0.020434102043509483,\\n        -0.008501828648149967,\\n        0.010932852514088154,\\n        0.026808785274624825,\\n        0.005871596746146679,\\n        -0.0004243739531375468,\\n        -0.0043994770385324955,\\n        0.03319697454571724,\\n        -0.016760556027293205,\\n        0.0012771313777193427,\\n        0.007765769027173519,\\n        0.002853075973689556,\\n        -0.000697653042152524,\\n        -0.004453499801456928,\\n        -0.012803389690816402,\\n        -0.030036645010113716,\\n        0.02590390481054783,\\n        0.008792201057076454,\\n        0.0159637201577425,\\n        0.025539251044392586,\\n        -0.0046898494474589825,\\n        0.0365193746984005,\\n        -0.02611999586224556,\\n        -0.022203346714377403,\\n        0.0023753123823553324,\\n        0.01688210666179657,\\n        -0.032224565744400024,\\n        -0.0002006860449910164,\\n        0.007286317180842161,\\n        -0.0017059368547052145,\\n        -0.019178073853254318,\\n        -0.0076172067783772945,\\n        0.002799053443595767,\\n        -0.01794905588030815,\\n        -0.03100905381143093,\\n        0.02657919004559517,\\n        -0.008218209259212017,\\n        -0.04481186345219612,\\n        0.003481090534478426,\\n        0.015153379179537296,\\n        -0.004122610669583082,\\n        -0.0030792963225394487,\\n        -0.03446650877594948,\\n        -0.018786408007144928,\\n        0.0302257239818573,\\n        0.007644217927008867,\\n        0.018529800698161125,\\n        -0.0014062795089557767,\\n        -0.003997683059424162,\\n        0.018988993018865585,\\n        -0.008771942928433418,\\n        -0.01928611844778061,\\n        -0.016341879963874817,\\n        -0.013053244911134243,\\n        0.006364554166793823,\\n        0.030873997136950493,\\n        0.0016147735295817256,\\n        0.013087009079754353,\\n        -0.0053887683898210526,\\n        0.01751687377691269,\\n        0.004666214343160391,\\n        -0.0027146427892148495,\\n        -0.05175378546118736,\\n        -0.019137555733323097,\\n        0.0069689336232841015,\\n        0.024161672219634056,\\n        -0.0064523410983383656,\\n        -0.0075901951640844345,\\n        0.01862434111535549,\\n        0.030522849410772324,\\n        0.02611999586224556,\\n        0.012202386744320393,\\n        0.0072593060322105885,\\n        0.010271074250340462,\\n        0.03092801943421364,\\n        0.006749466527253389,\\n        0.01808411255478859,\\n        -0.002189609222114086,\\n        0.0005111058126203716,\\n        0.017935549840331078,\\n        -0.007144507486373186,\\n        0.0018975487910211086,\\n        0.011500091291964054,\\n        -0.025606779381632805,\\n        0.033467087894678116,\\n        -0.009649812243878841,\\n        0.010041477158665657,\\n        -0.026565684005618095,\\n        0.02686280943453312,\\n        0.045081976801157,\\n        0.0035519953817129135,\\n        0.01152034942060709,\\n        -0.005162548273801804,\\n        -0.002206491306424141,\\n        -0.016112282872200012,\\n        0.023702478036284447,\\n        -0.03138721361756325,\\n        0.017611414194107056,\\n        -0.013829821720719337,\\n        0.018165146932005882,\\n        0.027659643441438675,\\n        0.0012366143055260181,\\n        -0.016085270792245865,\\n        -0.00223856745287776,\\n        -0.016868600621819496,\\n        -0.0010838313028216362,\\n        0.01834072172641754,\\n        0.010892335325479507,\\n        -0.026957347989082336,\\n        -0.002802429720759392,\\n        0.010304838418960571,\\n        -0.006236250512301922,\\n        0.0056420001201331615,\\n        -0.01177020464092493,\\n        0.01546400971710682,\\n        0.03430444002151489,\\n        -0.004075340460985899,\\n        -0.008258726447820663,\\n        -0.6651279926300049,\\n        -0.003524984000250697,\\n        0.006502987816929817,\\n        -0.005297604948282242,\\n        -0.0007613830384798348,\\n        0.004645955748856068,\\n        0.01688210666179657,\\n        0.011000380851328373,\\n        -0.007536172401160002,\\n        0.028226882219314575,\\n        -0.01309376209974289,\\n        -0.007718499284237623,\\n        -0.009474238380789757,\\n        -0.0015143250348046422,\\n        -0.019191579893231392,\\n        -0.017395323142409325,\\n        -0.012384713627398014,\\n        -0.016801072284579277,\\n        -0.012816895730793476,\\n        0.008906999602913857,\\n        -0.03057687170803547,\\n        0.022676046937704086,\\n        -0.019340142607688904,\\n        0.015234413556754589,\\n        0.030522849410772324,\\n        -0.012384713627398014,\\n        -0.000990135595202446,\\n        -0.025282643735408783,\\n        -0.02229788713157177,\\n        0.022135818377137184,\\n        -0.0076982406899333,\\n        -0.01122322492301464,\\n        0.027051888406276703,\\n        0.0230812169611454,\\n        0.029469406232237816,\\n        -0.020474620163440704,\\n        -0.004902563989162445,\\n        0.02718694508075714,\\n        -0.04286704584956169,\\n        0.03379122540354729,\\n        0.008204704150557518,\\n        -0.0033173339907079935,\\n        0.021082375198602676,\\n        -0.021501051262021065,\\n        -0.0043522072955966,\\n        -0.016409408301115036,\\n        0.02474241517484188,\\n        -0.0087044145911932,\\n        0.0005136380787007511,\\n        -0.028578029945492744,\\n        -0.01245899498462677,\\n        0.0074753970839083195,\\n        -0.00895426981151104,\\n        -0.005091643426567316,\\n        0.006286896765232086,\\n        0.002908786991611123,\\n        0.01961025595664978,\\n        0.0027095782570540905,\\n        0.019205084070563316,\\n        0.0023263543844223022,\\n        -0.00943372119218111,\\n        0.045811284333467484,\\n        -0.00444337073713541,\\n        -0.013742035254836082,\\n        -0.02753809280693531,\\n        0.003401744645088911,\\n        0.016720037907361984,\\n        0.03092801943421364,\\n        0.01830020360648632,\\n        -0.034601565450429916,\\n        0.0030573494732379913,\\n        0.029226303100585938,\\n        -0.007549678441137075,\\n        0.0075901951640844345,\\n        0.016301361843943596,\\n        0.003043843898922205,\\n        -0.001223952742293477,\\n        0.013816316612064838,\\n        0.020163988694548607,\\n        0.015126368030905724,\\n        -0.015139873139560223,\\n        -0.008785448037087917,\\n        -0.0037782154977321625,\\n        -0.020150482654571533,\\n        0.02155507355928421,\\n        0.007711746264249086,\\n        -0.0025356924161314964,\\n        -0.006908158306032419,\\n        5.09364836034365e-05,\\n        -0.011405551806092262,\\n        -0.012040318921208382,\\n        -0.0013387510553002357,\\n        0.004966715816408396,\\n        -0.041894637048244476,\\n        0.013937867246568203,\\n        0.01801658421754837,\\n        -0.009001539088785648,\\n        0.008994787000119686,\\n        0.00886648241430521,\\n        0.023702478036284447,\\n        0.0038524968549609184,\\n        -0.010858571156859398,\\n        -0.02092030644416809,\\n        0.002834505867213011,\\n        0.005013985559344292,\\n        -0.008218209259212017,\\n        -0.018462272360920906,\\n        0.0330619178712368,\\n        0.04027395322918892,\\n        -0.008744931779801846,\\n        -0.0026420496869832277,\\n        -0.005675764288753271,\\n        0.008852977305650711,\\n        0.009251394309103489,\\n        -0.002478293376043439,\\n        -0.02605246752500534,\\n        0.02002893202006817,\\n        0.0142484987154603,\\n        0.008630133233964443,\\n        -0.008251973427832127,\\n        0.010541187599301338,\\n        -0.011621642857789993,\\n        0.01840825006365776,\\n        -0.024728910997509956,\\n        -0.010014466010034084,\\n        -0.012296927161514759,\\n        -0.0017979444237425923,\\n        -0.008981280960142612,\\n        -0.03387225791811943,\\n        0.009447227232158184,\\n        0.0176654364913702,\\n        -0.0009572154376655817,\\n        -0.004125986713916063,\\n        -0.01274261437356472,\\n        0.001882354961708188,\\n        0.00665492657572031,\\n        -0.019029511138796806,\\n        -0.03854522481560707,\\n        0.022635528817772865,\\n        -0.03932855650782585,\\n        -0.017935549840331078,\\n        0.019137555733323097,\\n        0.017854515463113785,\\n        -0.01794905588030815,\\n        -0.028226882219314575,\\n        -0.020960824564099312,\\n        -0.0015404922887682915,\\n        0.001764180138707161,\\n        -0.028902167454361916,\\n        -0.0065637631341814995,\\n        0.009636306203901768,\\n        -0.01233744341880083,\\n        -0.02087979018688202,\\n        0.013289595022797585,\\n        -0.0148157374933362,\\n        -0.011466327123343945,\\n        -0.016233833506703377,\\n        -0.014937288127839565,\\n        -0.023634949699044228,\\n        -0.03160330280661583,\\n        0.013141032308340073,\\n        0.015369470231235027,\\n        -0.021136397495865822,\\n        -0.022905642166733742,\\n        -0.024999024346470833,\\n        -0.01688210666179657,\\n        -0.012729108333587646,\\n        0.03967970237135887,\\n        -0.04575726389884949,\\n        -0.025863388553261757,\\n        0.023837534710764885,\\n        -0.012317185290157795,\\n        0.003997683059424162,\\n        0.010210298001766205,\\n        -0.0060843112878501415,\\n        0.025863388553261757,\\n        -0.017138715833425522,\\n        -0.006320660933852196,\\n        0.011993048712611198,\\n        -0.006492858286947012,\\n        0.015653088688850403,\\n        0.0036060181446373463,\\n        -0.02169013023376465,\\n        0.0016586669953539968,\\n        0.009291911497712135,\\n        0.011446068063378334,\\n        0.04491991177201271,\\n        0.013316606171429157,\\n        -0.0022942782379686832,\\n        0.009913173504173756,\\n        -0.004818153101950884,\\n        0.0071850246749818325,\\n        -0.018070606514811516,\\n        0.01003472413867712,\\n        -0.0009344246354885399,\\n        0.0153964813798666,\\n        0.015653088688850403,\\n        0.0055001904256641865,\\n        0.0030944901518523693,\\n        0.03743775933980942,\\n        0.029037224128842354,\\n        0.00429143151268363,\\n        0.01928611844778061,\\n        -0.010500670410692692,\\n        0.009609295055270195,\\n        0.017017163336277008,\\n        -0.0009310481837019324,\\n        -0.025890398770570755,\\n        0.008191198110580444,\\n        -0.0011327894171699882,\\n        0.005797315388917923,\\n        -0.027740677818655968,\\n        -0.009453980252146721,\\n        -0.010730267502367496,\\n        0.015693606808781624,\\n        0.02286512590944767,\\n        -0.0020815636962652206,\\n        0.004818153101950884,\\n        -0.034709613770246506,\\n        0.017705954611301422,\\n        0.0213795006275177,\\n        -0.0056521291844546795,\\n        -0.011412303894758224,\\n        -0.0026369851548224688,\\n        0.005837832577526569,\\n        0.003325775032863021,\\n        0.034034326672554016,\\n        0.04654059186577797,\\n        -0.009258147329092026,\\n        -0.002203115029260516,\\n        -0.018880948424339294,\\n        0.024161672219634056,\\n        0.014950794167816639,\\n        0.013755540363490582,\\n        -0.02013697661459446,\\n        -0.0006900561274960637,\\n        0.01609877683222294,\\n        -0.0067528425715863705,\\n        0.00794809591025114,\\n        -0.025228621438145638,\\n        0.007610453758388758,\\n        0.01670653373003006,\\n        -0.00128557241987437,\\n        -0.014113441109657288,\\n        0.016828084364533424,\\n        0.002250384772196412,\\n        0.03805902227759361,\\n        0.01677406206727028,\\n        0.009960442781448364,\\n        -0.003947036806493998,\\n        -0.0043994770385324955,\\n        -0.008711167611181736,\\n        -0.002922292798757553,\\n        -0.0003201269428245723,\\n        -0.01635538600385189,\\n        0.0037241927348077297,\\n        -0.0025930916890501976,\\n        -0.013458415865898132,\\n        0.011689171195030212,\\n        0.03784292936325073,\\n        0.01943468116223812,\\n        0.022676046937704086,\\n        0.02884814515709877,\\n        -0.0017658683937042952,\\n        -0.006334166508167982,\\n        0.014464588835835457,\\n        0.011736440472304821,\\n        -0.004622321110218763,\\n        -0.004787765443325043,\\n        0.002299343002960086,\\n        0.008535593748092651,\\n        -0.014640163630247116,\\n        0.0021372747141867876,\\n        -0.014140453189611435,\\n        0.01840825006365776,\\n        0.009291911497712135,\\n        -0.019772322848439217,\\n        -0.0008521243580617011,\\n        0.019704794511198997,\\n        0.0174088291823864,\\n        -0.006877770181745291,\\n        -0.027011370286345482,\\n        -0.006641421001404524,\\n        0.0076982406899333,\\n        -0.0059053609147667885,\\n        -0.025998445227742195,\\n        0.013505685143172741,\\n        0.005375262815505266,\\n        -0.016476936638355255,\\n        0.0035823830403387547,\\n        -0.009899667464196682,\\n        -0.009339181706309319,\\n        -0.006320660933852196,\\n        -0.013228818774223328,\\n        0.0018975487910211086,\\n        -0.012101094238460064,\\n        0.011304258368909359,\\n        0.013742035254836082,\\n        0.0142620038241148,\\n        -0.03011767938733101,\\n        -0.002380377147346735,\\n        -0.0060539236292243,\\n        -0.008137175813317299,\\n        -0.008879988454282284,\\n        -0.0003192828444298357,\\n        0.010568198747932911,\\n        -0.0006221056682989001,\\n        -0.012884424068033695,\\n        -0.020717721432447433,\\n        -0.0225409884005785,\\n        0.017476357519626617,\\n        0.007954848930239677,\\n        -0.0030235853046178818,\\n        -0.032791804522275925,\\n        0.005861467681825161,\\n        0.016652509570121765,\\n        -0.0033105812035501003,\\n        0.018961982801556587,\\n        0.03357513248920441,\\n        0.012506265193223953,\\n        -0.002544133458286524,\\n        -0.04046303406357765,\\n        0.02020450495183468,\\n        -0.004372465889900923,\\n        0.0856260433793068,\\n        0.002562703797593713,\\n        -0.02279759757220745,\\n        0.020650193095207214,\\n        -0.01189175620675087,\\n        0.019340142607688904,\\n        -0.01734130084514618,\\n        -0.010777536779642105,\\n        0.02452632412314415,\\n        -0.002885152120143175,\\n        -0.025917410850524902,\\n        -0.013492180034518242,\\n        0.00851533468812704,\\n        -0.004899187479168177,\\n        0.025390688329935074,\\n        0.017827505245804787,\\n        -0.0032312353141605854,\\n        0.001186812063679099,\\n        0.010163028724491596,\\n        -0.0230812169611454,\\n        -0.0020072825718671083,\\n        -0.009393204003572464,\\n        0.016017742455005646,\\n        -0.0025998444762080908,\\n        -0.007144507486373186,\\n        -0.013451662845909595,\\n        -0.0039943065494298935,\\n        0.00806964747607708,\\n        0.022770585492253304,\\n        -0.00948774442076683,\\n        0.01075727865099907,\\n        0.0030759198125451803,\\n        -0.024013109505176544,\\n        -0.0038457440678030252,\\n        -0.015599066391587257,\\n        0.011101673357188702,\\n        0.011621642857789993,\\n        -0.012445488944649696,\\n        0.012870918028056622,\\n        -0.021365994587540627,\\n        0.015153379179537296,\\n        0.03206249698996544,\\n        0.013478673994541168,\\n        -0.016720037907361984,\\n        -0.0028193118050694466,\\n        -0.013296347111463547,\\n        0.001635876134969294,\\n        0.005398897919803858,\\n        -0.009703835472464561,\\n        -0.0028125590179115534,\\n        0.011297506280243397,\\n        -0.005935749039053917,\\n        -0.030603883787989616,\\n        0.009629554115235806,\\n        0.014802231453359127,\\n        0.013647494837641716,\\n        -0.01339764054864645,\\n        -0.002650490729138255,\\n        -0.0028935931622982025,\\n        -0.022743575274944305,\\n        -0.03376421332359314,\\n        -0.009656565263867378,\\n        -0.007164766080677509,\\n        -0.0011716182343661785,\\n        0.0035925123374909163,\\n        -0.03389926999807358,\\n        -0.006462470628321171,\\n        -0.018921464681625366,\\n        -0.005827703513205051,\\n        0.009595789946615696,\\n        0.025390688329935074,\\n        -0.02452632412314415,\\n        -0.04448772966861725,\\n        -0.004747248254716396,\\n        0.026822291314601898,\\n        -0.0008090749615803361,\\n        -0.005034244153648615,\\n        0.0011783710215240717,\\n        -0.024688392877578735,\\n        0.011358281597495079,\\n        -0.009778115898370743,\\n        -0.02566080167889595,\\n        -0.018597329035401344,\\n        -0.009055562317371368,\\n        -0.011648654006421566,\\n        0.005040997173637152,\\n        -0.0031890301033854485,\\n        0.015761135146021843,\\n        -0.0031569539569318295,\\n        0.011594630777835846,\\n        -0.005294228903949261,\\n        0.01893497072160244,\\n        0.04562220722436905,\\n        -0.03614121302962303,\\n        0.007792780641466379,\\n        0.017462851479649544,\\n        -0.003997683059424162,\\n        0.0067798541858792305,\\n        0.009987454861402512,\\n        -0.0225139781832695,\\n        -0.0092716533690691,\\n        -0.026768269017338753,\\n        -0.016436418518424034,\\n        0.012587298639118671,\\n        0.011398798786103725,\\n        0.016328373923897743,\\n        0.00027391218463890254,\\n        0.029280327260494232,\\n        0.018097618594765663,\\n        -0.020933812484145164,\\n        -0.006695443764328957,\\n        -0.04184061288833618,\\n        -0.009048809297382832,\\n        0.004129363223910332,\\n        0.003528360277414322,\\n        0.002262202324345708,\\n        0.0159637201577425,\\n        0.018880948424339294,\\n        -0.0005047749727964401,\\n        -0.005037620663642883,\\n        -0.017084691673517227,\\n        -0.03808603435754776,\\n        0.032089509069919586,\\n        0.022527484223246574,\\n        -0.003980800975114107,\\n        -0.022784091532230377,\\n        0.004348830785602331,\\n        -0.005885102320462465,\\n        -0.011142190545797348,\\n        0.015504526905715466,\\n        -0.013809563592076302,\\n        0.004048329312354326,\\n        -0.02067720517516136,\\n        -0.04297509044408798,\\n        -0.020272033289074898,\\n        -0.000584120920393616,\\n        -0.022149324417114258,\\n        0.020906800404191017,\\n        -0.017125209793448448,\\n        0.00536175724118948,\\n        -0.0036296530161052942,\\n        -0.019907381385564804,\\n        -0.011331270448863506,\\n        -0.01546400971710682,\\n        0.04640553519129753,\\n        -0.030468827113509178,\\n        -0.027848724275827408,\\n        0.016720037907361984,\\n        -0.023000182583928108,\\n        0.028875155374407768,\\n        -0.014586140401661396,\\n        -0.002562703797593713,\\n        -0.03692454472184181,\\n        0.003670169971883297,\\n        0.0022588258143514395,\\n        -0.015545044094324112,\\n        -0.019421175122261047,\\n        -0.022216852754354477,\\n        0.03670845180749893,\\n        0.010169780813157558,\\n        0.012202386744320393,\\n        -0.004801271017640829,\\n        0.003869378939270973,\\n        0.011155696585774422,\\n        -0.006401694845408201,\\n        -0.0010661050910130143,\\n        -0.004784388933330774,\\n        -0.014005395583808422,\\n        -0.0001811661059036851,\\n        0.0048687998205423355,\\n        0.026038961485028267,\\n        -0.016368890181183815,\\n        0.02356742136180401,\\n        0.017989574000239372,\\n        0.025228621438145638,\\n        0.02162260189652443,\\n        0.0005600639269687235,\\n        -0.009946937672793865,\\n        -0.02618752419948578,\\n        -0.013897350057959557,\\n        -0.009690329432487488,\\n        0.009845645166933537,\\n        0.003646535100415349,\\n        -0.003062414238229394,\\n        -0.024999024346470833,\\n        0.001968453638255596,\\n        0.0153964813798666,\\n        0.006638044491410255,\\n        -0.006604280322790146,\\n        0.02349989302456379,\\n        0.011885003186762333,\\n        -0.018070606514811516,\\n        -0.0006478508585132658,\\n        0.007353845983743668,\\n        0.045325081795454025,\\n        -0.0020883167162537575,\\n        -0.011756699532270432,\\n        0.004082093480974436,\\n        0.002177791902795434,\\n        0.01245899498462677,\\n        0.028226882219314575,\\n        -0.0009521508472971618,\\n        -0.015153379179537296,\\n        -0.0033291515428572893,\\n        -0.024202188476920128,\\n        0.012384713627398014,\\n        -0.0008930634357966483,\\n        -0.012891177088022232,\\n        0.002405700273811817,\\n        -0.004710108041763306,\\n        -0.025228621438145638,\\n        -0.008224962279200554,\\n        -0.02502603456377983,\\n        -0.009413463063538074,\\n        0.011776957660913467,\\n        0.0009504626505076885,\\n        -0.005760174710303545,\\n        0.007684735115617514,\\n        -0.015707112848758698,\\n        -0.017395323142409325,\\n        0.012627815827727318,\\n        -0.003528360277414322,\\n        0.03443949669599533,\\n        -0.002047799527645111,\\n        0.02035306766629219,\\n        -0.002650490729138255,\\n        0.023243285715579987,\\n        -0.030873997136950493,\\n        0.009670071303844452,\\n        0.0006254820618778467,\\n        -0.0008719608304090798,\\n        0.01688210666179657,\\n        -0.010588457807898521,\\n        0.0068541355431079865,\\n        -0.01546400971710682,\\n        0.011331270448863506,\\n        0.02056915871798992,\\n        -0.007272811606526375,\\n        -0.023905063048005104,\\n        0.008170939981937408,\\n        0.012114600278437138,\\n        -0.006340919528156519,\\n        -0.009426968172192574,\\n        -0.019299624487757683,\\n        0.005405650474131107,\\n        0.03965269401669502,\\n        0.001683990121819079,\\n        0.007090484723448753,\\n        -0.005631871055811644,\\n        -0.01819215901196003,\\n        -0.014856253750622272,\\n        0.009838892146945,\\n        -0.013208560645580292,\\n        0.014559129253029823,\\n        0.020663699135184288,\\n        -0.017152220010757446,\\n        -0.009980701841413975,\\n        0.023905063048005104,\\n        0.00895426981151104,\\n        0.005885102320462465,\\n        -0.010244062170386314,\\n        -0.005341498646885157,\\n        -0.04086820408701897,\\n        0.01572061888873577,\\n        -0.01038587186485529,\\n        -0.011425809934735298,\\n        0.013525944203138351,\\n        0.005638623610138893,\\n        0.00803588330745697,\\n        0.028469985350966454,\\n        -0.028794120997190475,\\n        -0.03687052056193352,\\n        0.011939026415348053,\\n        -0.006982439663261175,\\n        0.022838113829493523,\\n        0.01038587186485529,\\n        0.009690329432487488,\\n        -0.0046256971545517445,\\n        -0.009028551168739796,\\n        -0.042380839586257935,\\n        0.010392624884843826,\\n        0.010554693639278412,\\n        -0.010615468956530094,\\n        -0.011067909188568592,\\n        0.0087044145911932,\\n        0.011338023468852043,\\n        -0.023905063048005104,\\n        0.007022956386208534,\\n        0.0050680083222687244,\\n        0.010264321230351925,\\n        -0.03927453234791756,\\n        0.004186762496829033,\\n        -0.0002825642586685717,\\n        0.029928598552942276,\\n        0.0072120362892746925,\\n        0.006479352712631226,\\n        0.002410764805972576,\\n        0.014437577687203884,\\n        -0.029577450826764107,\\n        0.001482249004766345,\\n        -0.0041833859868347645,\\n        0.011682418175041676,\\n        0.008414042182266712,\\n        0.016544464975595474,\\n        -0.0023212896194308996,\\n        -0.03673546388745308,\\n        0.02216283045709133,\\n        -0.013519191183149815,\\n        -0.020380079746246338,\\n        -0.013586719520390034,\\n        0.02223035879433155,\\n        0.008204704150557518,\\n        0.011810721829533577,\\n        -0.005979642271995544,\\n        -0.02183869294822216,\\n        0.009426968172192574,\\n        0.009832139126956463,\\n        0.002562703797593713,\\n        -0.0005908737657591701,\\n        -0.007056720554828644,\\n        -0.013843327760696411,\\n        -0.00681361835449934,\\n        0.03862626105546951,\\n        -0.024026615545153618,\\n        0.0035992651246488094,\\n        -0.0015751005848869681,\\n        -0.005898608360439539,\\n        -0.0014273821143433452,\\n        -0.0307929627597332,\\n        -0.003504725405946374,\\n        -0.03230559825897217,\\n        0.017638426274061203,\\n        -0.0020950695034116507,\\n        -0.010649233125150204,\\n        0.03000963293015957,\\n        -0.00400781212374568,\\n        -0.006232874002307653,\\n        0.027659643441438675,\\n        0.019556231796741486,\\n        -0.01035210769623518,\\n        -0.017287276685237885,\\n        0.013586719520390034,\\n        0.01563958451151848,\\n        0.01992088556289673,\\n        0.007022956386208534,\\n        0.014802231453359127,\\n        0.011473080143332481,\\n        -0.01093960553407669,\\n        -0.011000380851328373,\\n        -0.03635730594396591,\\n        -0.016733543947339058,\\n        0.012904682196676731,\\n        0.025188103318214417,\\n        -0.00637130718678236,\\n        -0.02505304664373398,\\n        -0.006648173555731773,\\n        -0.012303679250180721,\\n        -0.008366771973669529,\\n        -0.012621062807738781,\\n        -0.0029476159252226353,\\n        0.01132451742887497,\\n        0.011405551806092262,\\n        -0.003866002429276705,\\n        0.0021541567984968424,\\n        -0.01447809487581253,\\n        -0.011560866609215736,\\n        -0.01476171426475048,\\n        0.010196792893111706,\\n        -0.01830020360648632,\\n        -0.013600225560367107,\\n        0.014788725413382053,\\n        -0.00618560379371047,\\n        -0.03009066730737686,\\n        -0.021393006667494774,\\n        -0.008839471265673637,\\n        -0.023418858647346497,\\n        -0.02439126744866371,\\n        0.006921663880348206,\\n        -0.004537910223007202,\\n        -0.003069167025387287,\\n        -0.012431983835995197,\\n        0.03497972711920738,\\n        0.00257114483974874,\\n        0.02343236468732357,\\n        0.007340339943766594,\\n        -0.0170981977134943,\\n        -0.006921663880348206,\\n        0.0005136380787007511,\\n        -0.007853556424379349,\\n        -0.020758239552378654,\\n        0.0472969114780426,\\n        0.01646343059837818,\\n        -0.01834072172641754,\\n        0.024796439334750175,\\n        -0.020690711215138435,\\n        -0.0104061309248209,\\n        0.020839272066950798,\\n        0.007205283269286156,\\n        -0.0015995795838534832,\\n        0.028307916596531868,\\n        -0.007954848930239677,\\n        -0.0005482464330270886,\\n        0.0008061206317506731,\\n        -0.007813039235770702,\\n        -0.027159933000802994,\\n        -0.002035982208326459,\\n        -0.0029357983730733395,\\n        -0.011939026415348053,\\n        0.009859150275588036,\\n        -0.012951952405273914,\\n        -0.00621261540800333,\\n        0.012641321867704391,\\n        -0.015247918665409088,\\n        -0.013883844949305058,\\n        0.01286416593939066,\\n        -0.004331948701292276,\\n        0.004152998328208923,\\n        -0.0404900461435318,\\n        0.006243003066629171,\\n        -0.0006841474096290767,\\n        -0.02895618975162506,\\n        0.014991311356425285,\\n        -0.01886744238436222,\\n        -0.013688012026250362,\\n        -0.00980512797832489,\\n        -0.0017110015032812953,\\n        -0.0030218970496207476,\\n        -0.019907381385564804,\\n        -0.003005014965310693,\\n        0.030765952542424202,\\n        0.007914331741631031,\\n        0.002878399332985282,\\n        -0.008562604896724224,\\n        0.017017163336277008,\\n        -0.011743193492293358,\\n        0.003190718125551939,\\n        0.22646333277225494,\\n        -0.011216471903026104,\\n        0.006151839625090361,\\n        0.009001539088785648,\\n        -0.010899088345468044,\\n        0.005429285578429699,\\n        0.015328953042626381,\\n        -0.00955527275800705,\\n        0.00781979225575924,\\n        0.020218010991811752,\\n        0.007542925421148539,\\n        -0.0182326752692461,\\n        -0.03217054158449173,\\n        0.01242523081600666,\\n        0.0015075721312314272,\\n        0.006320660933852196,\\n        -0.029334349557757378,\\n        -0.03281881660223007,\\n        -0.002746718702837825,\\n        -0.035763055086135864,\\n        0.013141032308340073,\\n        -0.003474337514489889,\\n        -0.02059617079794407,\\n        -0.02109588123857975,\\n        0.04300210252404213,\\n        0.009224383160471916,\\n        0.004237408749759197,\\n        -0.005550836678594351,\\n        0.023378342390060425,\\n        0.003764709923416376,\\n        -0.02020450495183468,\\n        0.005577848292887211,\\n        -0.0034338205587118864,\\n        0.004568298347294331,\\n        -0.0202450230717659,\\n        0.006631291471421719,\\n        -0.007988613098859787,\\n        -0.0082654794678092,\\n        0.021109387278556824,\\n        0.007596948184072971,\\n        0.0003009235661011189,\\n        0.013249077834188938,\\n        -0.016220327466726303,\\n        0.008643638342618942,\\n        0.013019480742514133,\\n        0.018178652971982956,\\n        -0.0002962809812743217,\\n        -0.03114411048591137,\\n        -0.0038086033891886473,\\n        0.025525745004415512,\\n        -0.019529221579432487,\\n        0.00334434537217021,\\n        0.002144027501344681,\\n        0.037815921008586884,\\n        -0.012884424068033695,\\n        0.01805710233747959,\\n        -0.00022326585894916207,\\n        0.0004566609859466553,\\n        -0.0013632301706820726,\\n        0.031063076108694077,\\n        0.006367930676788092,\\n        0.00592899601906538,\\n        -0.006914910860359669,\\n        0.04845840111374855,\\n        0.008292490616440773,\\n        0.03408835083246231,\\n        -0.016260845586657524,\\n        0.015409987419843674,\\n        0.008292490616440773,\\n        -0.03581707924604416,\\n        -0.003082672832533717,\\n        -0.026822291314601898,\\n        -0.0005174365942366421,\\n        -0.01453211810439825,\\n        -0.018070606514811516,\\n        -0.007907578721642494,\\n        0.013127526268362999,\\n        0.008177693001925945,\\n        0.02048812434077263,\\n        0.016193317249417305,\\n        -0.022676046937704086,\\n        -0.03203548491001129,\\n        -0.012479253113269806,\\n        -0.005344875156879425,\\n        0.002648802474141121,\\n        -0.023418858647346497,\\n        -0.0076644765213131905,\\n        -0.0028328176122158766,\\n        -0.006681937724351883,\\n        -0.008690908551216125,\\n        0.025755342096090317,\\n        -0.024688392877578735,\\n        -0.023972591385245323,\\n        -0.010095500387251377,\\n        0.008137175813317299,\\n        -0.016557971015572548,\\n        0.017044175416231155,\\n        0.014869759790599346,\\n        0.0068541355431079865,\\n        0.013012727722525597,\\n        0.00861662719398737,\\n        0.02441827952861786,\\n        0.012344196438789368,\\n        0.011351528577506542,\\n        -0.01985335722565651,\\n        -0.00024162515182979405,\\n        0.0002707467938307673,\\n        0.023878052830696106,\\n        0.0026910079177469015,\\n        -0.01692262478172779,\\n        0.005000479985028505,\\n        -0.03225157782435417,\\n        0.0023938827216625214,\\n        -0.01985335722565651,\\n        0.010264321230351925,\\n        -0.021176915615797043,\\n        -0.022149324417114258,\\n        0.0007200218387879431,\\n        0.01751687377691269,\\n        -0.02356742136180401,\\n        -0.004882305394858122,\\n        -0.025255631655454636,\\n        0.008360018953680992,\\n        0.02039358578622341,\\n        0.004943080712109804,\\n        -0.024999024346470833,\\n        -0.006050547119230032,\\n        -0.007299823220819235,\\n        0.013168043456971645,\\n        -0.004642579238861799,\\n        0.01801658421754837,\\n        -0.03606018051505089,\\n        0.011966037563979626,\\n        -0.000476497458294034,\\n        -0.0037376985419541597,\\n        -0.010682997293770313,\\n        0.018138134852051735,\\n        -0.011034145019948483,\\n        -0.00955527275800705,\\n        0.01060196291655302,\\n        -0.0024985517375171185,\\n        -0.019772322848439217,\\n        0.006800112780183554,\\n        0.007644217927008867,\\n        0.0006339231040328741,\\n        -0.016841590404510498,\\n        0.009994206950068474,\\n        0.001396150211803615,\\n        -0.01748986355960369,\\n        -0.007016203831881285,\\n        -0.013174796476960182,\\n        -0.005797315388917923,\\n        -0.000915854296181351,\\n        -0.012195633724331856,\\n        0.0032329235691577196,\\n        0.002496863715350628,\\n        -0.0006795048248022795,\\n        -0.018097618594765663,\\n        0.02092030644416809,\\n        -0.0033156457357108593,\\n        -0.026038961485028267,\\n        -0.014491600915789604,\\n        0.010865324176847935,\\n        0.00835326686501503,\\n        -0.013735282234847546,\\n        0.015072344802320004,\\n        -0.17384518682956696,\\n        0.004551416262984276,\\n        0.019529221579432487,\\n        -0.01777348294854164,\\n        0.01758440211415291,\\n        -0.021298466250300407,\\n        -0.003585759550333023,\\n        -0.0009783180430531502,\\n        -0.02159559167921543,\\n        0.02354040928184986,\\n        0.04232681915163994,\\n        -0.03114411048591137,\\n        -0.021987255662679672,\\n        -0.024593854323029518,\\n        -0.0013269336195662618,\\n        0.004180009476840496,\\n        0.007873814553022385,\\n        -0.008751683868467808,\\n        0.014896770939230919,\\n        0.02240593172609806,\\n        0.020380079746246338,\\n        -0.002083251951262355,\\n        0.005000479985028505,\\n        0.0038457440678030252,\\n        -0.0040044356137514114,\\n        -0.002125457162037492,\\n        -0.016841590404510498,\\n        0.037572816014289856,\\n        0.007718499284237623,\\n        -0.009636306203901768,\\n        0.013525944203138351,\\n        0.0022588258143514395,\\n        0.013262582942843437,\\n        0.016341879963874817,\\n        -0.004564921837300062,\\n        0.010325096547603607,\\n        -0.007650970946997404,\\n        -0.029010212048888206,\\n        -0.021298466250300407,\\n        0.015950214117765427,\\n        0.02655217796564102,\\n        0.017287276685237885,\\n        0.011020638979971409,\\n        -0.00978486891835928,\\n        -0.01939416490495205,\\n        -0.00444337073713541,\\n        0.021433522924780846,\\n        -0.004791141953319311,\\n        0.022878631949424744,\\n        -0.04427163675427437,\\n        0.01646343059837818,\\n        0.010581704787909985,\\n        -0.002368559595197439,\\n        -0.018462272360920906,\\n        0.0182326752692461,\\n        0.003511478193104267,\\n        0.00946748536080122,\\n        0.023594433441758156,\\n        -0.014275509864091873,\\n        0.0014383555389940739,\\n        -0.0005305202212184668,\\n        -0.01511286199092865,\\n        0.029469406232237816,\\n        -0.01343140471726656,\\n        0.017395323142409325,\\n        -0.003045532153919339,\\n        0.01447809487581253,\\n        0.00055964186321944,\\n        -0.012330691330134869,\\n        0.006192356813699007,\\n        -0.017476357519626617,\\n        -0.006533375475555658,\\n        -0.009575530886650085,\\n        -0.026228042319417,\\n        -0.018840432167053223,\\n        0.007954848930239677,\\n        -0.027267979457974434,\\n        -0.003572253743186593,\\n        -0.020218010991811752,\\n        0.009231136180460453,\\n        -0.004078716970980167,\\n        0.03795097768306732,\\n        -0.01624733954668045,\\n        0.009143348783254623,\\n        -0.010608715936541557,\\n        -0.0031029311940073967,\\n        0.009028551168739796,\\n        0.01620682328939438,\\n        0.009845645166933537,\\n        -0.02749757654964924,\\n        0.023986097425222397,\\n        -0.03011767938733101,\\n        -0.005040997173637152,\\n        -0.01504533365368843,\\n        0.030522849410772324,\\n        0.01447809487581253,\\n        0.015301941893994808,\\n        0.005587977357208729,\\n        -0.0028615170158445835,\\n        -0.0005701931659132242,\\n        -0.0092716533690691,\\n        -0.01659848727285862,\\n        0.002939174883067608,\\n        -0.008981280960142612,\\n        -0.013485427014529705,\\n        0.012479253113269806,\\n        0.023918569087982178,\\n        -0.006209238898009062,\\n        0.04005786404013634,\\n        -0.0005959383561275899,\\n        -0.02046111412346363,\\n        0.0011024016421288252,\\n        0.022432943806052208,\\n        0.01635538600385189,\\n        0.0024732286110520363,\\n        0.02806481532752514,\\n        0.011081415228545666,\\n        -0.00823171529918909,\\n        0.010959863662719727,\\n        0.01545050460845232,\\n        0.07476747781038284,\\n        0.014086429961025715,\\n        -0.009933431632816792,\\n        0.0028885283973068,\\n        -0.007461891509592533,\\n        -0.03795097768306732,\\n        -0.09972598403692245,\\n        -0.021393006667494774,\\n        0.013039739802479744,\\n        0.006621162407100201,\\n        0.015572055242955685,\\n        -0.001142918597906828,\\n        0.013586719520390034,\\n        -0.010196792893111706,\\n        0.013417898677289486,\\n        0.021136397495865822,\\n        -0.018219169229269028,\\n        -0.017638426274061203,\\n        -0.015288435854017735,\\n        -0.03217054158449173,\\n        -0.015436998568475246,\\n        -0.007542925421148539,\\n        -0.003953789360821247,\\n        -0.019475199282169342,\\n        -0.0048789288848638535,\\n        0.009190618991851807,\\n        -0.004926198627799749,\\n        -0.012634568847715855,\\n        -0.01613929495215416,\\n        -0.011405551806092262,\\n        -0.01459964644163847,\\n        -0.02166312001645565,\\n        -0.014005395583808422,\\n        0.008873235434293747,\\n        0.008724672719836235,\\n        0.006455717608332634,\\n        -0.0165309589356184,\\n        -0.0279297586530447,\\n        0.011148943565785885,\\n        0.0030404673889279366,\\n        0.016260845586657524,\\n        0.01666601561009884,\\n        -0.03954464569687843,\\n        -0.0018435260280966759,\\n        0.00558460084721446,\\n        -0.010163028724491596,\\n        0.0061585926450788975,\\n        0.021987255662679672,\\n        0.009008292108774185,\\n        -0.009177112951874733,\\n        0.030063655227422714,\\n        -0.01819215901196003,\\n        -0.019664278253912926,\\n        0.013492180034518242,\\n        0.005696022883057594,\\n        0.01762492023408413,\\n        -0.011013886891305447,\\n        -0.002336483681574464,\\n        -0.024958506226539612,\\n        -0.01180396880954504,\\n        0.015193896368145943,\\n        -0.018772903829813004,\\n        -0.004163127392530441,\\n        -0.017192738130688667,\\n        -0.008042635396122932,\\n        -0.002027540933340788,\\n        0.00580406840890646,\\n        -0.002078187419101596,\\n        -0.01847577840089798,\\n        0.03344007581472397,\\n        0.03676247596740723,\\n        0.0013092074077576399,\\n        0.0012180439662188292,\\n        -0.008792201057076454,\\n        0.02325678989291191,\\n        -0.005628494545817375,\\n        -0.023580927401781082,\\n        0.02467488683760166,\\n        0.006226120982319117,\\n        -0.010912594385445118,\\n        -0.023770006373524666,\\n        0.0002443685079924762,\\n        -0.015990732237696648,\\n        0.015085850842297077,\\n        -0.01897548884153366,\\n        -0.011020638979971409,\\n        -0.036789488047361374,\\n        -0.009933431632816792,\\n        -0.007826544344425201,\\n        -0.015315447002649307,\\n        -0.0008293334976769984,\\n        -0.012783131562173367,\\n        -0.03271077200770378,\\n        -0.042029693722724915,\\n        0.012330691330134869,\\n        -0.02930733747780323,\\n        -0.012661579996347427,\\n        0.026038961485028267,\\n        0.03606018051505089,\\n        0.013249077834188938,\\n        -0.010750525631010532,\\n        0.015004816465079784,\\n        -0.0024749168660491705,\\n        -0.018664857372641563,\\n        2.4663702788529918e-05,\\n        0.010392624884843826,\\n        0.014275509864091873,\\n        -0.003653287887573242,\\n        -0.03522282838821411,\\n        -0.005581224337220192,\\n        0.006546881049871445,\\n        -0.003381486050784588,\\n        -0.014221486635506153,\\n        -0.013066750951111317,\\n        0.006891276221722364,\\n        -0.005915490444749594,\\n        -0.015693606808781624,\\n        -0.0026791903655976057,\\n        -0.007239047437906265,\\n        0.006837253458797932,\\n        0.008947516791522503,\\n        0.009345934726297855,\\n        -0.014207981526851654,\\n        -0.013444909825921059,\\n        0.025809364393353462,\\n        0.0012138234451413155,\\n        -0.003781592007726431,\\n        0.007468644063919783,\\n        -0.0076644765213131905,\\n        -0.0014408878050744534,\\n        0.014748208224773407,\\n        0.0005887635052204132,\\n        -0.0048316591419279575,\\n        -0.01208758819848299,\\n        0.008684155531227589,\\n        -0.00230103125795722,\\n        -0.028469985350966454,\\n        -0.011594630777835846,\\n        0.019556231796741486,\\n        -0.00863688625395298,\\n        0.004618944600224495,\\n        0.0279567688703537,\\n        -0.002799053443595767,\\n        -0.012296927161514759,\\n        -0.0012433672090992332,\\n        0.019839851185679436,\\n        0.011783710680902004,\\n        -0.005625118035823107,\\n        -0.015261424705386162,\\n        -0.028713086619973183,\\n        0.004186762496829033,\\n        -0.02644413150846958,\\n        0.013444909825921059,\\n        -0.0031198132783174515,\\n        -0.025188103318214417,\\n        -0.00018749690207187086,\\n        0.02718694508075714,\\n        -0.013870338909327984,\\n        0.0148292426019907,\\n        0.014842748641967773,\\n        0.0004252180806361139,\\n        0.006050547119230032,\\n        -0.008738178759813309,\\n        -0.02884814515709877,\\n        0.007293070200830698,\\n        0.008170939981937408,\\n        0.008812460117042065,\\n        -0.016017742455005646,\\n        0.03298088535666466,\\n        0.012634568847715855,\\n        -0.00032899004872888327,\\n        -0.0006440524011850357,\\n        0.03352111205458641,\\n        0.0020680581219494343,\\n        -0.013262582942843437,\\n        0.005554213188588619,\\n        -0.002520498586818576,\\n        -0.03238663449883461,\\n        -0.0017321042250841856,\\n        -0.011216471903026104,\\n        0.009562025777995586,\\n        -0.0026049090083688498,\\n        0.04267796501517296,\\n        0.0037174399476498365,\\n        0.02039358578622341,\\n        0.004679719917476177,\\n        -0.028767110779881477,\\n        0.026781775057315826,\\n        0.009190618991851807,\\n        -0.020717721432447433,\\n        -0.01563958451151848,\\n        0.009285158477723598,\\n        0.04705381020903587,\\n        0.002957745222374797,\\n        -0.018840432167053223,\\n        0.017678942531347275,\\n        -0.004409606568515301,\\n        0.00017958341049961746,\\n        0.008785448037087917,\\n        0.008684155531227589,\\n        0.011939026415348053,\\n        -0.007495655678212643,\\n        0.008758436888456345,\\n        0.013357123360037804,\\n        -0.0028682700358331203,\\n        0.013600225560367107,\\n        0.01812463067471981,\\n        0.01072351448237896,\\n        0.012310432270169258,\\n        0.0028699582908302546,\\n        -0.01950220949947834,\\n        -0.0014839372597634792,\\n        -0.01286416593939066,\\n        -0.0007060941425152123,\\n        -0.008360018953680992,\\n        -0.02208179607987404,\\n        -0.010041477158665657,\\n        0.022838113829493523,\\n        0.023175757378339767,\\n        0.00923788920044899,\\n        0.0267952810972929,\\n        0.01242523081600666,\\n        -0.027200451120734215,\\n        0.011128684505820274,\\n        0.010966616682708263,\\n        -0.0065164933912456036,\\n        -0.014842748641967773,\\n        0.007286317180842161,\\n        -0.002145715756341815,\\n        -0.0007896605529822409,\\n        0.04324520379304886,\\n        -0.016125788912177086,\\n        0.02133898250758648,\\n        -0.006249756086617708,\\n        0.007752263452857733,\\n        -0.033034905791282654,\\n        0.008042635396122932,\\n        0.009305417537689209,\\n        0.01600423827767372,\\n        -0.010568198747932911,\\n        -0.0014366672839969397,\\n        -0.0142484987154603,\\n        0.008056141436100006,\\n        0.000470588740427047,\\n        0.0022115560714155436,\\n        0.05064631998538971,\\n        -0.02244644984602928,\\n        0.03338605538010597,\\n        0.0041462453082203865,\\n        0.0004359804152045399,\\n        -0.027848724275827408,\\n        0.007455138489603996,\\n        0.01488326583057642,\\n        0.011662159115076065,\\n        -0.001961700851097703,\\n        -0.00851533468812704,\\n        0.0005845429259352386,\\n        0.00053220841800794,\\n        0.007083732169121504,\\n        0.016625499352812767,\\n        -0.0165309589356184,\\n        -0.019299624487757683,\\n        0.02399960346519947,\\n        -0.01129075326025486,\\n        0.018003078177571297,\\n        -0.002427646890282631,\\n        -0.005922242999076843,\\n        0.02828090637922287,\\n        -0.007117496337741613,\\n        0.009575530886650085,\\n        -0.01006848830729723,\\n        -0.016557971015572548,\\n        -0.01585567556321621,\\n        0.011533855460584164,\\n        -0.014113441109657288,\\n        -0.020731227472424507,\\n        -0.04116532951593399,\\n        0.03522282838821411,\\n        0.0038761317264288664,\\n        -0.022568000480532646,\\n        -0.0033899270929396152,\\n        -0.0046999785117805,\\n        -0.022554494440555573,\\n        0.0003549462999217212,\\n        0.003869378939270973,\\n        0.019907381385564804,\\n        0.029361359775066376,\\n        0.003609394421800971,\\n        0.03090100921690464,\\n        -0.01488326583057642,\\n        -0.03430444002151489,\\n        -0.010608715936541557,\\n        -0.0013564772671088576,\\n        -0.007529419846832752,\\n        -0.014180969446897507,\\n        -0.020690711215138435\\n      ],\\n      \"index\": 0,\\n      \"object\": \"embedding\"\\n    }\\n  ],\\n  \"model\": \"ada\",\\n  \"object\": \"list\",\\n  \"usage\": {\\n    \"prompt_tokens\": 7,\\n    \"total_tokens\": 7\\n  }\\n}', parent_id='0x1bb2aa9e6b530b09', session_id=None, span_id='0xde147526538f191d', span_type='SpanKind.INTERNAL', start_time='2024-02-05T15:00:17.067114Z', status='OK', trace_id='0xccf497116a5a912b29e55229bbac0ba8', user_id=None)])]\n"
     ]
    }
   ],
   "source": [
    "logs_schema = StructType([\n",
    "        StructField('attributes', StringType(), True),\n",
    "        StructField('end_time', StringType(), True),\n",
    "        StructField('events', ArrayType(StringType(), True), True), \n",
    "        StructField('input', StringType(), True), \n",
    "        StructField('links', ArrayType(StringType(), True), True), \n",
    "        StructField('name', StringType(), True), \n",
    "        StructField('output', StringType(), True), \n",
    "        StructField('parent_id', StringType(), True), \n",
    "        StructField('session_id', StringType(), True), \n",
    "        StructField('span_id', StringType(), True), \n",
    "        StructField('span_type', StringType(), True), \n",
    "        StructField('start_time', StringType(), True), \n",
    "        StructField('status', StringType(), True), \n",
    "        StructField('trace_id', StringType(), True),\n",
    "        StructField('user_id', StringType(), True)\n",
    "])\n",
    "\n",
    "logs_data = df.select('data').collect()\n",
    "print(logs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkValueError",
     "evalue": "[CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m span_logs \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alanpoblette\\AppData\\Local\\anaconda3\\envs\\momo-assets\\lib\\site-packages\\pyspark\\sql\\session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m   1442\u001b[0m     )\n\u001b[1;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alanpoblette\\AppData\\Local\\anaconda3\\envs\\momo-assets\\lib\\site-packages\\pyspark\\sql\\session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32mc:\\Users\\alanpoblette\\AppData\\Local\\anaconda3\\envs\\momo-assets\\lib\\site-packages\\pyspark\\sql\\session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m   1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m-> 1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1094\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[0;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[1;32mc:\\Users\\alanpoblette\\AppData\\Local\\anaconda3\\envs\\momo-assets\\lib\\site-packages\\pyspark\\sql\\session.py:969\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    955\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[0;32m    956\u001b[0m     _merge_type,\n\u001b[0;32m    957\u001b[0m     (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m     ),\n\u001b[0;32m    967\u001b[0m )\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[0;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    972\u001b[0m     )\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m schema\n",
      "\u001b[1;31mPySparkValueError\u001b[0m: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring."
     ]
    }
   ],
   "source": [
    "span_logs = spark.createDataFrame(data=logs_data, schema=list[str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[INVALID_USAGE_OF_STAR_OR_REGEX] Invalid usage of '*' in expression `collect_list`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m traces \u001b[38;5;241m=\u001b[39m \u001b[43mspan_logs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan_logs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alanpoblette\\AppData\\Local\\anaconda3\\envs\\momo-assets\\lib\\site-packages\\pyspark\\sql\\group.py:186\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    185\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[1;32m--> 186\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[1;32mc:\\Users\\alanpoblette\\AppData\\Local\\anaconda3\\envs\\momo-assets\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\alanpoblette\\AppData\\Local\\anaconda3\\envs\\momo-assets\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [INVALID_USAGE_OF_STAR_OR_REGEX] Invalid usage of '*' in expression `collect_list`."
     ]
    }
   ],
   "source": [
    "traces = span_logs.groupBy(span_logs.trace_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momo-assets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
