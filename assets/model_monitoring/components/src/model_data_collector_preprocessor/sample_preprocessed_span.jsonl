{"specversion":"1.0","id":"6fee5e87-e702-40f5-9d5b-460e683b3f6e","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x7fd179134fb9e709","parent_id":null,"user_id":"","session_id":"","span_type":"SpanKind.INTERNAL","start_time":"2024-02-05T15:00:13.789782Z","end_time":"2024-02-05T15:00:18.791564Z","name":"promptflow.flow","status":"OK","input":"{\n  \"question\": \"what's Attention?\",\n  \"chat_history\": []\n}","output":"{\n  \"output\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\"\n}","attributes":{"framework":"promptflow","span_type":"Flow","inputs":"{\n  \"question\": \"what's Attention?\",\n  \"chat_history\": []\n}","output":"{\n  \"output\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\"\n}"},"events":[],"links":[]}],"contentrange":"bytes 0-867/868","correlationid":"ee3c0e82-7edf-480e-9c24-34e19f4f7f1d","xrequestid":"ee3c0e82-7edf-480e-9c24-34e19f4f7f1d","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"59ab58f7-cbf6-4f4f-af8d-7352eed49f0b","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:17Z","data":[{"name":"openai.resources.chat.completions.Completions.create","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x6275bb0c2a2594e0","span_type":"SpanKind.INTERNAL","parent_id":"0x21d2ebaaad77bedf","start_time":"2024-02-05T15:00:13.796476Z","end_time":"2024-02-05T15:00:14.436365Z","status":"OK","input":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Given the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history:\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Follow up Input: what's Attention? \\nStandalone Question:\"\n    }\n  ],\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": null,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8oukAwNXR2qyf3yQzLGoZ8o2M8Ka5\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"What is the meaning of Attention?\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707145214,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 7,\n    \"prompt_tokens\": 78,\n    \"total_tokens\": 85\n  }\n}","attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.chat.completions.Completions.create","node_name":"modify_query_with_history","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Given the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history:\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Follow up Input: what's Attention? \\nStandalone Question:\"\n    }\n  ],\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": null,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8oukAwNXR2qyf3yQzLGoZ8o2M8Ka5\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"What is the meaning of Attention?\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707145214,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 7,\n    \"prompt_tokens\": 78,\n    \"total_tokens\": 85\n  }\n}"},"events":[],"links":[]}],"correlationid":"0683a146-03f1-4aae-8131-d0d63b2e4187","xrequestid":"0683a146-03f1-4aae-8131-d0d63b2e4187","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-2125/2126"}
{"specversion":"1.0","id":"37730a53-a3fe-425d-98c9-a0128b271d30","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"generate_prompt_context","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x3d101a004d88e6d0","span_type":"SpanKind.INTERNAL","parent_id":"0x7fd179134fb9e709","start_time":"2024-02-05T15:00:17.668465Z","end_time":"2024-02-05T15:00:17.672230Z","status":"OK","input":"{\n  \"search_result\": [\n    {\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf13\",\n        \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n        \"mtime\": null,\n        \"page_number\": 13,\n        \"stats\": {\n          \"tiktokens\": 304,\n          \"chars\": 835,\n          \"lines\": 113\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.39159566164016724\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf12\",\n        \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n        \"mtime\": null,\n        \"page_number\": 12,\n        \"stats\": {\n          \"tiktokens\": 254,\n          \"chars\": 833,\n          \"lines\": 73\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.39190346002578735\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf14\",\n        \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n        \"mtime\": null,\n        \"page_number\": 14,\n        \"stats\": {\n          \"tiktokens\": 291,\n          \"chars\": 838,\n          \"lines\": 113\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.42808181047439575\n    }\n  ]\n}","output":"\"Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\"","attributes":{"framework":"promptflow","span_type":"Tool","function":"generate_prompt_context","node_name":"generate_prompt_context","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"search_result\": [\n    {\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf13\",\n        \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n        \"mtime\": null,\n        \"page_number\": 13,\n        \"stats\": {\n          \"tiktokens\": 304,\n          \"chars\": 835,\n          \"lines\": 113\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.39159566164016724\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf12\",\n        \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n        \"mtime\": null,\n        \"page_number\": 12,\n        \"stats\": {\n          \"tiktokens\": 254,\n          \"chars\": 833,\n          \"lines\": 73\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.39190346002578735\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf14\",\n        \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n        \"mtime\": null,\n        \"page_number\": 14,\n        \"stats\": {\n          \"tiktokens\": 291,\n          \"chars\": 838,\n          \"lines\": 113\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.42808181047439575\n    }\n  ]\n}","output":"\"Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\""},"events":[],"links":[]}],"modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-9664/9665","correlationid":"a8d25f2e-2089-4c40-8b8d-5288feaf4700","xrequestid":"a8d25f2e-2089-4c40-8b8d-5288feaf4700"}
{"specversion":"1.0","id":"7945cce6-a17f-4710-86ce-d6fa6c9f7c1f","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"answer_the_question_with_context","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0xcda5d0e89f4747df","span_type":"SpanKind.INTERNAL","parent_id":"0x7fd179134fb9e709","start_time":"2024-02-05T15:00:17.680046Z","end_time":"2024-02-05T15:00:18.788089Z","status":"OK","input":"{\n  \"prompt\": \"{{prompt_text}}\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"prompt_text\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf \\n\\n chat history: \\nuser: what's Attention? \\nassistant:\",\n  \"stream\": false\n}","output":"\"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\"","attributes":{"framework":"promptflow","span_type":"Tool","function":"AzureOpenAI.chat","node_name":"answer_the_question_with_context","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"prompt\": \"{{prompt_text}}\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"prompt_text\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf \\n\\n chat history: \\nuser: what's Attention? \\nassistant:\",\n  \"stream\": false\n}","output":"\"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\""},"events":[],"links":[]}],"contentrange":"bytes 0-5386/5387","correlationid":"566b7a59-9c2a-4756-ae2f-446a7f2cd9b6","xrequestid":"566b7a59-9c2a-4756-ae2f-446a7f2cd9b6","agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame"}
{"specversion":"1.0","id":"b55484ce-43a0-4f68-a5d4-4a592c64bfea","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"vector_lookup","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0xd25a6f7c89885b7f","span_type":"SpanKind.INTERNAL","parent_id":"0x7fd179134fb9e709","start_time":"2024-02-05T15:00:14.441572Z","end_time":"2024-02-05T15:00:17.625517Z","status":"OK","input":"{\n  \"mlindex_content\": \"embeddings:\\n  api_base: https://promptflow-ci-weu.openai.azure.com/\\n  api_type: azure\\n  api_version: 2023-03-15-preview\\n  batch_size: '16'\\n  connection:\\n    id: \\n      /subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/connections/azure_open_ai_connection\\n  connection_type: workspace_connection\\n  deployment: text-embedding-ada-003\\n  dimension: 1536\\n  file_format_version: '2'\\n  kind: open_ai\\n  model: text-embedding-ada-002\\n  schema_version: '2'\\nindex:\\n  engine: langchain.vectorstores.FAISS\\n  kind: faiss\\n  method: FlatL2\\n  path: \\n    azureml://subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourcegroups/promptflow/workspaces/pf-xp/datastores/workspaceblobstore/paths/azureml/f7ee0381-06fc-4566-820f-86ba174db987/index/\\nself:\\n  path: \\n    azureml://subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourcegroups/promptflow/workspaces/pf-xp/datastores/workspaceblobstore/paths/azureml/f7ee0381-06fc-4566-820f-86ba174db987/index/\\n  asset_id: \\n    azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/serene-honey-h9lrkyczzk/versions/1\\n\",\n  \"queries\": \"What is the meaning of Attention?\",\n  \"query_type\": \"Vector\",\n  \"top_k\": 3\n}","output":"[\n  {\n    \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n    \"metadata\": {\n      \"source_doc_id\": \"1706.03762.pdf13\",\n      \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n      \"mtime\": null,\n      \"page_number\": 13,\n      \"stats\": {\n        \"tiktokens\": 304,\n        \"chars\": 835,\n        \"lines\": 113\n      },\n      \"source\": {\n        \"filename\": \"1706.03762.pdf\",\n        \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n        \"mtime\": 1706775620.0\n      }\n    },\n    \"score\": 0.39159566164016724\n  },\n  {\n    \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n    \"metadata\": {\n      \"source_doc_id\": \"1706.03762.pdf12\",\n      \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n      \"mtime\": null,\n      \"page_number\": 12,\n      \"stats\": {\n        \"tiktokens\": 254,\n        \"chars\": 833,\n        \"lines\": 73\n      },\n      \"source\": {\n        \"filename\": \"1706.03762.pdf\",\n        \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n        \"mtime\": 1706775620.0\n      }\n    },\n    \"score\": 0.39190346002578735\n  },\n  {\n    \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n    \"metadata\": {\n      \"source_doc_id\": \"1706.03762.pdf14\",\n      \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n      \"mtime\": null,\n      \"page_number\": 14,\n      \"stats\": {\n        \"tiktokens\": 291,\n        \"chars\": 838,\n        \"lines\": 113\n      },\n      \"source\": {\n        \"filename\": \"1706.03762.pdf\",\n        \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n        \"mtime\": 1706775620.0\n      }\n    },\n    \"score\": 0.42808181047439575\n  }\n]","attributes":{"framework":"promptflow","span_type":"Tool","function":"search","node_name":"vector_lookup","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"mlindex_content\": \"embeddings:\\n  api_base: https://promptflow-ci-weu.openai.azure.com/\\n  api_type: azure\\n  api_version: 2023-03-15-preview\\n  batch_size: '16'\\n  connection:\\n    id: \\n      /subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/connections/azure_open_ai_connection\\n  connection_type: workspace_connection\\n  deployment: text-embedding-ada-003\\n  dimension: 1536\\n  file_format_version: '2'\\n  kind: open_ai\\n  model: text-embedding-ada-002\\n  schema_version: '2'\\nindex:\\n  engine: langchain.vectorstores.FAISS\\n  kind: faiss\\n  method: FlatL2\\n  path: \\n    azureml://subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourcegroups/promptflow/workspaces/pf-xp/datastores/workspaceblobstore/paths/azureml/f7ee0381-06fc-4566-820f-86ba174db987/index/\\nself:\\n  path: \\n    azureml://subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourcegroups/promptflow/workspaces/pf-xp/datastores/workspaceblobstore/paths/azureml/f7ee0381-06fc-4566-820f-86ba174db987/index/\\n  asset_id: \\n    azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/serene-honey-h9lrkyczzk/versions/1\\n\",\n  \"queries\": \"What is the meaning of Attention?\",\n  \"query_type\": \"Vector\",\n  \"top_k\": 3\n}","output":"[\n  {\n    \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n    \"metadata\": {\n      \"source_doc_id\": \"1706.03762.pdf13\",\n      \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n      \"mtime\": null,\n      \"page_number\": 13,\n      \"stats\": {\n        \"tiktokens\": 304,\n        \"chars\": 835,\n        \"lines\": 113\n      },\n      \"source\": {\n        \"filename\": \"1706.03762.pdf\",\n        \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n        \"mtime\": 1706775620.0\n      }\n    },\n    \"score\": 0.39159566164016724\n  },\n  {\n    \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n    \"metadata\": {\n      \"source_doc_id\": \"1706.03762.pdf12\",\n      \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n      \"mtime\": null,\n      \"page_number\": 12,\n      \"stats\": {\n        \"tiktokens\": 254,\n        \"chars\": 833,\n        \"lines\": 73\n      },\n      \"source\": {\n        \"filename\": \"1706.03762.pdf\",\n        \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n        \"mtime\": 1706775620.0\n      }\n    },\n    \"score\": 0.39190346002578735\n  },\n  {\n    \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n    \"metadata\": {\n      \"source_doc_id\": \"1706.03762.pdf14\",\n      \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n      \"mtime\": null,\n      \"page_number\": 14,\n      \"stats\": {\n        \"tiktokens\": 291,\n        \"chars\": 838,\n        \"lines\": 113\n      },\n      \"source\": {\n        \"filename\": \"1706.03762.pdf\",\n        \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n        \"mtime\": 1706775620.0\n      }\n    },\n    \"score\": 0.42808181047439575\n  }\n]"},"events":[],"links":[]}],"contentrange":"bytes 0-7205/7206","correlationid":"c8f691e6-376b-4ad0-973a-f5c1a1882b57","xrequestid":"c8f691e6-376b-4ad0-973a-f5c1a1882b57","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"bcf9fd6a-694b-43fc-a690-7bb16764adba","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"openai.resources.chat.completions.Completions.create","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x52355639a5440992","span_type":"SpanKind.INTERNAL","parent_id":"0xcda5d0e89f4747df","start_time":"2024-02-05T15:00:17.682481Z","end_time":"2024-02-05T15:00:18.787224Z","status":"OK","input":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf \\n\\n chat history: \\nuser: what's Attention? \\nassistant:\"\n    }\n  ],\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8oukE8DGO1ZnKNpbqZMpApVzxy0bX\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707145218,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 58,\n    \"prompt_tokens\": 1110,\n    \"total_tokens\": 1168\n  }\n}","attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.chat.completions.Completions.create","node_name":"answer_the_question_with_context","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf \\n\\n chat history: \\nuser: what's Attention? \\nassistant:\"\n    }\n  ],\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8oukE8DGO1ZnKNpbqZMpApVzxy0bX\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707145218,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 58,\n    \"prompt_tokens\": 1110,\n    \"total_tokens\": 1168\n  }\n}"},"events":[],"links":[]}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-6179/6180","correlationid":"b1ffd289-d325-477e-a3d5-84adf6cb34c8","xrequestid":"b1ffd289-d325-477e-a3d5-84adf6cb34c8","modelversion":"default"}
{"specversion":"1.0","id":"43d2493b-33d2-4cae-887c-430662155df8","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"Prompt_variants","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x1a77eee854345f20","span_type":"SpanKind.INTERNAL","parent_id":"0x7fd179134fb9e709","start_time":"2024-02-05T15:00:17.675380Z","end_time":"2024-02-05T15:00:17.677929Z","status":"OK","input":"{\n  \"template\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n {{contexts}} \\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\nuser: {{question}} \\nassistant:\",\n  \"chat_history\": [],\n  \"contexts\": \"Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n  \"question\": \"what's Attention?\"\n}","output":"\"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf \\n\\n chat history: \\nuser: what's Attention? \\nassistant:\"","attributes":{"framework":"promptflow","span_type":"Tool","function":"render_template_jinja2","node_name":"Prompt_variants","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"template\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n {{contexts}} \\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\nuser: {{question}} \\nassistant:\",\n  \"chat_history\": [],\n  \"contexts\": \"Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n  \"question\": \"what's Attention?\"\n}","output":"\"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf \\n\\n chat history: \\nuser: what's Attention? \\nassistant:\""},"events":[],"links":[]}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-9201/9202","correlationid":"81134217-0100-463c-943e-e739c36c902c","xrequestid":"81134217-0100-463c-943e-e739c36c902c","modelversion":"default"}
{"specversion":"1.0","id":"9be18d14-3fac-4942-b5e1-9ccda58d8c4e","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"search","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x1bb2aa9e6b530b09","span_type":"SpanKind.INTERNAL","parent_id":"0xd25a6f7c89885b7f","start_time":"2024-02-05T15:00:16.775105Z","end_time":"2024-02-05T15:00:17.620704Z","status":"UNSET","attributes":{"span_type":"Retrieval","retrieval.query":"What is the meaning of Attention?","retrieval.documents":"[\n    {\n        \"document.content\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n        \"document.score\": 0.39159566164016724,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf13\",\n            \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n            \"mtime\": null,\n            \"page_number\": 13,\n            \"stats\": {\n                \"tiktokens\": 304,\n                \"chars\": 835,\n                \"lines\": 113\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"document.score\": 0.39190346002578735,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf12\",\n            \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n            \"mtime\": null,\n            \"page_number\": 12,\n            \"stats\": {\n                \"tiktokens\": 254,\n                \"chars\": 833,\n                \"lines\": 73\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n        \"document.score\": 0.42808181047439575,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf14\",\n            \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n            \"mtime\": null,\n            \"page_number\": 14,\n            \"stats\": {\n                \"tiktokens\": 291,\n                \"chars\": 838,\n                \"lines\": 113\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml://locations/centralus/workspaces/e96a5813-c919-42fc-b816-908261dd06ab/data/vector-index-input-1706775617472/versions/1/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    }\n]"},"events":[],"links":[]}],"correlationid":"1c16dc7d-c10e-4d43-9b48-b9be92e2b4f8","xrequestid":"1c16dc7d-c10e-4d43-9b48-b9be92e2b4f8","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-6141/6142"}
{"specversion":"1.0","id":"d8fa94f7-0d57-478f-8398-612e070ca12a","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:17Z","data":[{"name":"modify_query_with_history","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x21d2ebaaad77bedf","span_type":"SpanKind.INTERNAL","parent_id":"0x7fd179134fb9e709","start_time":"2024-02-05T15:00:13.792668Z","end_time":"2024-02-05T15:00:14.437464Z","status":"OK","input":"{\n  \"prompt\": \"system: \\nGiven the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\n\\nuser:\\nFollow up Input: {{question}} \\nStandalone Question:\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"chat_history\": [],\n  \"question\": \"what's Attention?\"\n}","output":"\"What is the meaning of Attention?\"","attributes":{"framework":"promptflow","span_type":"Tool","function":"AzureOpenAI.chat","node_name":"modify_query_with_history","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"prompt\": \"system: \\nGiven the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\n\\nuser:\\nFollow up Input: {{question}} \\nStandalone Question:\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"chat_history\": [],\n  \"question\": \"what's Attention?\"\n}","output":"\"What is the meaning of Attention?\""},"events":[],"links":[]}],"xrequestid":"b7fdb4b5-4904-45db-b1a5-ec744a757585","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-1428/1429","correlationid":"b7fdb4b5-4904-45db-b1a5-ec744a757585"}
{"specversion":"1.0","id":"939b0ed2-e8b5-481a-983c-7fddbd752d4f","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"openai.resources.embeddings.Embeddings.create","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0xde147526538f191d","span_type":"SpanKind.INTERNAL","parent_id":"0x1bb2aa9e6b530b09","start_time":"2024-02-05T15:00:17.067114Z","end_time":"2024-02-05T15:00:17.619294Z","status":"OK","input":"{\n  \"input\": [\n    [\n      3923,\n      374,\n      279,\n      7438,\n      315,\n      63120,\n      30\n    ]\n  ],\n  \"model\": \"text-embedding-ada-002\"\n}","output":"{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.03181939572095871,\n        0.0023466129787266254,\n        0.03857223689556122,\n        -0.014059418812394142,\n        -0.005179430358111858,\n        0.014072923921048641,\n        -0.027767689898610115,\n        -0.017179232090711594,\n        -0.01000095997005701,\n        -0.023594433441758156,\n        0.0336291566491127,\n        0.020055942237377167,\n        -0.013937867246568203,\n        0.006378060206770897,\n        0.014329532161355019,\n        -0.013451662845909595,\n        0.043164171278476715,\n        0.010541187599301338,\n        -0.01359347254037857,\n        -0.030036645010113716,\n        -0.02895618975162506,\n        -7.66025623306632e-05,\n        0.0011108426842838526,\n        0.02378351241350174,\n        -0.007610453758388758,\n        0.011182707734405994,\n        0.011094920337200165,\n        -0.024728910997509956,\n        -0.006685314234346151,\n        -0.038842350244522095,\n        0.023418858647346497,\n        0.004946457222104073,\n        0.006573892664164305,\n        -0.020380079746246338,\n        -0.017003657296299934,\n        -0.004686472937464714,\n        0.004949833732098341,\n        -0.028605042025446892,\n        0.020326057448983192,\n        -0.00028868403751403093,\n        0.02526913769543171,\n        0.024013109505176544,\n        -0.0022486967500299215,\n        0.00564537663012743,\n        0.0006626226822845638,\n        0.006668432150036097,\n        0.008089905604720116,\n        -0.028361938893795013,\n        -0.010784289799630642,\n        0.014653668738901615,\n        0.028794120997190475,\n        0.03271077200770378,\n        -0.029847566038370132,\n        -0.02616051211953163,\n        0.0024951754603534937,\n        -0.004916069563478231,\n        -0.004412982612848282,\n        0.0013429715763777494,\n        0.013742035254836082,\n        0.0015455569373443723,\n        -0.0005807444686070085,\n        -0.013120773248374462,\n        -0.010547940619289875,\n        0.01214161142706871,\n        -0.0017489863093942404,\n        0.01996140368282795,\n        -0.00020986569870729,\n        0.019515715539455414,\n        -0.010784289799630642,\n        -0.007610453758388758,\n        0.020366573706269264,\n        0.022595012560486794,\n        0.025606779381632805,\n        -0.01546400971710682,\n        0.026065973564982414,\n        -0.01551803294569254,\n        -0.020434102043509483,\n        -0.008501828648149967,\n        0.010932852514088154,\n        0.026808785274624825,\n        0.005871596746146679,\n        -0.0004243739531375468,\n        -0.0043994770385324955,\n        0.03319697454571724,\n        -0.016760556027293205,\n        0.0012771313777193427,\n        0.007765769027173519,\n        0.002853075973689556,\n        -0.000697653042152524,\n        -0.004453499801456928,\n        -0.012803389690816402,\n        -0.030036645010113716,\n        0.02590390481054783,\n        0.008792201057076454,\n        0.0159637201577425,\n        0.025539251044392586,\n        -0.0046898494474589825,\n        0.0365193746984005,\n        -0.02611999586224556,\n        -0.022203346714377403,\n        0.0023753123823553324,\n        0.01688210666179657,\n        -0.032224565744400024,\n        -0.0002006860449910164,\n        0.007286317180842161,\n        -0.0017059368547052145,\n        -0.019178073853254318,\n        -0.0076172067783772945,\n        0.002799053443595767,\n        -0.01794905588030815,\n        -0.03100905381143093,\n        0.02657919004559517,\n        -0.008218209259212017,\n        -0.04481186345219612,\n        0.003481090534478426,\n        0.015153379179537296,\n        -0.004122610669583082,\n        -0.0030792963225394487,\n        -0.03446650877594948,\n        -0.018786408007144928,\n        0.0302257239818573,\n        0.007644217927008867,\n        0.018529800698161125,\n        -0.0014062795089557767,\n        -0.003997683059424162,\n        0.018988993018865585,\n        -0.008771942928433418,\n        -0.01928611844778061,\n        -0.016341879963874817,\n        -0.013053244911134243,\n        0.006364554166793823,\n        0.030873997136950493,\n        0.0016147735295817256,\n        0.013087009079754353,\n        -0.0053887683898210526,\n        0.01751687377691269,\n        0.004666214343160391,\n        -0.0027146427892148495,\n        -0.05175378546118736,\n        -0.019137555733323097,\n        0.0069689336232841015,\n        0.024161672219634056,\n        -0.0064523410983383656,\n        -0.0075901951640844345,\n        0.01862434111535549,\n        0.030522849410772324,\n        0.02611999586224556,\n        0.012202386744320393,\n        0.0072593060322105885,\n        0.010271074250340462,\n        0.03092801943421364,\n        0.006749466527253389,\n        0.01808411255478859,\n        -0.002189609222114086,\n        0.0005111058126203716,\n        0.017935549840331078,\n        -0.007144507486373186,\n        0.0018975487910211086,\n        0.011500091291964054,\n        -0.025606779381632805,\n        0.033467087894678116,\n        -0.009649812243878841,\n        0.010041477158665657,\n        -0.026565684005618095,\n        0.02686280943453312,\n        0.045081976801157,\n        0.0035519953817129135,\n        0.01152034942060709,\n        -0.005162548273801804,\n        -0.002206491306424141,\n        -0.016112282872200012,\n        0.023702478036284447,\n        -0.03138721361756325,\n        0.017611414194107056,\n        -0.013829821720719337,\n        0.018165146932005882,\n        0.027659643441438675,\n        0.0012366143055260181,\n        -0.016085270792245865,\n        -0.00223856745287776,\n        -0.016868600621819496,\n        -0.0010838313028216362,\n        0.01834072172641754,\n        0.010892335325479507,\n        -0.026957347989082336,\n        -0.002802429720759392,\n        0.010304838418960571,\n        -0.006236250512301922,\n        0.0056420001201331615,\n        -0.01177020464092493,\n        0.01546400971710682,\n        0.03430444002151489,\n        -0.004075340460985899,\n        -0.008258726447820663,\n        -0.6651279926300049,\n        -0.003524984000250697,\n        0.006502987816929817,\n        -0.005297604948282242,\n        -0.0007613830384798348,\n        0.004645955748856068,\n        0.01688210666179657,\n        0.011000380851328373,\n        -0.007536172401160002,\n        0.028226882219314575,\n        -0.01309376209974289,\n        -0.007718499284237623,\n        -0.009474238380789757,\n        -0.0015143250348046422,\n        -0.019191579893231392,\n        -0.017395323142409325,\n        -0.012384713627398014,\n        -0.016801072284579277,\n        -0.012816895730793476,\n        0.008906999602913857,\n        -0.03057687170803547,\n        0.022676046937704086,\n        -0.019340142607688904,\n        0.015234413556754589,\n        0.030522849410772324,\n        -0.012384713627398014,\n        -0.000990135595202446,\n        -0.025282643735408783,\n        -0.02229788713157177,\n        0.022135818377137184,\n        -0.0076982406899333,\n        -0.01122322492301464,\n        0.027051888406276703,\n        0.0230812169611454,\n        0.029469406232237816,\n        -0.020474620163440704,\n        -0.004902563989162445,\n        0.02718694508075714,\n        -0.04286704584956169,\n        0.03379122540354729,\n        0.008204704150557518,\n        -0.0033173339907079935,\n        0.021082375198602676,\n        -0.021501051262021065,\n        -0.0043522072955966,\n        -0.016409408301115036,\n        0.02474241517484188,\n        -0.0087044145911932,\n        0.0005136380787007511,\n        -0.028578029945492744,\n        -0.01245899498462677,\n        0.0074753970839083195,\n        -0.00895426981151104,\n        -0.005091643426567316,\n        0.006286896765232086,\n        0.002908786991611123,\n        0.01961025595664978,\n        0.0027095782570540905,\n        0.019205084070563316,\n        0.0023263543844223022,\n        -0.00943372119218111,\n        0.045811284333467484,\n        -0.00444337073713541,\n        -0.013742035254836082,\n        -0.02753809280693531,\n        0.003401744645088911,\n        0.016720037907361984,\n        0.03092801943421364,\n        0.01830020360648632,\n        -0.034601565450429916,\n        0.0030573494732379913,\n        0.029226303100585938,\n        -0.007549678441137075,\n        0.0075901951640844345,\n        0.016301361843943596,\n        0.003043843898922205,\n        -0.001223952742293477,\n        0.013816316612064838,\n        0.020163988694548607,\n        0.015126368030905724,\n        -0.015139873139560223,\n        -0.008785448037087917,\n        -0.0037782154977321625,\n        -0.020150482654571533,\n        0.02155507355928421,\n        0.007711746264249086,\n        -0.0025356924161314964,\n        -0.006908158306032419,\n        5.09364836034365e-05,\n        -0.011405551806092262,\n        -0.012040318921208382,\n        -0.0013387510553002357,\n        0.004966715816408396,\n        -0.041894637048244476,\n        0.013937867246568203,\n        0.01801658421754837,\n        -0.009001539088785648,\n        0.008994787000119686,\n        0.00886648241430521,\n        0.023702478036284447,\n        0.0038524968549609184,\n        -0.010858571156859398,\n        -0.02092030644416809,\n        0.002834505867213011,\n        0.005013985559344292,\n        -0.008218209259212017,\n        -0.018462272360920906,\n        0.0330619178712368,\n        0.04027395322918892,\n        -0.008744931779801846,\n        -0.0026420496869832277,\n        -0.005675764288753271,\n        0.008852977305650711,\n        0.009251394309103489,\n        -0.002478293376043439,\n        -0.02605246752500534,\n        0.02002893202006817,\n        0.0142484987154603,\n        0.008630133233964443,\n        -0.008251973427832127,\n        0.010541187599301338,\n        -0.011621642857789993,\n        0.01840825006365776,\n        -0.024728910997509956,\n        -0.010014466010034084,\n        -0.012296927161514759,\n        -0.0017979444237425923,\n        -0.008981280960142612,\n        -0.03387225791811943,\n        0.009447227232158184,\n        0.0176654364913702,\n        -0.0009572154376655817,\n        -0.004125986713916063,\n        -0.01274261437356472,\n        0.001882354961708188,\n        0.00665492657572031,\n        -0.019029511138796806,\n        -0.03854522481560707,\n        0.022635528817772865,\n        -0.03932855650782585,\n        -0.017935549840331078,\n        0.019137555733323097,\n        0.017854515463113785,\n        -0.01794905588030815,\n        -0.028226882219314575,\n        -0.020960824564099312,\n        -0.0015404922887682915,\n        0.001764180138707161,\n        -0.028902167454361916,\n        -0.0065637631341814995,\n        0.009636306203901768,\n        -0.01233744341880083,\n        -0.02087979018688202,\n        0.013289595022797585,\n        -0.0148157374933362,\n        -0.011466327123343945,\n        -0.016233833506703377,\n        -0.014937288127839565,\n        -0.023634949699044228,\n        -0.03160330280661583,\n        0.013141032308340073,\n        0.015369470231235027,\n        -0.021136397495865822,\n        -0.022905642166733742,\n        -0.024999024346470833,\n        -0.01688210666179657,\n        -0.012729108333587646,\n        0.03967970237135887,\n        -0.04575726389884949,\n        -0.025863388553261757,\n        0.023837534710764885,\n        -0.012317185290157795,\n        0.003997683059424162,\n        0.010210298001766205,\n        -0.0060843112878501415,\n        0.025863388553261757,\n        -0.017138715833425522,\n        -0.006320660933852196,\n        0.011993048712611198,\n        -0.006492858286947012,\n        0.015653088688850403,\n        0.0036060181446373463,\n        -0.02169013023376465,\n        0.0016586669953539968,\n        0.009291911497712135,\n        0.011446068063378334,\n        0.04491991177201271,\n        0.013316606171429157,\n        -0.0022942782379686832,\n        0.009913173504173756,\n        -0.004818153101950884,\n        0.0071850246749818325,\n        -0.018070606514811516,\n        0.01003472413867712,\n        -0.0009344246354885399,\n        0.0153964813798666,\n        0.015653088688850403,\n        0.0055001904256641865,\n        0.0030944901518523693,\n        0.03743775933980942,\n        0.029037224128842354,\n        0.00429143151268363,\n        0.01928611844778061,\n        -0.010500670410692692,\n        0.009609295055270195,\n        0.017017163336277008,\n        -0.0009310481837019324,\n        -0.025890398770570755,\n        0.008191198110580444,\n        -0.0011327894171699882,\n        0.005797315388917923,\n        -0.027740677818655968,\n        -0.009453980252146721,\n        -0.010730267502367496,\n        0.015693606808781624,\n        0.02286512590944767,\n        -0.0020815636962652206,\n        0.004818153101950884,\n        -0.034709613770246506,\n        0.017705954611301422,\n        0.0213795006275177,\n        -0.0056521291844546795,\n        -0.011412303894758224,\n        -0.0026369851548224688,\n        0.005837832577526569,\n        0.003325775032863021,\n        0.034034326672554016,\n        0.04654059186577797,\n        -0.009258147329092026,\n        -0.002203115029260516,\n        -0.018880948424339294,\n        0.024161672219634056,\n        0.014950794167816639,\n        0.013755540363490582,\n        -0.02013697661459446,\n        -0.0006900561274960637,\n        0.01609877683222294,\n        -0.0067528425715863705,\n        0.00794809591025114,\n        -0.025228621438145638,\n        0.007610453758388758,\n        0.01670653373003006,\n        -0.00128557241987437,\n        -0.014113441109657288,\n        0.016828084364533424,\n        0.002250384772196412,\n        0.03805902227759361,\n        0.01677406206727028,\n        0.009960442781448364,\n        -0.003947036806493998,\n        -0.0043994770385324955,\n        -0.008711167611181736,\n        -0.002922292798757553,\n        -0.0003201269428245723,\n        -0.01635538600385189,\n        0.0037241927348077297,\n        -0.0025930916890501976,\n        -0.013458415865898132,\n        0.011689171195030212,\n        0.03784292936325073,\n        0.01943468116223812,\n        0.022676046937704086,\n        0.02884814515709877,\n        -0.0017658683937042952,\n        -0.006334166508167982,\n        0.014464588835835457,\n        0.011736440472304821,\n        -0.004622321110218763,\n        -0.004787765443325043,\n        0.002299343002960086,\n        0.008535593748092651,\n        -0.014640163630247116,\n        0.0021372747141867876,\n        -0.014140453189611435,\n        0.01840825006365776,\n        0.009291911497712135,\n        -0.019772322848439217,\n        -0.0008521243580617011,\n        0.019704794511198997,\n        0.0174088291823864,\n        -0.006877770181745291,\n        -0.027011370286345482,\n        -0.006641421001404524,\n        0.0076982406899333,\n        -0.0059053609147667885,\n        -0.025998445227742195,\n        0.013505685143172741,\n        0.005375262815505266,\n        -0.016476936638355255,\n        0.0035823830403387547,\n        -0.009899667464196682,\n        -0.009339181706309319,\n        -0.006320660933852196,\n        -0.013228818774223328,\n        0.0018975487910211086,\n        -0.012101094238460064,\n        0.011304258368909359,\n        0.013742035254836082,\n        0.0142620038241148,\n        -0.03011767938733101,\n        -0.002380377147346735,\n        -0.0060539236292243,\n        -0.008137175813317299,\n        -0.008879988454282284,\n        -0.0003192828444298357,\n        0.010568198747932911,\n        -0.0006221056682989001,\n        -0.012884424068033695,\n        -0.020717721432447433,\n        -0.0225409884005785,\n        0.017476357519626617,\n        0.007954848930239677,\n        -0.0030235853046178818,\n        -0.032791804522275925,\n        0.005861467681825161,\n        0.016652509570121765,\n        -0.0033105812035501003,\n        0.018961982801556587,\n        0.03357513248920441,\n        0.012506265193223953,\n        -0.002544133458286524,\n        -0.04046303406357765,\n        0.02020450495183468,\n        -0.004372465889900923,\n        0.0856260433793068,\n        0.002562703797593713,\n        -0.02279759757220745,\n        0.020650193095207214,\n        -0.01189175620675087,\n        0.019340142607688904,\n        -0.01734130084514618,\n        -0.010777536779642105,\n        0.02452632412314415,\n        -0.002885152120143175,\n        -0.025917410850524902,\n        -0.013492180034518242,\n        0.00851533468812704,\n        -0.004899187479168177,\n        0.025390688329935074,\n        0.017827505245804787,\n        -0.0032312353141605854,\n        0.001186812063679099,\n        0.010163028724491596,\n        -0.0230812169611454,\n        -0.0020072825718671083,\n        -0.009393204003572464,\n        0.016017742455005646,\n        -0.0025998444762080908,\n        -0.007144507486373186,\n        -0.013451662845909595,\n        -0.0039943065494298935,\n        0.00806964747607708,\n        0.022770585492253304,\n        -0.00948774442076683,\n        0.01075727865099907,\n        0.0030759198125451803,\n        -0.024013109505176544,\n        -0.0038457440678030252,\n        -0.015599066391587257,\n        0.011101673357188702,\n        0.011621642857789993,\n        -0.012445488944649696,\n        0.012870918028056622,\n        -0.021365994587540627,\n        0.015153379179537296,\n        0.03206249698996544,\n        0.013478673994541168,\n        -0.016720037907361984,\n        -0.0028193118050694466,\n        -0.013296347111463547,\n        0.001635876134969294,\n        0.005398897919803858,\n        -0.009703835472464561,\n        -0.0028125590179115534,\n        0.011297506280243397,\n        -0.005935749039053917,\n        -0.030603883787989616,\n        0.009629554115235806,\n        0.014802231453359127,\n        0.013647494837641716,\n        -0.01339764054864645,\n        -0.002650490729138255,\n        -0.0028935931622982025,\n        -0.022743575274944305,\n        -0.03376421332359314,\n        -0.009656565263867378,\n        -0.007164766080677509,\n        -0.0011716182343661785,\n        0.0035925123374909163,\n        -0.03389926999807358,\n        -0.006462470628321171,\n        -0.018921464681625366,\n        -0.005827703513205051,\n        0.009595789946615696,\n        0.025390688329935074,\n        -0.02452632412314415,\n        -0.04448772966861725,\n        -0.004747248254716396,\n        0.026822291314601898,\n        -0.0008090749615803361,\n        -0.005034244153648615,\n        0.0011783710215240717,\n        -0.024688392877578735,\n        0.011358281597495079,\n        -0.009778115898370743,\n        -0.02566080167889595,\n        -0.018597329035401344,\n        -0.009055562317371368,\n        -0.011648654006421566,\n        0.005040997173637152,\n        -0.0031890301033854485,\n        0.015761135146021843,\n        -0.0031569539569318295,\n        0.011594630777835846,\n        -0.005294228903949261,\n        0.01893497072160244,\n        0.04562220722436905,\n        -0.03614121302962303,\n        0.007792780641466379,\n        0.017462851479649544,\n        -0.003997683059424162,\n        0.0067798541858792305,\n        0.009987454861402512,\n        -0.0225139781832695,\n        -0.0092716533690691,\n        -0.026768269017338753,\n        -0.016436418518424034,\n        0.012587298639118671,\n        0.011398798786103725,\n        0.016328373923897743,\n        0.00027391218463890254,\n        0.029280327260494232,\n        0.018097618594765663,\n        -0.020933812484145164,\n        -0.006695443764328957,\n        -0.04184061288833618,\n        -0.009048809297382832,\n        0.004129363223910332,\n        0.003528360277414322,\n        0.002262202324345708,\n        0.0159637201577425,\n        0.018880948424339294,\n        -0.0005047749727964401,\n        -0.005037620663642883,\n        -0.017084691673517227,\n        -0.03808603435754776,\n        0.032089509069919586,\n        0.022527484223246574,\n        -0.003980800975114107,\n        -0.022784091532230377,\n        0.004348830785602331,\n        -0.005885102320462465,\n        -0.011142190545797348,\n        0.015504526905715466,\n        -0.013809563592076302,\n        0.004048329312354326,\n        -0.02067720517516136,\n        -0.04297509044408798,\n        -0.020272033289074898,\n        -0.000584120920393616,\n        -0.022149324417114258,\n        0.020906800404191017,\n        -0.017125209793448448,\n        0.00536175724118948,\n        -0.0036296530161052942,\n        -0.019907381385564804,\n        -0.011331270448863506,\n        -0.01546400971710682,\n        0.04640553519129753,\n        -0.030468827113509178,\n        -0.027848724275827408,\n        0.016720037907361984,\n        -0.023000182583928108,\n        0.028875155374407768,\n        -0.014586140401661396,\n        -0.002562703797593713,\n        -0.03692454472184181,\n        0.003670169971883297,\n        0.0022588258143514395,\n        -0.015545044094324112,\n        -0.019421175122261047,\n        -0.022216852754354477,\n        0.03670845180749893,\n        0.010169780813157558,\n        0.012202386744320393,\n        -0.004801271017640829,\n        0.003869378939270973,\n        0.011155696585774422,\n        -0.006401694845408201,\n        -0.0010661050910130143,\n        -0.004784388933330774,\n        -0.014005395583808422,\n        -0.0001811661059036851,\n        0.0048687998205423355,\n        0.026038961485028267,\n        -0.016368890181183815,\n        0.02356742136180401,\n        0.017989574000239372,\n        0.025228621438145638,\n        0.02162260189652443,\n        0.0005600639269687235,\n        -0.009946937672793865,\n        -0.02618752419948578,\n        -0.013897350057959557,\n        -0.009690329432487488,\n        0.009845645166933537,\n        0.003646535100415349,\n        -0.003062414238229394,\n        -0.024999024346470833,\n        0.001968453638255596,\n        0.0153964813798666,\n        0.006638044491410255,\n        -0.006604280322790146,\n        0.02349989302456379,\n        0.011885003186762333,\n        -0.018070606514811516,\n        -0.0006478508585132658,\n        0.007353845983743668,\n        0.045325081795454025,\n        -0.0020883167162537575,\n        -0.011756699532270432,\n        0.004082093480974436,\n        0.002177791902795434,\n        0.01245899498462677,\n        0.028226882219314575,\n        -0.0009521508472971618,\n        -0.015153379179537296,\n        -0.0033291515428572893,\n        -0.024202188476920128,\n        0.012384713627398014,\n        -0.0008930634357966483,\n        -0.012891177088022232,\n        0.002405700273811817,\n        -0.004710108041763306,\n        -0.025228621438145638,\n        -0.008224962279200554,\n        -0.02502603456377983,\n        -0.009413463063538074,\n        0.011776957660913467,\n        0.0009504626505076885,\n        -0.005760174710303545,\n        0.007684735115617514,\n        -0.015707112848758698,\n        -0.017395323142409325,\n        0.012627815827727318,\n        -0.003528360277414322,\n        0.03443949669599533,\n        -0.002047799527645111,\n        0.02035306766629219,\n        -0.002650490729138255,\n        0.023243285715579987,\n        -0.030873997136950493,\n        0.009670071303844452,\n        0.0006254820618778467,\n        -0.0008719608304090798,\n        0.01688210666179657,\n        -0.010588457807898521,\n        0.0068541355431079865,\n        -0.01546400971710682,\n        0.011331270448863506,\n        0.02056915871798992,\n        -0.007272811606526375,\n        -0.023905063048005104,\n        0.008170939981937408,\n        0.012114600278437138,\n        -0.006340919528156519,\n        -0.009426968172192574,\n        -0.019299624487757683,\n        0.005405650474131107,\n        0.03965269401669502,\n        0.001683990121819079,\n        0.007090484723448753,\n        -0.005631871055811644,\n        -0.01819215901196003,\n        -0.014856253750622272,\n        0.009838892146945,\n        -0.013208560645580292,\n        0.014559129253029823,\n        0.020663699135184288,\n        -0.017152220010757446,\n        -0.009980701841413975,\n        0.023905063048005104,\n        0.00895426981151104,\n        0.005885102320462465,\n        -0.010244062170386314,\n        -0.005341498646885157,\n        -0.04086820408701897,\n        0.01572061888873577,\n        -0.01038587186485529,\n        -0.011425809934735298,\n        0.013525944203138351,\n        0.005638623610138893,\n        0.00803588330745697,\n        0.028469985350966454,\n        -0.028794120997190475,\n        -0.03687052056193352,\n        0.011939026415348053,\n        -0.006982439663261175,\n        0.022838113829493523,\n        0.01038587186485529,\n        0.009690329432487488,\n        -0.0046256971545517445,\n        -0.009028551168739796,\n        -0.042380839586257935,\n        0.010392624884843826,\n        0.010554693639278412,\n        -0.010615468956530094,\n        -0.011067909188568592,\n        0.0087044145911932,\n        0.011338023468852043,\n        -0.023905063048005104,\n        0.007022956386208534,\n        0.0050680083222687244,\n        0.010264321230351925,\n        -0.03927453234791756,\n        0.004186762496829033,\n        -0.0002825642586685717,\n        0.029928598552942276,\n        0.0072120362892746925,\n        0.006479352712631226,\n        0.002410764805972576,\n        0.014437577687203884,\n        -0.029577450826764107,\n        0.001482249004766345,\n        -0.0041833859868347645,\n        0.011682418175041676,\n        0.008414042182266712,\n        0.016544464975595474,\n        -0.0023212896194308996,\n        -0.03673546388745308,\n        0.02216283045709133,\n        -0.013519191183149815,\n        -0.020380079746246338,\n        -0.013586719520390034,\n        0.02223035879433155,\n        0.008204704150557518,\n        0.011810721829533577,\n        -0.005979642271995544,\n        -0.02183869294822216,\n        0.009426968172192574,\n        0.009832139126956463,\n        0.002562703797593713,\n        -0.0005908737657591701,\n        -0.007056720554828644,\n        -0.013843327760696411,\n        -0.00681361835449934,\n        0.03862626105546951,\n        -0.024026615545153618,\n        0.0035992651246488094,\n        -0.0015751005848869681,\n        -0.005898608360439539,\n        -0.0014273821143433452,\n        -0.0307929627597332,\n        -0.003504725405946374,\n        -0.03230559825897217,\n        0.017638426274061203,\n        -0.0020950695034116507,\n        -0.010649233125150204,\n        0.03000963293015957,\n        -0.00400781212374568,\n        -0.006232874002307653,\n        0.027659643441438675,\n        0.019556231796741486,\n        -0.01035210769623518,\n        -0.017287276685237885,\n        0.013586719520390034,\n        0.01563958451151848,\n        0.01992088556289673,\n        0.007022956386208534,\n        0.014802231453359127,\n        0.011473080143332481,\n        -0.01093960553407669,\n        -0.011000380851328373,\n        -0.03635730594396591,\n        -0.016733543947339058,\n        0.012904682196676731,\n        0.025188103318214417,\n        -0.00637130718678236,\n        -0.02505304664373398,\n        -0.006648173555731773,\n        -0.012303679250180721,\n        -0.008366771973669529,\n        -0.012621062807738781,\n        -0.0029476159252226353,\n        0.01132451742887497,\n        0.011405551806092262,\n        -0.003866002429276705,\n        0.0021541567984968424,\n        -0.01447809487581253,\n        -0.011560866609215736,\n        -0.01476171426475048,\n        0.010196792893111706,\n        -0.01830020360648632,\n        -0.013600225560367107,\n        0.014788725413382053,\n        -0.00618560379371047,\n        -0.03009066730737686,\n        -0.021393006667494774,\n        -0.008839471265673637,\n        -0.023418858647346497,\n        -0.02439126744866371,\n        0.006921663880348206,\n        -0.004537910223007202,\n        -0.003069167025387287,\n        -0.012431983835995197,\n        0.03497972711920738,\n        0.00257114483974874,\n        0.02343236468732357,\n        0.007340339943766594,\n        -0.0170981977134943,\n        -0.006921663880348206,\n        0.0005136380787007511,\n        -0.007853556424379349,\n        -0.020758239552378654,\n        0.0472969114780426,\n        0.01646343059837818,\n        -0.01834072172641754,\n        0.024796439334750175,\n        -0.020690711215138435,\n        -0.0104061309248209,\n        0.020839272066950798,\n        0.007205283269286156,\n        -0.0015995795838534832,\n        0.028307916596531868,\n        -0.007954848930239677,\n        -0.0005482464330270886,\n        0.0008061206317506731,\n        -0.007813039235770702,\n        -0.027159933000802994,\n        -0.002035982208326459,\n        -0.0029357983730733395,\n        -0.011939026415348053,\n        0.009859150275588036,\n        -0.012951952405273914,\n        -0.00621261540800333,\n        0.012641321867704391,\n        -0.015247918665409088,\n        -0.013883844949305058,\n        0.01286416593939066,\n        -0.004331948701292276,\n        0.004152998328208923,\n        -0.0404900461435318,\n        0.006243003066629171,\n        -0.0006841474096290767,\n        -0.02895618975162506,\n        0.014991311356425285,\n        -0.01886744238436222,\n        -0.013688012026250362,\n        -0.00980512797832489,\n        -0.0017110015032812953,\n        -0.0030218970496207476,\n        -0.019907381385564804,\n        -0.003005014965310693,\n        0.030765952542424202,\n        0.007914331741631031,\n        0.002878399332985282,\n        -0.008562604896724224,\n        0.017017163336277008,\n        -0.011743193492293358,\n        0.003190718125551939,\n        0.22646333277225494,\n        -0.011216471903026104,\n        0.006151839625090361,\n        0.009001539088785648,\n        -0.010899088345468044,\n        0.005429285578429699,\n        0.015328953042626381,\n        -0.00955527275800705,\n        0.00781979225575924,\n        0.020218010991811752,\n        0.007542925421148539,\n        -0.0182326752692461,\n        -0.03217054158449173,\n        0.01242523081600666,\n        0.0015075721312314272,\n        0.006320660933852196,\n        -0.029334349557757378,\n        -0.03281881660223007,\n        -0.002746718702837825,\n        -0.035763055086135864,\n        0.013141032308340073,\n        -0.003474337514489889,\n        -0.02059617079794407,\n        -0.02109588123857975,\n        0.04300210252404213,\n        0.009224383160471916,\n        0.004237408749759197,\n        -0.005550836678594351,\n        0.023378342390060425,\n        0.003764709923416376,\n        -0.02020450495183468,\n        0.005577848292887211,\n        -0.0034338205587118864,\n        0.004568298347294331,\n        -0.0202450230717659,\n        0.006631291471421719,\n        -0.007988613098859787,\n        -0.0082654794678092,\n        0.021109387278556824,\n        0.007596948184072971,\n        0.0003009235661011189,\n        0.013249077834188938,\n        -0.016220327466726303,\n        0.008643638342618942,\n        0.013019480742514133,\n        0.018178652971982956,\n        -0.0002962809812743217,\n        -0.03114411048591137,\n        -0.0038086033891886473,\n        0.025525745004415512,\n        -0.019529221579432487,\n        0.00334434537217021,\n        0.002144027501344681,\n        0.037815921008586884,\n        -0.012884424068033695,\n        0.01805710233747959,\n        -0.00022326585894916207,\n        0.0004566609859466553,\n        -0.0013632301706820726,\n        0.031063076108694077,\n        0.006367930676788092,\n        0.00592899601906538,\n        -0.006914910860359669,\n        0.04845840111374855,\n        0.008292490616440773,\n        0.03408835083246231,\n        -0.016260845586657524,\n        0.015409987419843674,\n        0.008292490616440773,\n        -0.03581707924604416,\n        -0.003082672832533717,\n        -0.026822291314601898,\n        -0.0005174365942366421,\n        -0.01453211810439825,\n        -0.018070606514811516,\n        -0.007907578721642494,\n        0.013127526268362999,\n        0.008177693001925945,\n        0.02048812434077263,\n        0.016193317249417305,\n        -0.022676046937704086,\n        -0.03203548491001129,\n        -0.012479253113269806,\n        -0.005344875156879425,\n        0.002648802474141121,\n        -0.023418858647346497,\n        -0.0076644765213131905,\n        -0.0028328176122158766,\n        -0.006681937724351883,\n        -0.008690908551216125,\n        0.025755342096090317,\n        -0.024688392877578735,\n        -0.023972591385245323,\n        -0.010095500387251377,\n        0.008137175813317299,\n        -0.016557971015572548,\n        0.017044175416231155,\n        0.014869759790599346,\n        0.0068541355431079865,\n        0.013012727722525597,\n        0.00861662719398737,\n        0.02441827952861786,\n        0.012344196438789368,\n        0.011351528577506542,\n        -0.01985335722565651,\n        -0.00024162515182979405,\n        0.0002707467938307673,\n        0.023878052830696106,\n        0.0026910079177469015,\n        -0.01692262478172779,\n        0.005000479985028505,\n        -0.03225157782435417,\n        0.0023938827216625214,\n        -0.01985335722565651,\n        0.010264321230351925,\n        -0.021176915615797043,\n        -0.022149324417114258,\n        0.0007200218387879431,\n        0.01751687377691269,\n        -0.02356742136180401,\n        -0.004882305394858122,\n        -0.025255631655454636,\n        0.008360018953680992,\n        0.02039358578622341,\n        0.004943080712109804,\n        -0.024999024346470833,\n        -0.006050547119230032,\n        -0.007299823220819235,\n        0.013168043456971645,\n        -0.004642579238861799,\n        0.01801658421754837,\n        -0.03606018051505089,\n        0.011966037563979626,\n        -0.000476497458294034,\n        -0.0037376985419541597,\n        -0.010682997293770313,\n        0.018138134852051735,\n        -0.011034145019948483,\n        -0.00955527275800705,\n        0.01060196291655302,\n        -0.0024985517375171185,\n        -0.019772322848439217,\n        0.006800112780183554,\n        0.007644217927008867,\n        0.0006339231040328741,\n        -0.016841590404510498,\n        0.009994206950068474,\n        0.001396150211803615,\n        -0.01748986355960369,\n        -0.007016203831881285,\n        -0.013174796476960182,\n        -0.005797315388917923,\n        -0.000915854296181351,\n        -0.012195633724331856,\n        0.0032329235691577196,\n        0.002496863715350628,\n        -0.0006795048248022795,\n        -0.018097618594765663,\n        0.02092030644416809,\n        -0.0033156457357108593,\n        -0.026038961485028267,\n        -0.014491600915789604,\n        0.010865324176847935,\n        0.00835326686501503,\n        -0.013735282234847546,\n        0.015072344802320004,\n        -0.17384518682956696,\n        0.004551416262984276,\n        0.019529221579432487,\n        -0.01777348294854164,\n        0.01758440211415291,\n        -0.021298466250300407,\n        -0.003585759550333023,\n        -0.0009783180430531502,\n        -0.02159559167921543,\n        0.02354040928184986,\n        0.04232681915163994,\n        -0.03114411048591137,\n        -0.021987255662679672,\n        -0.024593854323029518,\n        -0.0013269336195662618,\n        0.004180009476840496,\n        0.007873814553022385,\n        -0.008751683868467808,\n        0.014896770939230919,\n        0.02240593172609806,\n        0.020380079746246338,\n        -0.002083251951262355,\n        0.005000479985028505,\n        0.0038457440678030252,\n        -0.0040044356137514114,\n        -0.002125457162037492,\n        -0.016841590404510498,\n        0.037572816014289856,\n        0.007718499284237623,\n        -0.009636306203901768,\n        0.013525944203138351,\n        0.0022588258143514395,\n        0.013262582942843437,\n        0.016341879963874817,\n        -0.004564921837300062,\n        0.010325096547603607,\n        -0.007650970946997404,\n        -0.029010212048888206,\n        -0.021298466250300407,\n        0.015950214117765427,\n        0.02655217796564102,\n        0.017287276685237885,\n        0.011020638979971409,\n        -0.00978486891835928,\n        -0.01939416490495205,\n        -0.00444337073713541,\n        0.021433522924780846,\n        -0.004791141953319311,\n        0.022878631949424744,\n        -0.04427163675427437,\n        0.01646343059837818,\n        0.010581704787909985,\n        -0.002368559595197439,\n        -0.018462272360920906,\n        0.0182326752692461,\n        0.003511478193104267,\n        0.00946748536080122,\n        0.023594433441758156,\n        -0.014275509864091873,\n        0.0014383555389940739,\n        -0.0005305202212184668,\n        -0.01511286199092865,\n        0.029469406232237816,\n        -0.01343140471726656,\n        0.017395323142409325,\n        -0.003045532153919339,\n        0.01447809487581253,\n        0.00055964186321944,\n        -0.012330691330134869,\n        0.006192356813699007,\n        -0.017476357519626617,\n        -0.006533375475555658,\n        -0.009575530886650085,\n        -0.026228042319417,\n        -0.018840432167053223,\n        0.007954848930239677,\n        -0.027267979457974434,\n        -0.003572253743186593,\n        -0.020218010991811752,\n        0.009231136180460453,\n        -0.004078716970980167,\n        0.03795097768306732,\n        -0.01624733954668045,\n        0.009143348783254623,\n        -0.010608715936541557,\n        -0.0031029311940073967,\n        0.009028551168739796,\n        0.01620682328939438,\n        0.009845645166933537,\n        -0.02749757654964924,\n        0.023986097425222397,\n        -0.03011767938733101,\n        -0.005040997173637152,\n        -0.01504533365368843,\n        0.030522849410772324,\n        0.01447809487581253,\n        0.015301941893994808,\n        0.005587977357208729,\n        -0.0028615170158445835,\n        -0.0005701931659132242,\n        -0.0092716533690691,\n        -0.01659848727285862,\n        0.002939174883067608,\n        -0.008981280960142612,\n        -0.013485427014529705,\n        0.012479253113269806,\n        0.023918569087982178,\n        -0.006209238898009062,\n        0.04005786404013634,\n        -0.0005959383561275899,\n        -0.02046111412346363,\n        0.0011024016421288252,\n        0.022432943806052208,\n        0.01635538600385189,\n        0.0024732286110520363,\n        0.02806481532752514,\n        0.011081415228545666,\n        -0.00823171529918909,\n        0.010959863662719727,\n        0.01545050460845232,\n        0.07476747781038284,\n        0.014086429961025715,\n        -0.009933431632816792,\n        0.0028885283973068,\n        -0.007461891509592533,\n        -0.03795097768306732,\n        -0.09972598403692245,\n        -0.021393006667494774,\n        0.013039739802479744,\n        0.006621162407100201,\n        0.015572055242955685,\n        -0.001142918597906828,\n        0.013586719520390034,\n        -0.010196792893111706,\n        0.013417898677289486,\n        0.021136397495865822,\n        -0.018219169229269028,\n        -0.017638426274061203,\n        -0.015288435854017735,\n        -0.03217054158449173,\n        -0.015436998568475246,\n        -0.007542925421148539,\n        -0.003953789360821247,\n        -0.019475199282169342,\n        -0.0048789288848638535,\n        0.009190618991851807,\n        -0.004926198627799749,\n        -0.012634568847715855,\n        -0.01613929495215416,\n        -0.011405551806092262,\n        -0.01459964644163847,\n        -0.02166312001645565,\n        -0.014005395583808422,\n        0.008873235434293747,\n        0.008724672719836235,\n        0.006455717608332634,\n        -0.0165309589356184,\n        -0.0279297586530447,\n        0.011148943565785885,\n        0.0030404673889279366,\n        0.016260845586657524,\n        0.01666601561009884,\n        -0.03954464569687843,\n        -0.0018435260280966759,\n        0.00558460084721446,\n        -0.010163028724491596,\n        0.0061585926450788975,\n        0.021987255662679672,\n        0.009008292108774185,\n        -0.009177112951874733,\n        0.030063655227422714,\n        -0.01819215901196003,\n        -0.019664278253912926,\n        0.013492180034518242,\n        0.005696022883057594,\n        0.01762492023408413,\n        -0.011013886891305447,\n        -0.002336483681574464,\n        -0.024958506226539612,\n        -0.01180396880954504,\n        0.015193896368145943,\n        -0.018772903829813004,\n        -0.004163127392530441,\n        -0.017192738130688667,\n        -0.008042635396122932,\n        -0.002027540933340788,\n        0.00580406840890646,\n        -0.002078187419101596,\n        -0.01847577840089798,\n        0.03344007581472397,\n        0.03676247596740723,\n        0.0013092074077576399,\n        0.0012180439662188292,\n        -0.008792201057076454,\n        0.02325678989291191,\n        -0.005628494545817375,\n        -0.023580927401781082,\n        0.02467488683760166,\n        0.006226120982319117,\n        -0.010912594385445118,\n        -0.023770006373524666,\n        0.0002443685079924762,\n        -0.015990732237696648,\n        0.015085850842297077,\n        -0.01897548884153366,\n        -0.011020638979971409,\n        -0.036789488047361374,\n        -0.009933431632816792,\n        -0.007826544344425201,\n        -0.015315447002649307,\n        -0.0008293334976769984,\n        -0.012783131562173367,\n        -0.03271077200770378,\n        -0.042029693722724915,\n        0.012330691330134869,\n        -0.02930733747780323,\n        -0.012661579996347427,\n        0.026038961485028267,\n        0.03606018051505089,\n        0.013249077834188938,\n        -0.010750525631010532,\n        0.015004816465079784,\n        -0.0024749168660491705,\n        -0.018664857372641563,\n        2.4663702788529918e-05,\n        0.010392624884843826,\n        0.014275509864091873,\n        -0.003653287887573242,\n        -0.03522282838821411,\n        -0.005581224337220192,\n        0.006546881049871445,\n        -0.003381486050784588,\n        -0.014221486635506153,\n        -0.013066750951111317,\n        0.006891276221722364,\n        -0.005915490444749594,\n        -0.015693606808781624,\n        -0.0026791903655976057,\n        -0.007239047437906265,\n        0.006837253458797932,\n        0.008947516791522503,\n        0.009345934726297855,\n        -0.014207981526851654,\n        -0.013444909825921059,\n        0.025809364393353462,\n        0.0012138234451413155,\n        -0.003781592007726431,\n        0.007468644063919783,\n        -0.0076644765213131905,\n        -0.0014408878050744534,\n        0.014748208224773407,\n        0.0005887635052204132,\n        -0.0048316591419279575,\n        -0.01208758819848299,\n        0.008684155531227589,\n        -0.00230103125795722,\n        -0.028469985350966454,\n        -0.011594630777835846,\n        0.019556231796741486,\n        -0.00863688625395298,\n        0.004618944600224495,\n        0.0279567688703537,\n        -0.002799053443595767,\n        -0.012296927161514759,\n        -0.0012433672090992332,\n        0.019839851185679436,\n        0.011783710680902004,\n        -0.005625118035823107,\n        -0.015261424705386162,\n        -0.028713086619973183,\n        0.004186762496829033,\n        -0.02644413150846958,\n        0.013444909825921059,\n        -0.0031198132783174515,\n        -0.025188103318214417,\n        -0.00018749690207187086,\n        0.02718694508075714,\n        -0.013870338909327984,\n        0.0148292426019907,\n        0.014842748641967773,\n        0.0004252180806361139,\n        0.006050547119230032,\n        -0.008738178759813309,\n        -0.02884814515709877,\n        0.007293070200830698,\n        0.008170939981937408,\n        0.008812460117042065,\n        -0.016017742455005646,\n        0.03298088535666466,\n        0.012634568847715855,\n        -0.00032899004872888327,\n        -0.0006440524011850357,\n        0.03352111205458641,\n        0.0020680581219494343,\n        -0.013262582942843437,\n        0.005554213188588619,\n        -0.002520498586818576,\n        -0.03238663449883461,\n        -0.0017321042250841856,\n        -0.011216471903026104,\n        0.009562025777995586,\n        -0.0026049090083688498,\n        0.04267796501517296,\n        0.0037174399476498365,\n        0.02039358578622341,\n        0.004679719917476177,\n        -0.028767110779881477,\n        0.026781775057315826,\n        0.009190618991851807,\n        -0.020717721432447433,\n        -0.01563958451151848,\n        0.009285158477723598,\n        0.04705381020903587,\n        0.002957745222374797,\n        -0.018840432167053223,\n        0.017678942531347275,\n        -0.004409606568515301,\n        0.00017958341049961746,\n        0.008785448037087917,\n        0.008684155531227589,\n        0.011939026415348053,\n        -0.007495655678212643,\n        0.008758436888456345,\n        0.013357123360037804,\n        -0.0028682700358331203,\n        0.013600225560367107,\n        0.01812463067471981,\n        0.01072351448237896,\n        0.012310432270169258,\n        0.0028699582908302546,\n        -0.01950220949947834,\n        -0.0014839372597634792,\n        -0.01286416593939066,\n        -0.0007060941425152123,\n        -0.008360018953680992,\n        -0.02208179607987404,\n        -0.010041477158665657,\n        0.022838113829493523,\n        0.023175757378339767,\n        0.00923788920044899,\n        0.0267952810972929,\n        0.01242523081600666,\n        -0.027200451120734215,\n        0.011128684505820274,\n        0.010966616682708263,\n        -0.0065164933912456036,\n        -0.014842748641967773,\n        0.007286317180842161,\n        -0.002145715756341815,\n        -0.0007896605529822409,\n        0.04324520379304886,\n        -0.016125788912177086,\n        0.02133898250758648,\n        -0.006249756086617708,\n        0.007752263452857733,\n        -0.033034905791282654,\n        0.008042635396122932,\n        0.009305417537689209,\n        0.01600423827767372,\n        -0.010568198747932911,\n        -0.0014366672839969397,\n        -0.0142484987154603,\n        0.008056141436100006,\n        0.000470588740427047,\n        0.0022115560714155436,\n        0.05064631998538971,\n        -0.02244644984602928,\n        0.03338605538010597,\n        0.0041462453082203865,\n        0.0004359804152045399,\n        -0.027848724275827408,\n        0.007455138489603996,\n        0.01488326583057642,\n        0.011662159115076065,\n        -0.001961700851097703,\n        -0.00851533468812704,\n        0.0005845429259352386,\n        0.00053220841800794,\n        0.007083732169121504,\n        0.016625499352812767,\n        -0.0165309589356184,\n        -0.019299624487757683,\n        0.02399960346519947,\n        -0.01129075326025486,\n        0.018003078177571297,\n        -0.002427646890282631,\n        -0.005922242999076843,\n        0.02828090637922287,\n        -0.007117496337741613,\n        0.009575530886650085,\n        -0.01006848830729723,\n        -0.016557971015572548,\n        -0.01585567556321621,\n        0.011533855460584164,\n        -0.014113441109657288,\n        -0.020731227472424507,\n        -0.04116532951593399,\n        0.03522282838821411,\n        0.0038761317264288664,\n        -0.022568000480532646,\n        -0.0033899270929396152,\n        -0.0046999785117805,\n        -0.022554494440555573,\n        0.0003549462999217212,\n        0.003869378939270973,\n        0.019907381385564804,\n        0.029361359775066376,\n        0.003609394421800971,\n        0.03090100921690464,\n        -0.01488326583057642,\n        -0.03430444002151489,\n        -0.010608715936541557,\n        -0.0013564772671088576,\n        -0.007529419846832752,\n        -0.014180969446897507,\n        -0.020690711215138435\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"ada\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 7,\n    \"total_tokens\": 7\n  }\n}","attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.embeddings.Embeddings.create","node_name":"vector_lookup","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"input\": [\n    [\n      3923,\n      374,\n      279,\n      7438,\n      315,\n      63120,\n      30\n    ]\n  ],\n  \"model\": \"text-embedding-ada-002\"\n}","output":"{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.03181939572095871,\n        0.0023466129787266254,\n        0.03857223689556122,\n        -0.014059418812394142,\n        -0.005179430358111858,\n        0.014072923921048641,\n        -0.027767689898610115,\n        -0.017179232090711594,\n        -0.01000095997005701,\n        -0.023594433441758156,\n        0.0336291566491127,\n        0.020055942237377167,\n        -0.013937867246568203,\n        0.006378060206770897,\n        0.014329532161355019,\n        -0.013451662845909595,\n        0.043164171278476715,\n        0.010541187599301338,\n        -0.01359347254037857,\n        -0.030036645010113716,\n        -0.02895618975162506,\n        -7.66025623306632e-05,\n        0.0011108426842838526,\n        0.02378351241350174,\n        -0.007610453758388758,\n        0.011182707734405994,\n        0.011094920337200165,\n        -0.024728910997509956,\n        -0.006685314234346151,\n        -0.038842350244522095,\n        0.023418858647346497,\n        0.004946457222104073,\n        0.006573892664164305,\n        -0.020380079746246338,\n        -0.017003657296299934,\n        -0.004686472937464714,\n        0.004949833732098341,\n        -0.028605042025446892,\n        0.020326057448983192,\n        -0.00028868403751403093,\n        0.02526913769543171,\n        0.024013109505176544,\n        -0.0022486967500299215,\n        0.00564537663012743,\n        0.0006626226822845638,\n        0.006668432150036097,\n        0.008089905604720116,\n        -0.028361938893795013,\n        -0.010784289799630642,\n        0.014653668738901615,\n        0.028794120997190475,\n        0.03271077200770378,\n        -0.029847566038370132,\n        -0.02616051211953163,\n        0.0024951754603534937,\n        -0.004916069563478231,\n        -0.004412982612848282,\n        0.0013429715763777494,\n        0.013742035254836082,\n        0.0015455569373443723,\n        -0.0005807444686070085,\n        -0.013120773248374462,\n        -0.010547940619289875,\n        0.01214161142706871,\n        -0.0017489863093942404,\n        0.01996140368282795,\n        -0.00020986569870729,\n        0.019515715539455414,\n        -0.010784289799630642,\n        -0.007610453758388758,\n        0.020366573706269264,\n        0.022595012560486794,\n        0.025606779381632805,\n        -0.01546400971710682,\n        0.026065973564982414,\n        -0.01551803294569254,\n        -0.020434102043509483,\n        -0.008501828648149967,\n        0.010932852514088154,\n        0.026808785274624825,\n        0.005871596746146679,\n        -0.0004243739531375468,\n        -0.0043994770385324955,\n        0.03319697454571724,\n        -0.016760556027293205,\n        0.0012771313777193427,\n        0.007765769027173519,\n        0.002853075973689556,\n        -0.000697653042152524,\n        -0.004453499801456928,\n        -0.012803389690816402,\n        -0.030036645010113716,\n        0.02590390481054783,\n        0.008792201057076454,\n        0.0159637201577425,\n        0.025539251044392586,\n        -0.0046898494474589825,\n        0.0365193746984005,\n        -0.02611999586224556,\n        -0.022203346714377403,\n        0.0023753123823553324,\n        0.01688210666179657,\n        -0.032224565744400024,\n        -0.0002006860449910164,\n        0.007286317180842161,\n        -0.0017059368547052145,\n        -0.019178073853254318,\n        -0.0076172067783772945,\n        0.002799053443595767,\n        -0.01794905588030815,\n        -0.03100905381143093,\n        0.02657919004559517,\n        -0.008218209259212017,\n        -0.04481186345219612,\n        0.003481090534478426,\n        0.015153379179537296,\n        -0.004122610669583082,\n        -0.0030792963225394487,\n        -0.03446650877594948,\n        -0.018786408007144928,\n        0.0302257239818573,\n        0.007644217927008867,\n        0.018529800698161125,\n        -0.0014062795089557767,\n        -0.003997683059424162,\n        0.018988993018865585,\n        -0.008771942928433418,\n        -0.01928611844778061,\n        -0.016341879963874817,\n        -0.013053244911134243,\n        0.006364554166793823,\n        0.030873997136950493,\n        0.0016147735295817256,\n        0.013087009079754353,\n        -0.0053887683898210526,\n        0.01751687377691269,\n        0.004666214343160391,\n        -0.0027146427892148495,\n        -0.05175378546118736,\n        -0.019137555733323097,\n        0.0069689336232841015,\n        0.024161672219634056,\n        -0.0064523410983383656,\n        -0.0075901951640844345,\n        0.01862434111535549,\n        0.030522849410772324,\n        0.02611999586224556,\n        0.012202386744320393,\n        0.0072593060322105885,\n        0.010271074250340462,\n        0.03092801943421364,\n        0.006749466527253389,\n        0.01808411255478859,\n        -0.002189609222114086,\n        0.0005111058126203716,\n        0.017935549840331078,\n        -0.007144507486373186,\n        0.0018975487910211086,\n        0.011500091291964054,\n        -0.025606779381632805,\n        0.033467087894678116,\n        -0.009649812243878841,\n        0.010041477158665657,\n        -0.026565684005618095,\n        0.02686280943453312,\n        0.045081976801157,\n        0.0035519953817129135,\n        0.01152034942060709,\n        -0.005162548273801804,\n        -0.002206491306424141,\n        -0.016112282872200012,\n        0.023702478036284447,\n        -0.03138721361756325,\n        0.017611414194107056,\n        -0.013829821720719337,\n        0.018165146932005882,\n        0.027659643441438675,\n        0.0012366143055260181,\n        -0.016085270792245865,\n        -0.00223856745287776,\n        -0.016868600621819496,\n        -0.0010838313028216362,\n        0.01834072172641754,\n        0.010892335325479507,\n        -0.026957347989082336,\n        -0.002802429720759392,\n        0.010304838418960571,\n        -0.006236250512301922,\n        0.0056420001201331615,\n        -0.01177020464092493,\n        0.01546400971710682,\n        0.03430444002151489,\n        -0.004075340460985899,\n        -0.008258726447820663,\n        -0.6651279926300049,\n        -0.003524984000250697,\n        0.006502987816929817,\n        -0.005297604948282242,\n        -0.0007613830384798348,\n        0.004645955748856068,\n        0.01688210666179657,\n        0.011000380851328373,\n        -0.007536172401160002,\n        0.028226882219314575,\n        -0.01309376209974289,\n        -0.007718499284237623,\n        -0.009474238380789757,\n        -0.0015143250348046422,\n        -0.019191579893231392,\n        -0.017395323142409325,\n        -0.012384713627398014,\n        -0.016801072284579277,\n        -0.012816895730793476,\n        0.008906999602913857,\n        -0.03057687170803547,\n        0.022676046937704086,\n        -0.019340142607688904,\n        0.015234413556754589,\n        0.030522849410772324,\n        -0.012384713627398014,\n        -0.000990135595202446,\n        -0.025282643735408783,\n        -0.02229788713157177,\n        0.022135818377137184,\n        -0.0076982406899333,\n        -0.01122322492301464,\n        0.027051888406276703,\n        0.0230812169611454,\n        0.029469406232237816,\n        -0.020474620163440704,\n        -0.004902563989162445,\n        0.02718694508075714,\n        -0.04286704584956169,\n        0.03379122540354729,\n        0.008204704150557518,\n        -0.0033173339907079935,\n        0.021082375198602676,\n        -0.021501051262021065,\n        -0.0043522072955966,\n        -0.016409408301115036,\n        0.02474241517484188,\n        -0.0087044145911932,\n        0.0005136380787007511,\n        -0.028578029945492744,\n        -0.01245899498462677,\n        0.0074753970839083195,\n        -0.00895426981151104,\n        -0.005091643426567316,\n        0.006286896765232086,\n        0.002908786991611123,\n        0.01961025595664978,\n        0.0027095782570540905,\n        0.019205084070563316,\n        0.0023263543844223022,\n        -0.00943372119218111,\n        0.045811284333467484,\n        -0.00444337073713541,\n        -0.013742035254836082,\n        -0.02753809280693531,\n        0.003401744645088911,\n        0.016720037907361984,\n        0.03092801943421364,\n        0.01830020360648632,\n        -0.034601565450429916,\n        0.0030573494732379913,\n        0.029226303100585938,\n        -0.007549678441137075,\n        0.0075901951640844345,\n        0.016301361843943596,\n        0.003043843898922205,\n        -0.001223952742293477,\n        0.013816316612064838,\n        0.020163988694548607,\n        0.015126368030905724,\n        -0.015139873139560223,\n        -0.008785448037087917,\n        -0.0037782154977321625,\n        -0.020150482654571533,\n        0.02155507355928421,\n        0.007711746264249086,\n        -0.0025356924161314964,\n        -0.006908158306032419,\n        5.09364836034365e-05,\n        -0.011405551806092262,\n        -0.012040318921208382,\n        -0.0013387510553002357,\n        0.004966715816408396,\n        -0.041894637048244476,\n        0.013937867246568203,\n        0.01801658421754837,\n        -0.009001539088785648,\n        0.008994787000119686,\n        0.00886648241430521,\n        0.023702478036284447,\n        0.0038524968549609184,\n        -0.010858571156859398,\n        -0.02092030644416809,\n        0.002834505867213011,\n        0.005013985559344292,\n        -0.008218209259212017,\n        -0.018462272360920906,\n        0.0330619178712368,\n        0.04027395322918892,\n        -0.008744931779801846,\n        -0.0026420496869832277,\n        -0.005675764288753271,\n        0.008852977305650711,\n        0.009251394309103489,\n        -0.002478293376043439,\n        -0.02605246752500534,\n        0.02002893202006817,\n        0.0142484987154603,\n        0.008630133233964443,\n        -0.008251973427832127,\n        0.010541187599301338,\n        -0.011621642857789993,\n        0.01840825006365776,\n        -0.024728910997509956,\n        -0.010014466010034084,\n        -0.012296927161514759,\n        -0.0017979444237425923,\n        -0.008981280960142612,\n        -0.03387225791811943,\n        0.009447227232158184,\n        0.0176654364913702,\n        -0.0009572154376655817,\n        -0.004125986713916063,\n        -0.01274261437356472,\n        0.001882354961708188,\n        0.00665492657572031,\n        -0.019029511138796806,\n        -0.03854522481560707,\n        0.022635528817772865,\n        -0.03932855650782585,\n        -0.017935549840331078,\n        0.019137555733323097,\n        0.017854515463113785,\n        -0.01794905588030815,\n        -0.028226882219314575,\n        -0.020960824564099312,\n        -0.0015404922887682915,\n        0.001764180138707161,\n        -0.028902167454361916,\n        -0.0065637631341814995,\n        0.009636306203901768,\n        -0.01233744341880083,\n        -0.02087979018688202,\n        0.013289595022797585,\n        -0.0148157374933362,\n        -0.011466327123343945,\n        -0.016233833506703377,\n        -0.014937288127839565,\n        -0.023634949699044228,\n        -0.03160330280661583,\n        0.013141032308340073,\n        0.015369470231235027,\n        -0.021136397495865822,\n        -0.022905642166733742,\n        -0.024999024346470833,\n        -0.01688210666179657,\n        -0.012729108333587646,\n        0.03967970237135887,\n        -0.04575726389884949,\n        -0.025863388553261757,\n        0.023837534710764885,\n        -0.012317185290157795,\n        0.003997683059424162,\n        0.010210298001766205,\n        -0.0060843112878501415,\n        0.025863388553261757,\n        -0.017138715833425522,\n        -0.006320660933852196,\n        0.011993048712611198,\n        -0.006492858286947012,\n        0.015653088688850403,\n        0.0036060181446373463,\n        -0.02169013023376465,\n        0.0016586669953539968,\n        0.009291911497712135,\n        0.011446068063378334,\n        0.04491991177201271,\n        0.013316606171429157,\n        -0.0022942782379686832,\n        0.009913173504173756,\n        -0.004818153101950884,\n        0.0071850246749818325,\n        -0.018070606514811516,\n        0.01003472413867712,\n        -0.0009344246354885399,\n        0.0153964813798666,\n        0.015653088688850403,\n        0.0055001904256641865,\n        0.0030944901518523693,\n        0.03743775933980942,\n        0.029037224128842354,\n        0.00429143151268363,\n        0.01928611844778061,\n        -0.010500670410692692,\n        0.009609295055270195,\n        0.017017163336277008,\n        -0.0009310481837019324,\n        -0.025890398770570755,\n        0.008191198110580444,\n        -0.0011327894171699882,\n        0.005797315388917923,\n        -0.027740677818655968,\n        -0.009453980252146721,\n        -0.010730267502367496,\n        0.015693606808781624,\n        0.02286512590944767,\n        -0.0020815636962652206,\n        0.004818153101950884,\n        -0.034709613770246506,\n        0.017705954611301422,\n        0.0213795006275177,\n        -0.0056521291844546795,\n        -0.011412303894758224,\n        -0.0026369851548224688,\n        0.005837832577526569,\n        0.003325775032863021,\n        0.034034326672554016,\n        0.04654059186577797,\n        -0.009258147329092026,\n        -0.002203115029260516,\n        -0.018880948424339294,\n        0.024161672219634056,\n        0.014950794167816639,\n        0.013755540363490582,\n        -0.02013697661459446,\n        -0.0006900561274960637,\n        0.01609877683222294,\n        -0.0067528425715863705,\n        0.00794809591025114,\n        -0.025228621438145638,\n        0.007610453758388758,\n        0.01670653373003006,\n        -0.00128557241987437,\n        -0.014113441109657288,\n        0.016828084364533424,\n        0.002250384772196412,\n        0.03805902227759361,\n        0.01677406206727028,\n        0.009960442781448364,\n        -0.003947036806493998,\n        -0.0043994770385324955,\n        -0.008711167611181736,\n        -0.002922292798757553,\n        -0.0003201269428245723,\n        -0.01635538600385189,\n        0.0037241927348077297,\n        -0.0025930916890501976,\n        -0.013458415865898132,\n        0.011689171195030212,\n        0.03784292936325073,\n        0.01943468116223812,\n        0.022676046937704086,\n        0.02884814515709877,\n        -0.0017658683937042952,\n        -0.006334166508167982,\n        0.014464588835835457,\n        0.011736440472304821,\n        -0.004622321110218763,\n        -0.004787765443325043,\n        0.002299343002960086,\n        0.008535593748092651,\n        -0.014640163630247116,\n        0.0021372747141867876,\n        -0.014140453189611435,\n        0.01840825006365776,\n        0.009291911497712135,\n        -0.019772322848439217,\n        -0.0008521243580617011,\n        0.019704794511198997,\n        0.0174088291823864,\n        -0.006877770181745291,\n        -0.027011370286345482,\n        -0.006641421001404524,\n        0.0076982406899333,\n        -0.0059053609147667885,\n        -0.025998445227742195,\n        0.013505685143172741,\n        0.005375262815505266,\n        -0.016476936638355255,\n        0.0035823830403387547,\n        -0.009899667464196682,\n        -0.009339181706309319,\n        -0.006320660933852196,\n        -0.013228818774223328,\n        0.0018975487910211086,\n        -0.012101094238460064,\n        0.011304258368909359,\n        0.013742035254836082,\n        0.0142620038241148,\n        -0.03011767938733101,\n        -0.002380377147346735,\n        -0.0060539236292243,\n        -0.008137175813317299,\n        -0.008879988454282284,\n        -0.0003192828444298357,\n        0.010568198747932911,\n        -0.0006221056682989001,\n        -0.012884424068033695,\n        -0.020717721432447433,\n        -0.0225409884005785,\n        0.017476357519626617,\n        0.007954848930239677,\n        -0.0030235853046178818,\n        -0.032791804522275925,\n        0.005861467681825161,\n        0.016652509570121765,\n        -0.0033105812035501003,\n        0.018961982801556587,\n        0.03357513248920441,\n        0.012506265193223953,\n        -0.002544133458286524,\n        -0.04046303406357765,\n        0.02020450495183468,\n        -0.004372465889900923,\n        0.0856260433793068,\n        0.002562703797593713,\n        -0.02279759757220745,\n        0.020650193095207214,\n        -0.01189175620675087,\n        0.019340142607688904,\n        -0.01734130084514618,\n        -0.010777536779642105,\n        0.02452632412314415,\n        -0.002885152120143175,\n        -0.025917410850524902,\n        -0.013492180034518242,\n        0.00851533468812704,\n        -0.004899187479168177,\n        0.025390688329935074,\n        0.017827505245804787,\n        -0.0032312353141605854,\n        0.001186812063679099,\n        0.010163028724491596,\n        -0.0230812169611454,\n        -0.0020072825718671083,\n        -0.009393204003572464,\n        0.016017742455005646,\n        -0.0025998444762080908,\n        -0.007144507486373186,\n        -0.013451662845909595,\n        -0.0039943065494298935,\n        0.00806964747607708,\n        0.022770585492253304,\n        -0.00948774442076683,\n        0.01075727865099907,\n        0.0030759198125451803,\n        -0.024013109505176544,\n        -0.0038457440678030252,\n        -0.015599066391587257,\n        0.011101673357188702,\n        0.011621642857789993,\n        -0.012445488944649696,\n        0.012870918028056622,\n        -0.021365994587540627,\n        0.015153379179537296,\n        0.03206249698996544,\n        0.013478673994541168,\n        -0.016720037907361984,\n        -0.0028193118050694466,\n        -0.013296347111463547,\n        0.001635876134969294,\n        0.005398897919803858,\n        -0.009703835472464561,\n        -0.0028125590179115534,\n        0.011297506280243397,\n        -0.005935749039053917,\n        -0.030603883787989616,\n        0.009629554115235806,\n        0.014802231453359127,\n        0.013647494837641716,\n        -0.01339764054864645,\n        -0.002650490729138255,\n        -0.0028935931622982025,\n        -0.022743575274944305,\n        -0.03376421332359314,\n        -0.009656565263867378,\n        -0.007164766080677509,\n        -0.0011716182343661785,\n        0.0035925123374909163,\n        -0.03389926999807358,\n        -0.006462470628321171,\n        -0.018921464681625366,\n        -0.005827703513205051,\n        0.009595789946615696,\n        0.025390688329935074,\n        -0.02452632412314415,\n        -0.04448772966861725,\n        -0.004747248254716396,\n        0.026822291314601898,\n        -0.0008090749615803361,\n        -0.005034244153648615,\n        0.0011783710215240717,\n        -0.024688392877578735,\n        0.011358281597495079,\n        -0.009778115898370743,\n        -0.02566080167889595,\n        -0.018597329035401344,\n        -0.009055562317371368,\n        -0.011648654006421566,\n        0.005040997173637152,\n        -0.0031890301033854485,\n        0.015761135146021843,\n        -0.0031569539569318295,\n        0.011594630777835846,\n        -0.005294228903949261,\n        0.01893497072160244,\n        0.04562220722436905,\n        -0.03614121302962303,\n        0.007792780641466379,\n        0.017462851479649544,\n        -0.003997683059424162,\n        0.0067798541858792305,\n        0.009987454861402512,\n        -0.0225139781832695,\n        -0.0092716533690691,\n        -0.026768269017338753,\n        -0.016436418518424034,\n        0.012587298639118671,\n        0.011398798786103725,\n        0.016328373923897743,\n        0.00027391218463890254,\n        0.029280327260494232,\n        0.018097618594765663,\n        -0.020933812484145164,\n        -0.006695443764328957,\n        -0.04184061288833618,\n        -0.009048809297382832,\n        0.004129363223910332,\n        0.003528360277414322,\n        0.002262202324345708,\n        0.0159637201577425,\n        0.018880948424339294,\n        -0.0005047749727964401,\n        -0.005037620663642883,\n        -0.017084691673517227,\n        -0.03808603435754776,\n        0.032089509069919586,\n        0.022527484223246574,\n        -0.003980800975114107,\n        -0.022784091532230377,\n        0.004348830785602331,\n        -0.005885102320462465,\n        -0.011142190545797348,\n        0.015504526905715466,\n        -0.013809563592076302,\n        0.004048329312354326,\n        -0.02067720517516136,\n        -0.04297509044408798,\n        -0.020272033289074898,\n        -0.000584120920393616,\n        -0.022149324417114258,\n        0.020906800404191017,\n        -0.017125209793448448,\n        0.00536175724118948,\n        -0.0036296530161052942,\n        -0.019907381385564804,\n        -0.011331270448863506,\n        -0.01546400971710682,\n        0.04640553519129753,\n        -0.030468827113509178,\n        -0.027848724275827408,\n        0.016720037907361984,\n        -0.023000182583928108,\n        0.028875155374407768,\n        -0.014586140401661396,\n        -0.002562703797593713,\n        -0.03692454472184181,\n        0.003670169971883297,\n        0.0022588258143514395,\n        -0.015545044094324112,\n        -0.019421175122261047,\n        -0.022216852754354477,\n        0.03670845180749893,\n        0.010169780813157558,\n        0.012202386744320393,\n        -0.004801271017640829,\n        0.003869378939270973,\n        0.011155696585774422,\n        -0.006401694845408201,\n        -0.0010661050910130143,\n        -0.004784388933330774,\n        -0.014005395583808422,\n        -0.0001811661059036851,\n        0.0048687998205423355,\n        0.026038961485028267,\n        -0.016368890181183815,\n        0.02356742136180401,\n        0.017989574000239372,\n        0.025228621438145638,\n        0.02162260189652443,\n        0.0005600639269687235,\n        -0.009946937672793865,\n        -0.02618752419948578,\n        -0.013897350057959557,\n        -0.009690329432487488,\n        0.009845645166933537,\n        0.003646535100415349,\n        -0.003062414238229394,\n        -0.024999024346470833,\n        0.001968453638255596,\n        0.0153964813798666,\n        0.006638044491410255,\n        -0.006604280322790146,\n        0.02349989302456379,\n        0.011885003186762333,\n        -0.018070606514811516,\n        -0.0006478508585132658,\n        0.007353845983743668,\n        0.045325081795454025,\n        -0.0020883167162537575,\n        -0.011756699532270432,\n        0.004082093480974436,\n        0.002177791902795434,\n        0.01245899498462677,\n        0.028226882219314575,\n        -0.0009521508472971618,\n        -0.015153379179537296,\n        -0.0033291515428572893,\n        -0.024202188476920128,\n        0.012384713627398014,\n        -0.0008930634357966483,\n        -0.012891177088022232,\n        0.002405700273811817,\n        -0.004710108041763306,\n        -0.025228621438145638,\n        -0.008224962279200554,\n        -0.02502603456377983,\n        -0.009413463063538074,\n        0.011776957660913467,\n        0.0009504626505076885,\n        -0.005760174710303545,\n        0.007684735115617514,\n        -0.015707112848758698,\n        -0.017395323142409325,\n        0.012627815827727318,\n        -0.003528360277414322,\n        0.03443949669599533,\n        -0.002047799527645111,\n        0.02035306766629219,\n        -0.002650490729138255,\n        0.023243285715579987,\n        -0.030873997136950493,\n        0.009670071303844452,\n        0.0006254820618778467,\n        -0.0008719608304090798,\n        0.01688210666179657,\n        -0.010588457807898521,\n        0.0068541355431079865,\n        -0.01546400971710682,\n        0.011331270448863506,\n        0.02056915871798992,\n        -0.007272811606526375,\n        -0.023905063048005104,\n        0.008170939981937408,\n        0.012114600278437138,\n        -0.006340919528156519,\n        -0.009426968172192574,\n        -0.019299624487757683,\n        0.005405650474131107,\n        0.03965269401669502,\n        0.001683990121819079,\n        0.007090484723448753,\n        -0.005631871055811644,\n        -0.01819215901196003,\n        -0.014856253750622272,\n        0.009838892146945,\n        -0.013208560645580292,\n        0.014559129253029823,\n        0.020663699135184288,\n        -0.017152220010757446,\n        -0.009980701841413975,\n        0.023905063048005104,\n        0.00895426981151104,\n        0.005885102320462465,\n        -0.010244062170386314,\n        -0.005341498646885157,\n        -0.04086820408701897,\n        0.01572061888873577,\n        -0.01038587186485529,\n        -0.011425809934735298,\n        0.013525944203138351,\n        0.005638623610138893,\n        0.00803588330745697,\n        0.028469985350966454,\n        -0.028794120997190475,\n        -0.03687052056193352,\n        0.011939026415348053,\n        -0.006982439663261175,\n        0.022838113829493523,\n        0.01038587186485529,\n        0.009690329432487488,\n        -0.0046256971545517445,\n        -0.009028551168739796,\n        -0.042380839586257935,\n        0.010392624884843826,\n        0.010554693639278412,\n        -0.010615468956530094,\n        -0.011067909188568592,\n        0.0087044145911932,\n        0.011338023468852043,\n        -0.023905063048005104,\n        0.007022956386208534,\n        0.0050680083222687244,\n        0.010264321230351925,\n        -0.03927453234791756,\n        0.004186762496829033,\n        -0.0002825642586685717,\n        0.029928598552942276,\n        0.0072120362892746925,\n        0.006479352712631226,\n        0.002410764805972576,\n        0.014437577687203884,\n        -0.029577450826764107,\n        0.001482249004766345,\n        -0.0041833859868347645,\n        0.011682418175041676,\n        0.008414042182266712,\n        0.016544464975595474,\n        -0.0023212896194308996,\n        -0.03673546388745308,\n        0.02216283045709133,\n        -0.013519191183149815,\n        -0.020380079746246338,\n        -0.013586719520390034,\n        0.02223035879433155,\n        0.008204704150557518,\n        0.011810721829533577,\n        -0.005979642271995544,\n        -0.02183869294822216,\n        0.009426968172192574,\n        0.009832139126956463,\n        0.002562703797593713,\n        -0.0005908737657591701,\n        -0.007056720554828644,\n        -0.013843327760696411,\n        -0.00681361835449934,\n        0.03862626105546951,\n        -0.024026615545153618,\n        0.0035992651246488094,\n        -0.0015751005848869681,\n        -0.005898608360439539,\n        -0.0014273821143433452,\n        -0.0307929627597332,\n        -0.003504725405946374,\n        -0.03230559825897217,\n        0.017638426274061203,\n        -0.0020950695034116507,\n        -0.010649233125150204,\n        0.03000963293015957,\n        -0.00400781212374568,\n        -0.006232874002307653,\n        0.027659643441438675,\n        0.019556231796741486,\n        -0.01035210769623518,\n        -0.017287276685237885,\n        0.013586719520390034,\n        0.01563958451151848,\n        0.01992088556289673,\n        0.007022956386208534,\n        0.014802231453359127,\n        0.011473080143332481,\n        -0.01093960553407669,\n        -0.011000380851328373,\n        -0.03635730594396591,\n        -0.016733543947339058,\n        0.012904682196676731,\n        0.025188103318214417,\n        -0.00637130718678236,\n        -0.02505304664373398,\n        -0.006648173555731773,\n        -0.012303679250180721,\n        -0.008366771973669529,\n        -0.012621062807738781,\n        -0.0029476159252226353,\n        0.01132451742887497,\n        0.011405551806092262,\n        -0.003866002429276705,\n        0.0021541567984968424,\n        -0.01447809487581253,\n        -0.011560866609215736,\n        -0.01476171426475048,\n        0.010196792893111706,\n        -0.01830020360648632,\n        -0.013600225560367107,\n        0.014788725413382053,\n        -0.00618560379371047,\n        -0.03009066730737686,\n        -0.021393006667494774,\n        -0.008839471265673637,\n        -0.023418858647346497,\n        -0.02439126744866371,\n        0.006921663880348206,\n        -0.004537910223007202,\n        -0.003069167025387287,\n        -0.012431983835995197,\n        0.03497972711920738,\n        0.00257114483974874,\n        0.02343236468732357,\n        0.007340339943766594,\n        -0.0170981977134943,\n        -0.006921663880348206,\n        0.0005136380787007511,\n        -0.007853556424379349,\n        -0.020758239552378654,\n        0.0472969114780426,\n        0.01646343059837818,\n        -0.01834072172641754,\n        0.024796439334750175,\n        -0.020690711215138435,\n        -0.0104061309248209,\n        0.020839272066950798,\n        0.007205283269286156,\n        -0.0015995795838534832,\n        0.028307916596531868,\n        -0.007954848930239677,\n        -0.0005482464330270886,\n        0.0008061206317506731,\n        -0.007813039235770702,\n        -0.027159933000802994,\n        -0.002035982208326459,\n        -0.0029357983730733395,\n        -0.011939026415348053,\n        0.009859150275588036,\n        -0.012951952405273914,\n        -0.00621261540800333,\n        0.012641321867704391,\n        -0.015247918665409088,\n        -0.013883844949305058,\n        0.01286416593939066,\n        -0.004331948701292276,\n        0.004152998328208923,\n        -0.0404900461435318,\n        0.006243003066629171,\n        -0.0006841474096290767,\n        -0.02895618975162506,\n        0.014991311356425285,\n        -0.01886744238436222,\n        -0.013688012026250362,\n        -0.00980512797832489,\n        -0.0017110015032812953,\n        -0.0030218970496207476,\n        -0.019907381385564804,\n        -0.003005014965310693,\n        0.030765952542424202,\n        0.007914331741631031,\n        0.002878399332985282,\n        -0.008562604896724224,\n        0.017017163336277008,\n        -0.011743193492293358,\n        0.003190718125551939,\n        0.22646333277225494,\n        -0.011216471903026104,\n        0.006151839625090361,\n        0.009001539088785648,\n        -0.010899088345468044,\n        0.005429285578429699,\n        0.015328953042626381,\n        -0.00955527275800705,\n        0.00781979225575924,\n        0.020218010991811752,\n        0.007542925421148539,\n        -0.0182326752692461,\n        -0.03217054158449173,\n        0.01242523081600666,\n        0.0015075721312314272,\n        0.006320660933852196,\n        -0.029334349557757378,\n        -0.03281881660223007,\n        -0.002746718702837825,\n        -0.035763055086135864,\n        0.013141032308340073,\n        -0.003474337514489889,\n        -0.02059617079794407,\n        -0.02109588123857975,\n        0.04300210252404213,\n        0.009224383160471916,\n        0.004237408749759197,\n        -0.005550836678594351,\n        0.023378342390060425,\n        0.003764709923416376,\n        -0.02020450495183468,\n        0.005577848292887211,\n        -0.0034338205587118864,\n        0.004568298347294331,\n        -0.0202450230717659,\n        0.006631291471421719,\n        -0.007988613098859787,\n        -0.0082654794678092,\n        0.021109387278556824,\n        0.007596948184072971,\n        0.0003009235661011189,\n        0.013249077834188938,\n        -0.016220327466726303,\n        0.008643638342618942,\n        0.013019480742514133,\n        0.018178652971982956,\n        -0.0002962809812743217,\n        -0.03114411048591137,\n        -0.0038086033891886473,\n        0.025525745004415512,\n        -0.019529221579432487,\n        0.00334434537217021,\n        0.002144027501344681,\n        0.037815921008586884,\n        -0.012884424068033695,\n        0.01805710233747959,\n        -0.00022326585894916207,\n        0.0004566609859466553,\n        -0.0013632301706820726,\n        0.031063076108694077,\n        0.006367930676788092,\n        0.00592899601906538,\n        -0.006914910860359669,\n        0.04845840111374855,\n        0.008292490616440773,\n        0.03408835083246231,\n        -0.016260845586657524,\n        0.015409987419843674,\n        0.008292490616440773,\n        -0.03581707924604416,\n        -0.003082672832533717,\n        -0.026822291314601898,\n        -0.0005174365942366421,\n        -0.01453211810439825,\n        -0.018070606514811516,\n        -0.007907578721642494,\n        0.013127526268362999,\n        0.008177693001925945,\n        0.02048812434077263,\n        0.016193317249417305,\n        -0.022676046937704086,\n        -0.03203548491001129,\n        -0.012479253113269806,\n        -0.005344875156879425,\n        0.002648802474141121,\n        -0.023418858647346497,\n        -0.0076644765213131905,\n        -0.0028328176122158766,\n        -0.006681937724351883,\n        -0.008690908551216125,\n        0.025755342096090317,\n        -0.024688392877578735,\n        -0.023972591385245323,\n        -0.010095500387251377,\n        0.008137175813317299,\n        -0.016557971015572548,\n        0.017044175416231155,\n        0.014869759790599346,\n        0.0068541355431079865,\n        0.013012727722525597,\n        0.00861662719398737,\n        0.02441827952861786,\n        0.012344196438789368,\n        0.011351528577506542,\n        -0.01985335722565651,\n        -0.00024162515182979405,\n        0.0002707467938307673,\n        0.023878052830696106,\n        0.0026910079177469015,\n        -0.01692262478172779,\n        0.005000479985028505,\n        -0.03225157782435417,\n        0.0023938827216625214,\n        -0.01985335722565651,\n        0.010264321230351925,\n        -0.021176915615797043,\n        -0.022149324417114258,\n        0.0007200218387879431,\n        0.01751687377691269,\n        -0.02356742136180401,\n        -0.004882305394858122,\n        -0.025255631655454636,\n        0.008360018953680992,\n        0.02039358578622341,\n        0.004943080712109804,\n        -0.024999024346470833,\n        -0.006050547119230032,\n        -0.007299823220819235,\n        0.013168043456971645,\n        -0.004642579238861799,\n        0.01801658421754837,\n        -0.03606018051505089,\n        0.011966037563979626,\n        -0.000476497458294034,\n        -0.0037376985419541597,\n        -0.010682997293770313,\n        0.018138134852051735,\n        -0.011034145019948483,\n        -0.00955527275800705,\n        0.01060196291655302,\n        -0.0024985517375171185,\n        -0.019772322848439217,\n        0.006800112780183554,\n        0.007644217927008867,\n        0.0006339231040328741,\n        -0.016841590404510498,\n        0.009994206950068474,\n        0.001396150211803615,\n        -0.01748986355960369,\n        -0.007016203831881285,\n        -0.013174796476960182,\n        -0.005797315388917923,\n        -0.000915854296181351,\n        -0.012195633724331856,\n        0.0032329235691577196,\n        0.002496863715350628,\n        -0.0006795048248022795,\n        -0.018097618594765663,\n        0.02092030644416809,\n        -0.0033156457357108593,\n        -0.026038961485028267,\n        -0.014491600915789604,\n        0.010865324176847935,\n        0.00835326686501503,\n        -0.013735282234847546,\n        0.015072344802320004,\n        -0.17384518682956696,\n        0.004551416262984276,\n        0.019529221579432487,\n        -0.01777348294854164,\n        0.01758440211415291,\n        -0.021298466250300407,\n        -0.003585759550333023,\n        -0.0009783180430531502,\n        -0.02159559167921543,\n        0.02354040928184986,\n        0.04232681915163994,\n        -0.03114411048591137,\n        -0.021987255662679672,\n        -0.024593854323029518,\n        -0.0013269336195662618,\n        0.004180009476840496,\n        0.007873814553022385,\n        -0.008751683868467808,\n        0.014896770939230919,\n        0.02240593172609806,\n        0.020380079746246338,\n        -0.002083251951262355,\n        0.005000479985028505,\n        0.0038457440678030252,\n        -0.0040044356137514114,\n        -0.002125457162037492,\n        -0.016841590404510498,\n        0.037572816014289856,\n        0.007718499284237623,\n        -0.009636306203901768,\n        0.013525944203138351,\n        0.0022588258143514395,\n        0.013262582942843437,\n        0.016341879963874817,\n        -0.004564921837300062,\n        0.010325096547603607,\n        -0.007650970946997404,\n        -0.029010212048888206,\n        -0.021298466250300407,\n        0.015950214117765427,\n        0.02655217796564102,\n        0.017287276685237885,\n        0.011020638979971409,\n        -0.00978486891835928,\n        -0.01939416490495205,\n        -0.00444337073713541,\n        0.021433522924780846,\n        -0.004791141953319311,\n        0.022878631949424744,\n        -0.04427163675427437,\n        0.01646343059837818,\n        0.010581704787909985,\n        -0.002368559595197439,\n        -0.018462272360920906,\n        0.0182326752692461,\n        0.003511478193104267,\n        0.00946748536080122,\n        0.023594433441758156,\n        -0.014275509864091873,\n        0.0014383555389940739,\n        -0.0005305202212184668,\n        -0.01511286199092865,\n        0.029469406232237816,\n        -0.01343140471726656,\n        0.017395323142409325,\n        -0.003045532153919339,\n        0.01447809487581253,\n        0.00055964186321944,\n        -0.012330691330134869,\n        0.006192356813699007,\n        -0.017476357519626617,\n        -0.006533375475555658,\n        -0.009575530886650085,\n        -0.026228042319417,\n        -0.018840432167053223,\n        0.007954848930239677,\n        -0.027267979457974434,\n        -0.003572253743186593,\n        -0.020218010991811752,\n        0.009231136180460453,\n        -0.004078716970980167,\n        0.03795097768306732,\n        -0.01624733954668045,\n        0.009143348783254623,\n        -0.010608715936541557,\n        -0.0031029311940073967,\n        0.009028551168739796,\n        0.01620682328939438,\n        0.009845645166933537,\n        -0.02749757654964924,\n        0.023986097425222397,\n        -0.03011767938733101,\n        -0.005040997173637152,\n        -0.01504533365368843,\n        0.030522849410772324,\n        0.01447809487581253,\n        0.015301941893994808,\n        0.005587977357208729,\n        -0.0028615170158445835,\n        -0.0005701931659132242,\n        -0.0092716533690691,\n        -0.01659848727285862,\n        0.002939174883067608,\n        -0.008981280960142612,\n        -0.013485427014529705,\n        0.012479253113269806,\n        0.023918569087982178,\n        -0.006209238898009062,\n        0.04005786404013634,\n        -0.0005959383561275899,\n        -0.02046111412346363,\n        0.0011024016421288252,\n        0.022432943806052208,\n        0.01635538600385189,\n        0.0024732286110520363,\n        0.02806481532752514,\n        0.011081415228545666,\n        -0.00823171529918909,\n        0.010959863662719727,\n        0.01545050460845232,\n        0.07476747781038284,\n        0.014086429961025715,\n        -0.009933431632816792,\n        0.0028885283973068,\n        -0.007461891509592533,\n        -0.03795097768306732,\n        -0.09972598403692245,\n        -0.021393006667494774,\n        0.013039739802479744,\n        0.006621162407100201,\n        0.015572055242955685,\n        -0.001142918597906828,\n        0.013586719520390034,\n        -0.010196792893111706,\n        0.013417898677289486,\n        0.021136397495865822,\n        -0.018219169229269028,\n        -0.017638426274061203,\n        -0.015288435854017735,\n        -0.03217054158449173,\n        -0.015436998568475246,\n        -0.007542925421148539,\n        -0.003953789360821247,\n        -0.019475199282169342,\n        -0.0048789288848638535,\n        0.009190618991851807,\n        -0.004926198627799749,\n        -0.012634568847715855,\n        -0.01613929495215416,\n        -0.011405551806092262,\n        -0.01459964644163847,\n        -0.02166312001645565,\n        -0.014005395583808422,\n        0.008873235434293747,\n        0.008724672719836235,\n        0.006455717608332634,\n        -0.0165309589356184,\n        -0.0279297586530447,\n        0.011148943565785885,\n        0.0030404673889279366,\n        0.016260845586657524,\n        0.01666601561009884,\n        -0.03954464569687843,\n        -0.0018435260280966759,\n        0.00558460084721446,\n        -0.010163028724491596,\n        0.0061585926450788975,\n        0.021987255662679672,\n        0.009008292108774185,\n        -0.009177112951874733,\n        0.030063655227422714,\n        -0.01819215901196003,\n        -0.019664278253912926,\n        0.013492180034518242,\n        0.005696022883057594,\n        0.01762492023408413,\n        -0.011013886891305447,\n        -0.002336483681574464,\n        -0.024958506226539612,\n        -0.01180396880954504,\n        0.015193896368145943,\n        -0.018772903829813004,\n        -0.004163127392530441,\n        -0.017192738130688667,\n        -0.008042635396122932,\n        -0.002027540933340788,\n        0.00580406840890646,\n        -0.002078187419101596,\n        -0.01847577840089798,\n        0.03344007581472397,\n        0.03676247596740723,\n        0.0013092074077576399,\n        0.0012180439662188292,\n        -0.008792201057076454,\n        0.02325678989291191,\n        -0.005628494545817375,\n        -0.023580927401781082,\n        0.02467488683760166,\n        0.006226120982319117,\n        -0.010912594385445118,\n        -0.023770006373524666,\n        0.0002443685079924762,\n        -0.015990732237696648,\n        0.015085850842297077,\n        -0.01897548884153366,\n        -0.011020638979971409,\n        -0.036789488047361374,\n        -0.009933431632816792,\n        -0.007826544344425201,\n        -0.015315447002649307,\n        -0.0008293334976769984,\n        -0.012783131562173367,\n        -0.03271077200770378,\n        -0.042029693722724915,\n        0.012330691330134869,\n        -0.02930733747780323,\n        -0.012661579996347427,\n        0.026038961485028267,\n        0.03606018051505089,\n        0.013249077834188938,\n        -0.010750525631010532,\n        0.015004816465079784,\n        -0.0024749168660491705,\n        -0.018664857372641563,\n        2.4663702788529918e-05,\n        0.010392624884843826,\n        0.014275509864091873,\n        -0.003653287887573242,\n        -0.03522282838821411,\n        -0.005581224337220192,\n        0.006546881049871445,\n        -0.003381486050784588,\n        -0.014221486635506153,\n        -0.013066750951111317,\n        0.006891276221722364,\n        -0.005915490444749594,\n        -0.015693606808781624,\n        -0.0026791903655976057,\n        -0.007239047437906265,\n        0.006837253458797932,\n        0.008947516791522503,\n        0.009345934726297855,\n        -0.014207981526851654,\n        -0.013444909825921059,\n        0.025809364393353462,\n        0.0012138234451413155,\n        -0.003781592007726431,\n        0.007468644063919783,\n        -0.0076644765213131905,\n        -0.0014408878050744534,\n        0.014748208224773407,\n        0.0005887635052204132,\n        -0.0048316591419279575,\n        -0.01208758819848299,\n        0.008684155531227589,\n        -0.00230103125795722,\n        -0.028469985350966454,\n        -0.011594630777835846,\n        0.019556231796741486,\n        -0.00863688625395298,\n        0.004618944600224495,\n        0.0279567688703537,\n        -0.002799053443595767,\n        -0.012296927161514759,\n        -0.0012433672090992332,\n        0.019839851185679436,\n        0.011783710680902004,\n        -0.005625118035823107,\n        -0.015261424705386162,\n        -0.028713086619973183,\n        0.004186762496829033,\n        -0.02644413150846958,\n        0.013444909825921059,\n        -0.0031198132783174515,\n        -0.025188103318214417,\n        -0.00018749690207187086,\n        0.02718694508075714,\n        -0.013870338909327984,\n        0.0148292426019907,\n        0.014842748641967773,\n        0.0004252180806361139,\n        0.006050547119230032,\n        -0.008738178759813309,\n        -0.02884814515709877,\n        0.007293070200830698,\n        0.008170939981937408,\n        0.008812460117042065,\n        -0.016017742455005646,\n        0.03298088535666466,\n        0.012634568847715855,\n        -0.00032899004872888327,\n        -0.0006440524011850357,\n        0.03352111205458641,\n        0.0020680581219494343,\n        -0.013262582942843437,\n        0.005554213188588619,\n        -0.002520498586818576,\n        -0.03238663449883461,\n        -0.0017321042250841856,\n        -0.011216471903026104,\n        0.009562025777995586,\n        -0.0026049090083688498,\n        0.04267796501517296,\n        0.0037174399476498365,\n        0.02039358578622341,\n        0.004679719917476177,\n        -0.028767110779881477,\n        0.026781775057315826,\n        0.009190618991851807,\n        -0.020717721432447433,\n        -0.01563958451151848,\n        0.009285158477723598,\n        0.04705381020903587,\n        0.002957745222374797,\n        -0.018840432167053223,\n        0.017678942531347275,\n        -0.004409606568515301,\n        0.00017958341049961746,\n        0.008785448037087917,\n        0.008684155531227589,\n        0.011939026415348053,\n        -0.007495655678212643,\n        0.008758436888456345,\n        0.013357123360037804,\n        -0.0028682700358331203,\n        0.013600225560367107,\n        0.01812463067471981,\n        0.01072351448237896,\n        0.012310432270169258,\n        0.0028699582908302546,\n        -0.01950220949947834,\n        -0.0014839372597634792,\n        -0.01286416593939066,\n        -0.0007060941425152123,\n        -0.008360018953680992,\n        -0.02208179607987404,\n        -0.010041477158665657,\n        0.022838113829493523,\n        0.023175757378339767,\n        0.00923788920044899,\n        0.0267952810972929,\n        0.01242523081600666,\n        -0.027200451120734215,\n        0.011128684505820274,\n        0.010966616682708263,\n        -0.0065164933912456036,\n        -0.014842748641967773,\n        0.007286317180842161,\n        -0.002145715756341815,\n        -0.0007896605529822409,\n        0.04324520379304886,\n        -0.016125788912177086,\n        0.02133898250758648,\n        -0.006249756086617708,\n        0.007752263452857733,\n        -0.033034905791282654,\n        0.008042635396122932,\n        0.009305417537689209,\n        0.01600423827767372,\n        -0.010568198747932911,\n        -0.0014366672839969397,\n        -0.0142484987154603,\n        0.008056141436100006,\n        0.000470588740427047,\n        0.0022115560714155436,\n        0.05064631998538971,\n        -0.02244644984602928,\n        0.03338605538010597,\n        0.0041462453082203865,\n        0.0004359804152045399,\n        -0.027848724275827408,\n        0.007455138489603996,\n        0.01488326583057642,\n        0.011662159115076065,\n        -0.001961700851097703,\n        -0.00851533468812704,\n        0.0005845429259352386,\n        0.00053220841800794,\n        0.007083732169121504,\n        0.016625499352812767,\n        -0.0165309589356184,\n        -0.019299624487757683,\n        0.02399960346519947,\n        -0.01129075326025486,\n        0.018003078177571297,\n        -0.002427646890282631,\n        -0.005922242999076843,\n        0.02828090637922287,\n        -0.007117496337741613,\n        0.009575530886650085,\n        -0.01006848830729723,\n        -0.016557971015572548,\n        -0.01585567556321621,\n        0.011533855460584164,\n        -0.014113441109657288,\n        -0.020731227472424507,\n        -0.04116532951593399,\n        0.03522282838821411,\n        0.0038761317264288664,\n        -0.022568000480532646,\n        -0.0033899270929396152,\n        -0.0046999785117805,\n        -0.022554494440555573,\n        0.0003549462999217212,\n        0.003869378939270973,\n        0.019907381385564804,\n        0.029361359775066376,\n        0.003609394421800971,\n        0.03090100921690464,\n        -0.01488326583057642,\n        -0.03430444002151489,\n        -0.010608715936541557,\n        -0.0013564772671088576,\n        -0.007529419846832752,\n        -0.014180969446897507,\n        -0.020690711215138435\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"ada\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 7,\n    \"total_tokens\": 7\n  }\n}"},"events":[],"links":[]}],"xrequestid":"ba98f534-0803-40f6-a028-24dfd031f5eb","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-49333/49334","correlationid":"ba98f534-0803-40f6-a028-24dfd031f5eb"}