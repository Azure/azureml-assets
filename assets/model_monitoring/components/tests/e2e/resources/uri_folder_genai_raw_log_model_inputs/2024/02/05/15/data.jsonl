{"specversion":"1.0","id":"59ab58f7-cbf6-4f4f-af8d-7352eed49f0b","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:17Z","data":[{"name":"openai.resources.chat.completions.Completions.create","context":{"request_id":"0xccf497116a5a912b29e55229bbac0ba8","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x6275bb0c2a2594e0","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x21d2ebaaad77bedf","start_time":"2024-02-05T15:00:13.796476Z","end_time":"2024-02-05T15:00:14.436365Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.chat.completions.Completions.create","node_name":"modify_query_with_history","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Given the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history:\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Follow up Input: what's Attention? \\nStandalone Question:\"\n    }\n  ],\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": null,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8oukAwNXR2qyf3yQzLGoZ8o2M8Ka5\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"What is the meaning of Attention?\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707145214,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 7,\n    \"prompt_tokens\": 78,\n    \"total_tokens\": 85\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"correlationid":"0683a146-03f1-4aae-8131-d0d63b2e4187","xrequestid":"0683a146-03f1-4aae-8131-d0d63b2e4187","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-2125/2126"}
{"specversion":"1.0","id":"d8fa94f7-0d57-478f-8398-612e070ca12a","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:17Z","data":[{"name":"modify_query_with_history","context":{"request_id":"0xccf497116a5a912b29e55229bbac0ba8","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x21d2ebaaad77bedf","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x7fd179134fb9e709","start_time":"2024-02-05T15:00:13.792668Z","end_time":"2024-02-05T15:00:14.437464Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"AzureOpenAI.chat","node_name":"modify_query_with_history","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"prompt\": \"system: \\nGiven the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\n\\nuser:\\nFollow up Input: {{question}} \\nStandalone Question:\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"chat_history\": [],\n  \"question\": \"what's Attention?\"\n}","output":"\"What is the meaning of Attention?\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"xrequestid":"b7fdb4b5-4904-45db-b1a5-ec744a757585","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-1428/1429","correlationid":"b7fdb4b5-4904-45db-b1a5-ec744a757585"}
{"specversion":"1.0","id":"939b0ed2-e8b5-481a-983c-7fddbd752d4f","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"openai.resources.embeddings.Embeddings.create","context":{"request_id":"0xccf497116a5a912b29e55229bbac0ba8","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0xde147526538f191d","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x1bb2aa9e6b530b09","start_time":"2024-02-05T15:00:17.067114Z","end_time":"2024-02-05T15:00:17.619294Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.embeddings.Embeddings.create","node_name":"vector_lookup","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"input\": [\n    [\n      3923,\n      374,\n      279,\n      7438,\n      315,\n      63120,\n      30\n    ]\n  ],\n  \"model\": \"text-embedding-ada-002\"\n}","output":"{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.03181939572095871,\n        0.0023466129787266254,\n        0.03857223689556122,\n        -0.014059418812394142,\n        -0.005179430358111858,\n        0.014072923921048641,\n        -0.027767689898610115,\n        -0.017179232090711594,\n        -0.01000095997005701,\n        -0.023594433441758156,\n        0.0336291566491127,\n        0.020055942237377167,\n        -0.013937867246568203,\n        0.006378060206770897,\n        0.014329532161355019,\n        -0.013451662845909595,\n        0.043164171278476715,\n        0.010541187599301338,\n        -0.01359347254037857,\n        -0.030036645010113716,\n        -0.02895618975162506,\n        -7.66025623306632e-05,\n        0.0011108426842838526,\n        0.02378351241350174,\n        -0.007610453758388758,\n        0.011182707734405994,\n        0.011094920337200165,\n        -0.024728910997509956,\n        -0.006685314234346151,\n        -0.038842350244522095,\n        0.023418858647346497,\n        0.004946457222104073,\n        0.006573892664164305,\n        -0.020380079746246338,\n        -0.017003657296299934,\n        -0.004686472937464714,\n        0.004949833732098341,\n        -0.028605042025446892,\n        0.020326057448983192,\n        -0.00028868403751403093,\n        0.02526913769543171,\n        0.024013109505176544,\n        -0.0022486967500299215,\n        0.00564537663012743,\n        0.0006626226822845638,\n        0.006668432150036097,\n        0.008089905604720116,\n        -0.028361938893795013,\n        -0.010784289799630642,\n        0.014653668738901615,\n        0.028794120997190475,\n        0.03271077200770378,\n        -0.029847566038370132,\n        -0.02616051211953163,\n        0.0024951754603534937,\n        -0.004916069563478231,\n        -0.004412982612848282,\n        0.0013429715763777494,\n        0.013742035254836082,\n        0.0015455569373443723,\n        -0.0005807444686070085,\n        -0.013120773248374462,\n        -0.010547940619289875,\n        0.01214161142706871,\n        -0.0017489863093942404,\n        0.01996140368282795,\n        -0.00020986569870729,\n        0.019515715539455414,\n        -0.010784289799630642,\n        -0.007610453758388758,\n        0.020366573706269264,\n        0.022595012560486794,\n        0.025606779381632805,\n        -0.01546400971710682,\n        0.026065973564982414,\n        -0.01551803294569254,\n        -0.020434102043509483,\n        -0.008501828648149967,\n        0.010932852514088154,\n        0.026808785274624825,\n        0.005871596746146679,\n        -0.0004243739531375468,\n        -0.0043994770385324955,\n        0.03319697454571724,\n        -0.016760556027293205,\n        0.0012771313777193427,\n        0.007765769027173519,\n        0.002853075973689556,\n        -0.000697653042152524,\n        -0.004453499801456928,\n        -0.012803389690816402,\n        -0.030036645010113716,\n        0.02590390481054783,\n        0.008792201057076454,\n        0.0159637201577425,\n        0.025539251044392586,\n        -0.0046898494474589825,\n        0.0365193746984005,\n        -0.02611999586224556,\n        -0.022203346714377403,\n        0.0023753123823553324,\n        0.01688210666179657,\n        -0.032224565744400024,\n        -0.0002006860449910164,\n        0.007286317180842161,\n        -0.0017059368547052145,\n        -0.019178073853254318,\n        -0.0076172067783772945,\n        0.002799053443595767,\n        -0.01794905588030815,\n        -0.03100905381143093,\n        0.02657919004559517,\n        -0.008218209259212017,\n        -0.04481186345219612,\n        0.003481090534478426,\n        0.015153379179537296,\n        -0.004122610669583082,\n        -0.0030792963225394487,\n        -0.03446650877594948,\n        -0.018786408007144928,\n        0.0302257239818573,\n        0.007644217927008867,\n        0.018529800698161125,\n        -0.0014062795089557767,\n        -0.003997683059424162,\n        0.018988993018865585,\n        -0.008771942928433418,\n        -0.01928611844778061,\n        -0.016341879963874817,\n        -0.013053244911134243,\n        0.006364554166793823,\n        0.030873997136950493,\n        0.0016147735295817256,\n        0.013087009079754353,\n        -0.0053887683898210526,\n        0.01751687377691269,\n        0.004666214343160391,\n        -0.0027146427892148495,\n        -0.05175378546118736,\n        -0.019137555733323097,\n        0.0069689336232841015,\n        0.024161672219634056,\n        -0.0064523410983383656,\n        -0.0075901951640844345,\n        0.01862434111535549,\n        0.030522849410772324,\n        0.02611999586224556,\n        0.012202386744320393,\n        0.0072593060322105885,\n        0.010271074250340462,\n        0.03092801943421364,\n        0.006749466527253389,\n        0.01808411255478859,\n        -0.002189609222114086,\n        0.0005111058126203716,\n        0.017935549840331078,\n        -0.007144507486373186,\n        0.0018975487910211086,\n        0.011500091291964054,\n        -0.025606779381632805,\n        0.033467087894678116,\n        -0.009649812243878841,\n        0.010041477158665657,\n        -0.026565684005618095,\n        0.02686280943453312,\n        0.045081976801157,\n        0.0035519953817129135,\n        0.01152034942060709,\n        -0.005162548273801804,\n        -0.002206491306424141,\n        -0.016112282872200012,\n        0.023702478036284447,\n        -0.03138721361756325,\n        0.017611414194107056,\n        -0.013829821720719337,\n        0.018165146932005882,\n        0.027659643441438675,\n        0.0012366143055260181,\n        -0.016085270792245865,\n        -0.00223856745287776,\n        -0.016868600621819496,\n        -0.0010838313028216362,\n        0.01834072172641754,\n        0.010892335325479507,\n        -0.026957347989082336,\n        -0.002802429720759392,\n        0.010304838418960571,\n        -0.006236250512301922,\n        0.0056420001201331615,\n        -0.01177020464092493,\n        0.01546400971710682,\n        0.03430444002151489,\n        -0.004075340460985899,\n        -0.008258726447820663,\n        -0.6651279926300049,\n        -0.003524984000250697,\n        0.006502987816929817,\n        -0.005297604948282242,\n        -0.0007613830384798348,\n        0.004645955748856068,\n        0.01688210666179657,\n        0.011000380851328373,\n        -0.007536172401160002,\n        0.028226882219314575,\n        -0.01309376209974289,\n        -0.007718499284237623,\n        -0.009474238380789757,\n        -0.0015143250348046422,\n        -0.019191579893231392,\n        -0.017395323142409325,\n        -0.012384713627398014,\n        -0.016801072284579277,\n        -0.012816895730793476,\n        0.008906999602913857,\n        -0.03057687170803547,\n        0.022676046937704086,\n        -0.019340142607688904,\n        0.015234413556754589,\n        0.030522849410772324,\n        -0.012384713627398014,\n        -0.000990135595202446,\n        -0.025282643735408783,\n        -0.02229788713157177,\n        0.022135818377137184,\n        -0.0076982406899333,\n        -0.01122322492301464,\n        0.027051888406276703,\n        0.0230812169611454,\n        0.029469406232237816,\n        -0.020474620163440704,\n        -0.004902563989162445,\n        0.02718694508075714,\n        -0.04286704584956169,\n        0.03379122540354729,\n        0.008204704150557518,\n        -0.0033173339907079935,\n        0.021082375198602676,\n        -0.021501051262021065,\n        -0.0043522072955966,\n        -0.016409408301115036,\n        0.02474241517484188,\n        -0.0087044145911932,\n        0.0005136380787007511,\n        -0.028578029945492744,\n        -0.01245899498462677,\n        0.0074753970839083195,\n        -0.00895426981151104,\n        -0.005091643426567316,\n        0.006286896765232086,\n        0.002908786991611123,\n        0.01961025595664978,\n        0.0027095782570540905,\n        0.019205084070563316,\n        0.0023263543844223022,\n        -0.00943372119218111,\n        0.045811284333467484,\n        -0.00444337073713541,\n        -0.013742035254836082,\n        -0.02753809280693531,\n        0.003401744645088911,\n        0.016720037907361984,\n        0.03092801943421364,\n        0.01830020360648632,\n        -0.034601565450429916,\n        0.0030573494732379913,\n        0.029226303100585938,\n        -0.007549678441137075,\n        0.0075901951640844345,\n        0.016301361843943596,\n        0.003043843898922205,\n        -0.001223952742293477,\n        0.013816316612064838,\n        0.020163988694548607,\n        0.015126368030905724,\n        -0.015139873139560223,\n        -0.008785448037087917,\n        -0.0037782154977321625,\n        -0.020150482654571533,\n        0.02155507355928421,\n        0.007711746264249086,\n        -0.0025356924161314964,\n        -0.006908158306032419,\n        5.09364836034365e-05,\n        -0.011405551806092262,\n        -0.012040318921208382,\n        -0.0013387510553002357,\n        0.004966715816408396,\n        -0.041894637048244476,\n        0.013937867246568203,\n        0.01801658421754837,\n        -0.009001539088785648,\n        0.008994787000119686,\n        0.00886648241430521,\n        0.023702478036284447,\n        0.0038524968549609184,\n        -0.010858571156859398,\n        -0.02092030644416809,\n        0.002834505867213011,\n        0.005013985559344292,\n        -0.008218209259212017,\n        -0.018462272360920906,\n        0.0330619178712368,\n        0.04027395322918892,\n        -0.008744931779801846,\n        -0.0026420496869832277,\n        -0.005675764288753271,\n        0.008852977305650711,\n        0.009251394309103489,\n        -0.002478293376043439,\n        -0.02605246752500534,\n        0.02002893202006817,\n        0.0142484987154603,\n        0.008630133233964443,\n        -0.008251973427832127,\n        0.010541187599301338,\n        -0.011621642857789993,\n        0.01840825006365776,\n        -0.024728910997509956,\n        -0.010014466010034084,\n        -0.012296927161514759,\n        -0.0017979444237425923,\n        -0.008981280960142612,\n        -0.03387225791811943,\n        0.009447227232158184,\n        0.0176654364913702,\n        -0.0009572154376655817,\n        -0.004125986713916063,\n        -0.01274261437356472,\n        0.001882354961708188,\n        0.00665492657572031,\n        -0.019029511138796806,\n        -0.03854522481560707,\n        0.022635528817772865,\n        -0.03932855650782585,\n        -0.017935549840331078,\n        0.019137555733323097,\n        0.017854515463113785,\n        -0.01794905588030815,\n        -0.028226882219314575,\n        -0.020960824564099312,\n        -0.0015404922887682915,\n        0.001764180138707161,\n        -0.028902167454361916,\n        -0.0065637631341814995,\n        0.009636306203901768,\n        -0.01233744341880083,\n        -0.02087979018688202,\n        0.013289595022797585,\n        -0.0148157374933362,\n        -0.011466327123343945,\n        -0.016233833506703377,\n        -0.014937288127839565,\n        -0.023634949699044228,\n        -0.03160330280661583,\n        0.013141032308340073,\n        0.015369470231235027,\n        -0.021136397495865822,\n        -0.022905642166733742,\n        -0.024999024346470833,\n        -0.01688210666179657,\n        -0.012729108333587646,\n        0.03967970237135887,\n        -0.04575726389884949,\n        -0.025863388553261757,\n        0.023837534710764885,\n        -0.012317185290157795,\n        0.003997683059424162,\n        0.010210298001766205,\n        -0.0060843112878501415,\n        0.025863388553261757,\n        -0.017138715833425522,\n        -0.006320660933852196,\n        0.011993048712611198,\n        -0.006492858286947012,\n        0.015653088688850403,\n        0.0036060181446373463,\n        -0.02169013023376465,\n        0.0016586669953539968,\n        0.009291911497712135,\n        0.011446068063378334,\n        0.04491991177201271,\n        0.013316606171429157,\n        -0.0022942782379686832,\n        0.009913173504173756,\n        -0.004818153101950884,\n        0.0071850246749818325,\n        -0.018070606514811516,\n        0.01003472413867712,\n        -0.0009344246354885399,\n        0.0153964813798666,\n        0.015653088688850403,\n        0.0055001904256641865,\n        0.0030944901518523693,\n        0.03743775933980942,\n        0.029037224128842354,\n        0.00429143151268363,\n        0.01928611844778061,\n        -0.010500670410692692,\n        0.009609295055270195,\n        0.017017163336277008,\n        -0.0009310481837019324,\n        -0.025890398770570755,\n        0.008191198110580444,\n        -0.0011327894171699882,\n        0.005797315388917923,\n        -0.027740677818655968,\n        -0.009453980252146721,\n        -0.010730267502367496,\n        0.015693606808781624,\n        0.02286512590944767,\n        -0.0020815636962652206,\n        0.004818153101950884,\n        -0.034709613770246506,\n        0.017705954611301422,\n        0.0213795006275177,\n        -0.0056521291844546795,\n        -0.011412303894758224,\n        -0.0026369851548224688,\n        0.005837832577526569,\n        0.003325775032863021,\n        0.034034326672554016,\n        0.04654059186577797,\n        -0.009258147329092026,\n        -0.002203115029260516,\n        -0.018880948424339294,\n        0.024161672219634056,\n        0.014950794167816639,\n        0.013755540363490582,\n        -0.02013697661459446,\n        -0.0006900561274960637,\n        0.01609877683222294,\n        -0.0067528425715863705,\n        0.00794809591025114,\n        -0.025228621438145638,\n        0.007610453758388758,\n        0.01670653373003006,\n        -0.00128557241987437,\n        -0.014113441109657288,\n        0.016828084364533424,\n        0.002250384772196412,\n        0.03805902227759361,\n        0.01677406206727028,\n        0.009960442781448364,\n        -0.003947036806493998,\n        -0.0043994770385324955,\n        -0.008711167611181736,\n        -0.002922292798757553,\n        -0.0003201269428245723,\n        -0.01635538600385189,\n        0.0037241927348077297,\n        -0.0025930916890501976,\n        -0.013458415865898132,\n        0.011689171195030212,\n        0.03784292936325073,\n        0.01943468116223812,\n        0.022676046937704086,\n        0.02884814515709877,\n        -0.0017658683937042952,\n        -0.006334166508167982,\n        0.014464588835835457,\n        0.011736440472304821,\n        -0.004622321110218763,\n        -0.004787765443325043,\n        0.002299343002960086,\n        0.008535593748092651,\n        -0.014640163630247116,\n        0.0021372747141867876,\n        -0.014140453189611435,\n        0.01840825006365776,\n        0.009291911497712135,\n        -0.019772322848439217,\n        -0.0008521243580617011,\n        0.019704794511198997,\n        0.0174088291823864,\n        -0.006877770181745291,\n        -0.027011370286345482,\n        -0.006641421001404524,\n        0.0076982406899333,\n        -0.0059053609147667885,\n        -0.025998445227742195,\n        0.013505685143172741,\n        0.005375262815505266,\n        -0.016476936638355255,\n        0.0035823830403387547,\n        -0.009899667464196682,\n        -0.009339181706309319,\n        -0.006320660933852196,\n        -0.013228818774223328,\n        0.0018975487910211086,\n        -0.012101094238460064,\n        0.011304258368909359,\n        0.013742035254836082,\n        0.0142620038241148,\n        -0.03011767938733101,\n        -0.002380377147346735,\n        -0.0060539236292243,\n        -0.008137175813317299,\n        -0.008879988454282284,\n        -0.0003192828444298357,\n        0.010568198747932911,\n        -0.0006221056682989001,\n        -0.012884424068033695,\n        -0.020717721432447433,\n        -0.0225409884005785,\n        0.017476357519626617,\n        0.007954848930239677,\n        -0.0030235853046178818,\n        -0.032791804522275925,\n        0.005861467681825161,\n        0.016652509570121765,\n        -0.0033105812035501003,\n        0.018961982801556587,\n        0.03357513248920441,\n        0.012506265193223953,\n        -0.002544133458286524,\n        -0.04046303406357765,\n        0.02020450495183468,\n        -0.004372465889900923,\n        0.0856260433793068,\n        0.002562703797593713,\n        -0.02279759757220745,\n        0.020650193095207214,\n        -0.01189175620675087,\n        0.019340142607688904,\n        -0.01734130084514618,\n        -0.010777536779642105,\n        0.02452632412314415,\n        -0.002885152120143175,\n        -0.025917410850524902,\n        -0.013492180034518242,\n        0.00851533468812704,\n        -0.004899187479168177,\n        0.025390688329935074,\n        0.017827505245804787,\n        -0.0032312353141605854,\n        0.001186812063679099,\n        0.010163028724491596,\n        -0.0230812169611454,\n        -0.0020072825718671083,\n        -0.009393204003572464,\n        0.016017742455005646,\n        -0.0025998444762080908,\n        -0.007144507486373186,\n        -0.013451662845909595,\n        -0.0039943065494298935,\n        0.00806964747607708,\n        0.022770585492253304,\n        -0.00948774442076683,\n        0.01075727865099907,\n        0.0030759198125451803,\n        -0.024013109505176544,\n        -0.0038457440678030252,\n        -0.015599066391587257,\n        0.011101673357188702,\n        0.011621642857789993,\n        -0.012445488944649696,\n        0.012870918028056622,\n        -0.021365994587540627,\n        0.015153379179537296,\n        0.03206249698996544,\n        0.013478673994541168,\n        -0.016720037907361984,\n        -0.0028193118050694466,\n        -0.013296347111463547,\n        0.001635876134969294,\n        0.005398897919803858,\n        -0.009703835472464561,\n        -0.0028125590179115534,\n        0.011297506280243397,\n        -0.005935749039053917,\n        -0.030603883787989616,\n        0.009629554115235806,\n        0.014802231453359127,\n        0.013647494837641716,\n        -0.01339764054864645,\n        -0.002650490729138255,\n        -0.0028935931622982025,\n        -0.022743575274944305,\n        -0.03376421332359314,\n        -0.009656565263867378,\n        -0.007164766080677509,\n        -0.0011716182343661785,\n        0.0035925123374909163,\n        -0.03389926999807358,\n        -0.006462470628321171,\n        -0.018921464681625366,\n        -0.005827703513205051,\n        0.009595789946615696,\n        0.025390688329935074,\n        -0.02452632412314415,\n        -0.04448772966861725,\n        -0.004747248254716396,\n        0.026822291314601898,\n        -0.0008090749615803361,\n        -0.005034244153648615,\n        0.0011783710215240717,\n        -0.024688392877578735,\n        0.011358281597495079,\n        -0.009778115898370743,\n        -0.02566080167889595,\n        -0.018597329035401344,\n        -0.009055562317371368,\n        -0.011648654006421566,\n        0.005040997173637152,\n        -0.0031890301033854485,\n        0.015761135146021843,\n        -0.0031569539569318295,\n        0.011594630777835846,\n        -0.005294228903949261,\n        0.01893497072160244,\n        0.04562220722436905,\n        -0.03614121302962303,\n        0.007792780641466379,\n        0.017462851479649544,\n        -0.003997683059424162,\n        0.0067798541858792305,\n        0.009987454861402512,\n        -0.0225139781832695,\n        -0.0092716533690691,\n        -0.026768269017338753,\n        -0.016436418518424034,\n        0.012587298639118671,\n        0.011398798786103725,\n        0.016328373923897743,\n        0.00027391218463890254,\n        0.029280327260494232,\n        0.018097618594765663,\n        -0.020933812484145164,\n        -0.006695443764328957,\n        -0.04184061288833618,\n        -0.009048809297382832,\n        0.004129363223910332,\n        0.003528360277414322,\n        0.002262202324345708,\n        0.0159637201577425,\n        0.018880948424339294,\n        -0.0005047749727964401,\n        -0.005037620663642883,\n        -0.017084691673517227,\n        -0.03808603435754776,\n        0.032089509069919586,\n        0.022527484223246574,\n        -0.003980800975114107,\n        -0.022784091532230377,\n        0.004348830785602331,\n        -0.005885102320462465,\n        -0.011142190545797348,\n        0.015504526905715466,\n        -0.013809563592076302,\n        0.004048329312354326,\n        -0.02067720517516136,\n        -0.04297509044408798,\n        -0.020272033289074898,\n        -0.000584120920393616,\n        -0.022149324417114258,\n        0.020906800404191017,\n        -0.017125209793448448,\n        0.00536175724118948,\n        -0.0036296530161052942,\n        -0.019907381385564804,\n        -0.011331270448863506,\n        -0.01546400971710682,\n        0.04640553519129753,\n        -0.030468827113509178,\n        -0.027848724275827408,\n        0.016720037907361984,\n        -0.023000182583928108,\n        0.028875155374407768,\n        -0.014586140401661396,\n        -0.002562703797593713,\n        -0.03692454472184181,\n        0.003670169971883297,\n        0.0022588258143514395,\n        -0.015545044094324112,\n        -0.019421175122261047,\n        -0.022216852754354477,\n        0.03670845180749893,\n        0.010169780813157558,\n        0.012202386744320393,\n        -0.004801271017640829,\n        0.003869378939270973,\n        0.011155696585774422,\n        -0.006401694845408201,\n        -0.0010661050910130143,\n        -0.004784388933330774,\n        -0.014005395583808422,\n        -0.0001811661059036851,\n        0.0048687998205423355,\n        0.026038961485028267,\n        -0.016368890181183815,\n        0.02356742136180401,\n        0.017989574000239372,\n        0.025228621438145638,\n        0.02162260189652443,\n        0.0005600639269687235,\n        -0.009946937672793865,\n        -0.02618752419948578,\n        -0.013897350057959557,\n        -0.009690329432487488,\n        0.009845645166933537,\n        0.003646535100415349,\n        -0.003062414238229394,\n        -0.024999024346470833,\n        0.001968453638255596,\n        0.0153964813798666,\n        0.006638044491410255,\n        -0.006604280322790146,\n        0.02349989302456379,\n        0.011885003186762333,\n        -0.018070606514811516,\n        -0.0006478508585132658,\n        0.007353845983743668,\n        0.045325081795454025,\n        -0.0020883167162537575,\n        -0.011756699532270432,\n        0.004082093480974436,\n        0.002177791902795434,\n        0.01245899498462677,\n        0.028226882219314575,\n        -0.0009521508472971618,\n        -0.015153379179537296,\n        -0.0033291515428572893,\n        -0.024202188476920128,\n        0.012384713627398014,\n        -0.0008930634357966483,\n        -0.012891177088022232,\n        0.002405700273811817,\n        -0.004710108041763306,\n        -0.025228621438145638,\n        -0.008224962279200554,\n        -0.02502603456377983,\n        -0.009413463063538074,\n        0.011776957660913467,\n        0.0009504626505076885,\n        -0.005760174710303545,\n        0.007684735115617514,\n        -0.015707112848758698,\n        -0.017395323142409325,\n        0.012627815827727318,\n        -0.003528360277414322,\n        0.03443949669599533,\n        -0.002047799527645111,\n        0.02035306766629219,\n        -0.002650490729138255,\n        0.023243285715579987,\n        -0.030873997136950493,\n        0.009670071303844452,\n        0.0006254820618778467,\n        -0.0008719608304090798,\n        0.01688210666179657,\n        -0.010588457807898521,\n        0.0068541355431079865,\n        -0.01546400971710682,\n        0.011331270448863506,\n        0.02056915871798992,\n        -0.007272811606526375,\n        -0.023905063048005104,\n        0.008170939981937408,\n        0.012114600278437138,\n        -0.006340919528156519,\n        -0.009426968172192574,\n        -0.019299624487757683,\n        0.005405650474131107,\n        0.03965269401669502,\n        0.001683990121819079,\n        0.007090484723448753,\n        -0.005631871055811644,\n        -0.01819215901196003,\n        -0.014856253750622272,\n        0.009838892146945,\n        -0.013208560645580292,\n        0.014559129253029823,\n        0.020663699135184288,\n        -0.017152220010757446,\n        -0.009980701841413975,\n        0.023905063048005104,\n        0.00895426981151104,\n        0.005885102320462465,\n        -0.010244062170386314,\n        -0.005341498646885157,\n        -0.04086820408701897,\n        0.01572061888873577,\n        -0.01038587186485529,\n        -0.011425809934735298,\n        0.013525944203138351,\n        0.005638623610138893,\n        0.00803588330745697,\n        0.028469985350966454,\n        -0.028794120997190475,\n        -0.03687052056193352,\n        0.011939026415348053,\n        -0.006982439663261175,\n        0.022838113829493523,\n        0.01038587186485529,\n        0.009690329432487488,\n        -0.0046256971545517445,\n        -0.009028551168739796,\n        -0.042380839586257935,\n        0.010392624884843826,\n        0.010554693639278412,\n        -0.010615468956530094,\n        -0.011067909188568592,\n        0.0087044145911932,\n        0.011338023468852043,\n        -0.023905063048005104,\n        0.007022956386208534,\n        0.0050680083222687244,\n        0.010264321230351925,\n        -0.03927453234791756,\n        0.004186762496829033,\n        -0.0002825642586685717,\n        0.029928598552942276,\n        0.0072120362892746925,\n        0.006479352712631226,\n        0.002410764805972576,\n        0.014437577687203884,\n        -0.029577450826764107,\n        0.001482249004766345,\n        -0.0041833859868347645,\n        0.011682418175041676,\n        0.008414042182266712,\n        0.016544464975595474,\n        -0.0023212896194308996,\n        -0.03673546388745308,\n        0.02216283045709133,\n        -0.013519191183149815,\n        -0.020380079746246338,\n        -0.013586719520390034,\n        0.02223035879433155,\n        0.008204704150557518,\n        0.011810721829533577,\n        -0.005979642271995544,\n        -0.02183869294822216,\n        0.009426968172192574,\n        0.009832139126956463,\n        0.002562703797593713,\n        -0.0005908737657591701,\n        -0.007056720554828644,\n        -0.013843327760696411,\n        -0.00681361835449934,\n        0.03862626105546951,\n        -0.024026615545153618,\n        0.0035992651246488094,\n        -0.0015751005848869681,\n        -0.005898608360439539,\n        -0.0014273821143433452,\n        -0.0307929627597332,\n        -0.003504725405946374,\n        -0.03230559825897217,\n        0.017638426274061203,\n        -0.0020950695034116507,\n        -0.010649233125150204,\n        0.03000963293015957,\n        -0.00400781212374568,\n        -0.006232874002307653,\n        0.027659643441438675,\n        0.019556231796741486,\n        -0.01035210769623518,\n        -0.017287276685237885,\n        0.013586719520390034,\n        0.01563958451151848,\n        0.01992088556289673,\n        0.007022956386208534,\n        0.014802231453359127,\n        0.011473080143332481,\n        -0.01093960553407669,\n        -0.011000380851328373,\n        -0.03635730594396591,\n        -0.016733543947339058,\n        0.012904682196676731,\n        0.025188103318214417,\n        -0.00637130718678236,\n        -0.02505304664373398,\n        -0.006648173555731773,\n        -0.012303679250180721,\n        -0.008366771973669529,\n        -0.012621062807738781,\n        -0.0029476159252226353,\n        0.01132451742887497,\n        0.011405551806092262,\n        -0.003866002429276705,\n        0.0021541567984968424,\n        -0.01447809487581253,\n        -0.011560866609215736,\n        -0.01476171426475048,\n        0.010196792893111706,\n        -0.01830020360648632,\n        -0.013600225560367107,\n        0.014788725413382053,\n        -0.00618560379371047,\n        -0.03009066730737686,\n        -0.021393006667494774,\n        -0.008839471265673637,\n        -0.023418858647346497,\n        -0.02439126744866371,\n        0.006921663880348206,\n        -0.004537910223007202,\n        -0.003069167025387287,\n        -0.012431983835995197,\n        0.03497972711920738,\n        0.00257114483974874,\n        0.02343236468732357,\n        0.007340339943766594,\n        -0.0170981977134943,\n        -0.006921663880348206,\n        0.0005136380787007511,\n        -0.007853556424379349,\n        -0.020758239552378654,\n        0.0472969114780426,\n        0.01646343059837818,\n        -0.01834072172641754,\n        0.024796439334750175,\n        -0.020690711215138435,\n        -0.0104061309248209,\n        0.020839272066950798,\n        0.007205283269286156,\n        -0.0015995795838534832,\n        0.028307916596531868,\n        -0.007954848930239677,\n        -0.0005482464330270886,\n        0.0008061206317506731,\n        -0.007813039235770702,\n        -0.027159933000802994,\n        -0.002035982208326459,\n        -0.0029357983730733395,\n        -0.011939026415348053,\n        0.009859150275588036,\n        -0.012951952405273914,\n        -0.00621261540800333,\n        0.012641321867704391,\n        -0.015247918665409088,\n        -0.013883844949305058,\n        0.01286416593939066,\n        -0.004331948701292276,\n        0.004152998328208923,\n        -0.0404900461435318,\n        0.006243003066629171,\n        -0.0006841474096290767,\n        -0.02895618975162506,\n        0.014991311356425285,\n        -0.01886744238436222,\n        -0.013688012026250362,\n        -0.00980512797832489,\n        -0.0017110015032812953,\n        -0.0030218970496207476,\n        -0.019907381385564804,\n        -0.003005014965310693,\n        0.030765952542424202,\n        0.007914331741631031,\n        0.002878399332985282,\n        -0.008562604896724224,\n        0.017017163336277008,\n        -0.011743193492293358,\n        0.003190718125551939,\n        0.22646333277225494,\n        -0.011216471903026104,\n        0.006151839625090361,\n        0.009001539088785648,\n        -0.010899088345468044,\n        0.005429285578429699,\n        0.015328953042626381,\n        -0.00955527275800705,\n        0.00781979225575924,\n        0.020218010991811752,\n        0.007542925421148539,\n        -0.0182326752692461,\n        -0.03217054158449173,\n        0.01242523081600666,\n        0.0015075721312314272,\n        0.006320660933852196,\n        -0.029334349557757378,\n        -0.03281881660223007,\n        -0.002746718702837825,\n        -0.035763055086135864,\n        0.013141032308340073,\n        -0.003474337514489889,\n        -0.02059617079794407,\n        -0.02109588123857975,\n        0.04300210252404213,\n        0.009224383160471916,\n        0.004237408749759197,\n        -0.005550836678594351,\n        0.023378342390060425,\n        0.003764709923416376,\n        -0.02020450495183468,\n        0.005577848292887211,\n        -0.0034338205587118864,\n        0.004568298347294331,\n        -0.0202450230717659,\n        0.006631291471421719,\n        -0.007988613098859787,\n        -0.0082654794678092,\n        0.021109387278556824,\n        0.007596948184072971,\n        0.0003009235661011189,\n        0.013249077834188938,\n        -0.016220327466726303,\n        0.008643638342618942,\n        0.013019480742514133,\n        0.018178652971982956,\n        -0.0002962809812743217,\n        -0.03114411048591137,\n        -0.0038086033891886473,\n        0.025525745004415512,\n        -0.019529221579432487,\n        0.00334434537217021,\n        0.002144027501344681,\n        0.037815921008586884,\n        -0.012884424068033695,\n        0.01805710233747959,\n        -0.00022326585894916207,\n        0.0004566609859466553,\n        -0.0013632301706820726,\n        0.031063076108694077,\n        0.006367930676788092,\n        0.00592899601906538,\n        -0.006914910860359669,\n        0.04845840111374855,\n        0.008292490616440773,\n        0.03408835083246231,\n        -0.016260845586657524,\n        0.015409987419843674,\n        0.008292490616440773,\n        -0.03581707924604416,\n        -0.003082672832533717,\n        -0.026822291314601898,\n        -0.0005174365942366421,\n        -0.01453211810439825,\n        -0.018070606514811516,\n        -0.007907578721642494,\n        0.013127526268362999,\n        0.008177693001925945,\n        0.02048812434077263,\n        0.016193317249417305,\n        -0.022676046937704086,\n        -0.03203548491001129,\n        -0.012479253113269806,\n        -0.005344875156879425,\n        0.002648802474141121,\n        -0.023418858647346497,\n        -0.0076644765213131905,\n        -0.0028328176122158766,\n        -0.006681937724351883,\n        -0.008690908551216125,\n        0.025755342096090317,\n        -0.024688392877578735,\n        -0.023972591385245323,\n        -0.010095500387251377,\n        0.008137175813317299,\n        -0.016557971015572548,\n        0.017044175416231155,\n        0.014869759790599346,\n        0.0068541355431079865,\n        0.013012727722525597,\n        0.00861662719398737,\n        0.02441827952861786,\n        0.012344196438789368,\n        0.011351528577506542,\n        -0.01985335722565651,\n        -0.00024162515182979405,\n        0.0002707467938307673,\n        0.023878052830696106,\n        0.0026910079177469015,\n        -0.01692262478172779,\n        0.005000479985028505,\n        -0.03225157782435417,\n        0.0023938827216625214,\n        -0.01985335722565651,\n        0.010264321230351925,\n        -0.021176915615797043,\n        -0.022149324417114258,\n        0.0007200218387879431,\n        0.01751687377691269,\n        -0.02356742136180401,\n        -0.004882305394858122,\n        -0.025255631655454636,\n        0.008360018953680992,\n        0.02039358578622341,\n        0.004943080712109804,\n        -0.024999024346470833,\n        -0.006050547119230032,\n        -0.007299823220819235,\n        0.013168043456971645,\n        -0.004642579238861799,\n        0.01801658421754837,\n        -0.03606018051505089,\n        0.011966037563979626,\n        -0.000476497458294034,\n        -0.0037376985419541597,\n        -0.010682997293770313,\n        0.018138134852051735,\n        -0.011034145019948483,\n        -0.00955527275800705,\n        0.01060196291655302,\n        -0.0024985517375171185,\n        -0.019772322848439217,\n        0.006800112780183554,\n        0.007644217927008867,\n        0.0006339231040328741,\n        -0.016841590404510498,\n        0.009994206950068474,\n        0.001396150211803615,\n        -0.01748986355960369,\n        -0.007016203831881285,\n        -0.013174796476960182,\n        -0.005797315388917923,\n        -0.000915854296181351,\n        -0.012195633724331856,\n        0.0032329235691577196,\n        0.002496863715350628,\n        -0.0006795048248022795,\n        -0.018097618594765663,\n        0.02092030644416809,\n        -0.0033156457357108593,\n        -0.026038961485028267,\n        -0.014491600915789604,\n        0.010865324176847935,\n        0.00835326686501503,\n        -0.013735282234847546,\n        0.015072344802320004,\n        -0.17384518682956696,\n        0.004551416262984276,\n        0.019529221579432487,\n        -0.01777348294854164,\n        0.01758440211415291,\n        -0.021298466250300407,\n        -0.003585759550333023,\n        -0.0009783180430531502,\n        -0.02159559167921543,\n        0.02354040928184986,\n        0.04232681915163994,\n        -0.03114411048591137,\n        -0.021987255662679672,\n        -0.024593854323029518,\n        -0.0013269336195662618,\n        0.004180009476840496,\n        0.007873814553022385,\n        -0.008751683868467808,\n        0.014896770939230919,\n        0.02240593172609806,\n        0.020380079746246338,\n        -0.002083251951262355,\n        0.005000479985028505,\n        0.0038457440678030252,\n        -0.0040044356137514114,\n        -0.002125457162037492,\n        -0.016841590404510498,\n        0.037572816014289856,\n        0.007718499284237623,\n        -0.009636306203901768,\n        0.013525944203138351,\n        0.0022588258143514395,\n        0.013262582942843437,\n        0.016341879963874817,\n        -0.004564921837300062,\n        0.010325096547603607,\n        -0.007650970946997404,\n        -0.029010212048888206,\n        -0.021298466250300407,\n        0.015950214117765427,\n        0.02655217796564102,\n        0.017287276685237885,\n        0.011020638979971409,\n        -0.00978486891835928,\n        -0.01939416490495205,\n        -0.00444337073713541,\n        0.021433522924780846,\n        -0.004791141953319311,\n        0.022878631949424744,\n        -0.04427163675427437,\n        0.01646343059837818,\n        0.010581704787909985,\n        -0.002368559595197439,\n        -0.018462272360920906,\n        0.0182326752692461,\n        0.003511478193104267,\n        0.00946748536080122,\n        0.023594433441758156,\n        -0.014275509864091873,\n        0.0014383555389940739,\n        -0.0005305202212184668,\n        -0.01511286199092865,\n        0.029469406232237816,\n        -0.01343140471726656,\n        0.017395323142409325,\n        -0.003045532153919339,\n        0.01447809487581253,\n        0.00055964186321944,\n        -0.012330691330134869,\n        0.006192356813699007,\n        -0.017476357519626617,\n        -0.006533375475555658,\n        -0.009575530886650085,\n        -0.026228042319417,\n        -0.018840432167053223,\n        0.007954848930239677,\n        -0.027267979457974434,\n        -0.003572253743186593,\n        -0.020218010991811752,\n        0.009231136180460453,\n        -0.004078716970980167,\n        0.03795097768306732,\n        -0.01624733954668045,\n        0.009143348783254623,\n        -0.010608715936541557,\n        -0.0031029311940073967,\n        0.009028551168739796,\n        0.01620682328939438,\n        0.009845645166933537,\n        -0.02749757654964924,\n        0.023986097425222397,\n        -0.03011767938733101,\n        -0.005040997173637152,\n        -0.01504533365368843,\n        0.030522849410772324,\n        0.01447809487581253,\n        0.015301941893994808,\n        0.005587977357208729,\n        -0.0028615170158445835,\n        -0.0005701931659132242,\n        -0.0092716533690691,\n        -0.01659848727285862,\n        0.002939174883067608,\n        -0.008981280960142612,\n        -0.013485427014529705,\n        0.012479253113269806,\n        0.023918569087982178,\n        -0.006209238898009062,\n        0.04005786404013634,\n        -0.0005959383561275899,\n        -0.02046111412346363,\n        0.0011024016421288252,\n        0.022432943806052208,\n        0.01635538600385189,\n        0.0024732286110520363,\n        0.02806481532752514,\n        0.011081415228545666,\n        -0.00823171529918909,\n        0.010959863662719727,\n        0.01545050460845232,\n        0.07476747781038284,\n        0.014086429961025715,\n        -0.009933431632816792,\n        0.0028885283973068,\n        -0.007461891509592533,\n        -0.03795097768306732,\n        -0.09972598403692245,\n        -0.021393006667494774,\n        0.013039739802479744,\n        0.006621162407100201,\n        0.015572055242955685,\n        -0.001142918597906828,\n        0.013586719520390034,\n        -0.010196792893111706,\n        0.013417898677289486,\n        0.021136397495865822,\n        -0.018219169229269028,\n        -0.017638426274061203,\n        -0.015288435854017735,\n        -0.03217054158449173,\n        -0.015436998568475246,\n        -0.007542925421148539,\n        -0.003953789360821247,\n        -0.019475199282169342,\n        -0.0048789288848638535,\n        0.009190618991851807,\n        -0.004926198627799749,\n        -0.012634568847715855,\n        -0.01613929495215416,\n        -0.011405551806092262,\n        -0.01459964644163847,\n        -0.02166312001645565,\n        -0.014005395583808422,\n        0.008873235434293747,\n        0.008724672719836235,\n        0.006455717608332634,\n        -0.0165309589356184,\n        -0.0279297586530447,\n        0.011148943565785885,\n        0.0030404673889279366,\n        0.016260845586657524,\n        0.01666601561009884,\n        -0.03954464569687843,\n        -0.0018435260280966759,\n        0.00558460084721446,\n        -0.010163028724491596,\n        0.0061585926450788975,\n        0.021987255662679672,\n        0.009008292108774185,\n        -0.009177112951874733,\n        0.030063655227422714,\n        -0.01819215901196003,\n        -0.019664278253912926,\n        0.013492180034518242,\n        0.005696022883057594,\n        0.01762492023408413,\n        -0.011013886891305447,\n        -0.002336483681574464,\n        -0.024958506226539612,\n        -0.01180396880954504,\n        0.015193896368145943,\n        -0.018772903829813004,\n        -0.004163127392530441,\n        -0.017192738130688667,\n        -0.008042635396122932,\n        -0.002027540933340788,\n        0.00580406840890646,\n        -0.002078187419101596,\n        -0.01847577840089798,\n        0.03344007581472397,\n        0.03676247596740723,\n        0.0013092074077576399,\n        0.0012180439662188292,\n        -0.008792201057076454,\n        0.02325678989291191,\n        -0.005628494545817375,\n        -0.023580927401781082,\n        0.02467488683760166,\n        0.006226120982319117,\n        -0.010912594385445118,\n        -0.023770006373524666,\n        0.0002443685079924762,\n        -0.015990732237696648,\n        0.015085850842297077,\n        -0.01897548884153366,\n        -0.011020638979971409,\n        -0.036789488047361374,\n        -0.009933431632816792,\n        -0.007826544344425201,\n        -0.015315447002649307,\n        -0.0008293334976769984,\n        -0.012783131562173367,\n        -0.03271077200770378,\n        -0.042029693722724915,\n        0.012330691330134869,\n        -0.02930733747780323,\n        -0.012661579996347427,\n        0.026038961485028267,\n        0.03606018051505089,\n        0.013249077834188938,\n        -0.010750525631010532,\n        0.015004816465079784,\n        -0.0024749168660491705,\n        -0.018664857372641563,\n        2.4663702788529918e-05,\n        0.010392624884843826,\n        0.014275509864091873,\n        -0.003653287887573242,\n        -0.03522282838821411,\n        -0.005581224337220192,\n        0.006546881049871445,\n        -0.003381486050784588,\n        -0.014221486635506153,\n        -0.013066750951111317,\n        0.006891276221722364,\n        -0.005915490444749594,\n        -0.015693606808781624,\n        -0.0026791903655976057,\n        -0.007239047437906265,\n        0.006837253458797932,\n        0.008947516791522503,\n        0.009345934726297855,\n        -0.014207981526851654,\n        -0.013444909825921059,\n        0.025809364393353462,\n        0.0012138234451413155,\n        -0.003781592007726431,\n        0.007468644063919783,\n        -0.0076644765213131905,\n        -0.0014408878050744534,\n        0.014748208224773407,\n        0.0005887635052204132,\n        -0.0048316591419279575,\n        -0.01208758819848299,\n        0.008684155531227589,\n        -0.00230103125795722,\n        -0.028469985350966454,\n        -0.011594630777835846,\n        0.019556231796741486,\n        -0.00863688625395298,\n        0.004618944600224495,\n        0.0279567688703537,\n        -0.002799053443595767,\n        -0.012296927161514759,\n        -0.0012433672090992332,\n        0.019839851185679436,\n        0.011783710680902004,\n        -0.005625118035823107,\n        -0.015261424705386162,\n        -0.028713086619973183,\n        0.004186762496829033,\n        -0.02644413150846958,\n        0.013444909825921059,\n        -0.0031198132783174515,\n        -0.025188103318214417,\n        -0.00018749690207187086,\n        0.02718694508075714,\n        -0.013870338909327984,\n        0.0148292426019907,\n        0.014842748641967773,\n        0.0004252180806361139,\n        0.006050547119230032,\n        -0.008738178759813309,\n        -0.02884814515709877,\n        0.007293070200830698,\n        0.008170939981937408,\n        0.008812460117042065,\n        -0.016017742455005646,\n        0.03298088535666466,\n        0.012634568847715855,\n        -0.00032899004872888327,\n        -0.0006440524011850357,\n        0.03352111205458641,\n        0.0020680581219494343,\n        -0.013262582942843437,\n        0.005554213188588619,\n        -0.002520498586818576,\n        -0.03238663449883461,\n        -0.0017321042250841856,\n        -0.011216471903026104,\n        0.009562025777995586,\n        -0.0026049090083688498,\n        0.04267796501517296,\n        0.0037174399476498365,\n        0.02039358578622341,\n        0.004679719917476177,\n        -0.028767110779881477,\n        0.026781775057315826,\n        0.009190618991851807,\n        -0.020717721432447433,\n        -0.01563958451151848,\n        0.009285158477723598,\n        0.04705381020903587,\n        0.002957745222374797,\n        -0.018840432167053223,\n        0.017678942531347275,\n        -0.004409606568515301,\n        0.00017958341049961746,\n        0.008785448037087917,\n        0.008684155531227589,\n        0.011939026415348053,\n        -0.007495655678212643,\n        0.008758436888456345,\n        0.013357123360037804,\n        -0.0028682700358331203,\n        0.013600225560367107,\n        0.01812463067471981,\n        0.01072351448237896,\n        0.012310432270169258,\n        0.0028699582908302546,\n        -0.01950220949947834,\n        -0.0014839372597634792,\n        -0.01286416593939066,\n        -0.0007060941425152123,\n        -0.008360018953680992,\n        -0.02208179607987404,\n        -0.010041477158665657,\n        0.022838113829493523,\n        0.023175757378339767,\n        0.00923788920044899,\n        0.0267952810972929,\n        0.01242523081600666,\n        -0.027200451120734215,\n        0.011128684505820274,\n        0.010966616682708263,\n        -0.0065164933912456036,\n        -0.014842748641967773,\n        0.007286317180842161,\n        -0.002145715756341815,\n        -0.0007896605529822409,\n        0.04324520379304886,\n        -0.016125788912177086,\n        0.02133898250758648,\n        -0.006249756086617708,\n        0.007752263452857733,\n        -0.033034905791282654,\n        0.008042635396122932,\n        0.009305417537689209,\n        0.01600423827767372,\n        -0.010568198747932911,\n        -0.0014366672839969397,\n        -0.0142484987154603,\n        0.008056141436100006,\n        0.000470588740427047,\n        0.0022115560714155436,\n        0.05064631998538971,\n        -0.02244644984602928,\n        0.03338605538010597,\n        0.0041462453082203865,\n        0.0004359804152045399,\n        -0.027848724275827408,\n        0.007455138489603996,\n        0.01488326583057642,\n        0.011662159115076065,\n        -0.001961700851097703,\n        -0.00851533468812704,\n        0.0005845429259352386,\n        0.00053220841800794,\n        0.007083732169121504,\n        0.016625499352812767,\n        -0.0165309589356184,\n        -0.019299624487757683,\n        0.02399960346519947,\n        -0.01129075326025486,\n        0.018003078177571297,\n        -0.002427646890282631,\n        -0.005922242999076843,\n        0.02828090637922287,\n        -0.007117496337741613,\n        0.009575530886650085,\n        -0.01006848830729723,\n        -0.016557971015572548,\n        -0.01585567556321621,\n        0.011533855460584164,\n        -0.014113441109657288,\n        -0.020731227472424507,\n        -0.04116532951593399,\n        0.03522282838821411,\n        0.0038761317264288664,\n        -0.022568000480532646,\n        -0.0033899270929396152,\n        -0.0046999785117805,\n        -0.022554494440555573,\n        0.0003549462999217212,\n        0.003869378939270973,\n        0.019907381385564804,\n        0.029361359775066376,\n        0.003609394421800971,\n        0.03090100921690464,\n        -0.01488326583057642,\n        -0.03430444002151489,\n        -0.010608715936541557,\n        -0.0013564772671088576,\n        -0.007529419846832752,\n        -0.014180969446897507,\n        -0.020690711215138435\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"ada\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 7,\n    \"total_tokens\": 7\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"xrequestid":"ba98f534-0803-40f6-a028-24dfd031f5eb","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-49333/49334","correlationid":"ba98f534-0803-40f6-a028-24dfd031f5eb"}
{"specversion":"1.0","id":"43d2493b-33d2-4cae-887c-430662155df8","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"Prompt_variants","context":{"request_id":"0xccf497116a5a912b29e55229bbac0ba8","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x1a77eee854345f20","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x7fd179134fb9e709","start_time":"2024-02-05T15:00:17.675380Z","end_time":"2024-02-05T15:00:17.677929Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"render_template_jinja2","node_name":"Prompt_variants","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"template\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n {{contexts}} \\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\nuser: {{question}} \\nassistant:\",\n  \"chat_history\": [],\n  \"contexts\": \"Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n  \"question\": \"what's Attention?\"\n}","output":"\"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\nuser: what's Attention? \\nassistant:\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-9201/9202","correlationid":"81134217-0100-463c-943e-e739c36c902c","xrequestid":"81134217-0100-463c-943e-e739c36c902c","modelversion":"default"}
{"specversion":"1.0","id":"9be18d14-3fac-4942-b5e1-9ccda58d8c4e","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"search","context":{"request_id":"0xccf497116a5a912b29e55229bbac0ba8","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x1bb2aa9e6b530b09","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xd25a6f7c89885b7f","start_time":"2024-02-05T15:00:16.775105Z","end_time":"2024-02-05T15:00:17.620704Z","status":{"status_code":"UNSET"},"attributes":{"span_type":"Retrieval","retrieval.query":"What is the meaning of Attention?","retrieval.documents":"[\n    {\n        \"document.content\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n        \"document.score\": 0.39159566164016724,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf13\",\n            \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n            \"mtime\": null,\n            \"page_number\": 13,\n            \"stats\": {\n                \"tiktokens\": 304,\n                \"chars\": 835,\n                \"lines\": 113\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"document.score\": 0.39190346002578735,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf12\",\n            \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n            \"mtime\": null,\n            \"page_number\": 12,\n            \"stats\": {\n                \"tiktokens\": 254,\n                \"chars\": 833,\n                \"lines\": 73\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n        \"document.score\": 0.42808181047439575,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf14\",\n            \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n            \"mtime\": null,\n            \"page_number\": 14,\n            \"stats\": {\n                \"tiktokens\": 291,\n                \"chars\": 838,\n                \"lines\": 113\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    }\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"correlationid":"1c16dc7d-c10e-4d43-9b48-b9be92e2b4f8","xrequestid":"1c16dc7d-c10e-4d43-9b48-b9be92e2b4f8","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-6141/6142"}
{"specversion":"1.0","id":"bcf9fd6a-694b-43fc-a690-7bb16764adba","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"openai.resources.chat.completions.Completions.create","context":{"request_id":"0xccf497116a5a912b29e55229bbac0ba8","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x52355639a5440992","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xcda5d0e89f4747df","start_time":"2024-02-05T15:00:17.682481Z","end_time":"2024-02-05T15:00:18.787224Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.chat.completions.Completions.create","node_name":"answer_the_question_with_context","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\nuser: what's Attention? \\nassistant:\"\n    }\n  ],\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8oukE8DGO1ZnKNpbqZMpApVzxy0bX\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707145218,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 58,\n    \"prompt_tokens\": 1110,\n    \"total_tokens\": 1168\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-6179/6180","correlationid":"b1ffd289-d325-477e-a3d5-84adf6cb34c8","xrequestid":"b1ffd289-d325-477e-a3d5-84adf6cb34c8","modelversion":"default"}
{"specversion":"1.0","id":"b55484ce-43a0-4f68-a5d4-4a592c64bfea","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"vector_lookup","context":{"request_id":"0xccf497116a5a912b29e55229bbac0ba8","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0xd25a6f7c89885b7f","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x7fd179134fb9e709","start_time":"2024-02-05T15:00:14.441572Z","end_time":"2024-02-05T15:00:17.625517Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"search","node_name":"vector_lookup","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"mlindex_content\": \"embeddings:\\n  api_base: https:\/\/promptflow-ci-weu.openai.azure.com\/\\n  api_type: azure\\n  api_version: 2023-03-15-preview\\n  batch_size: '16'\\n  connection:\\n    id: \\n      \/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourceGroups\/promptflow\/providers\/Microsoft.MachineLearningServices\/workspaces\/pf-xp\/connections\/azure_open_ai_connection\\n  connection_type: workspace_connection\\n  deployment: text-embedding-ada-003\\n  dimension: 1536\\n  file_format_version: '2'\\n  kind: open_ai\\n  model: text-embedding-ada-002\\n  schema_version: '2'\\nindex:\\n  engine: langchain.vectorstores.FAISS\\n  kind: faiss\\n  method: FlatL2\\n  path: \\n    azureml:\/\/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourcegroups\/promptflow\/workspaces\/pf-xp\/datastores\/workspaceblobstore\/paths\/azureml\/f7ee0381-06fc-4566-820f-86ba174db987\/index\/\\nself:\\n  path: \\n    azureml:\/\/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourcegroups\/promptflow\/workspaces\/pf-xp\/datastores\/workspaceblobstore\/paths\/azureml\/f7ee0381-06fc-4566-820f-86ba174db987\/index\/\\n  asset_id: \\n    azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/serene-honey-h9lrkyczzk\/versions\/1\\n\",\n  \"queries\": \"What is the meaning of Attention?\",\n  \"query_type\": \"Vector\",\n  \"top_k\": 3\n}","output":"[\n  {\n    \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n    \"metadata\": {\n      \"source_doc_id\": \"1706.03762.pdf13\",\n      \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n      \"mtime\": null,\n      \"page_number\": 13,\n      \"stats\": {\n        \"tiktokens\": 304,\n        \"chars\": 835,\n        \"lines\": 113\n      },\n      \"source\": {\n        \"filename\": \"1706.03762.pdf\",\n        \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n        \"mtime\": 1706775620.0\n      }\n    },\n    \"score\": 0.39159566164016724\n  },\n  {\n    \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n    \"metadata\": {\n      \"source_doc_id\": \"1706.03762.pdf12\",\n      \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n      \"mtime\": null,\n      \"page_number\": 12,\n      \"stats\": {\n        \"tiktokens\": 254,\n        \"chars\": 833,\n        \"lines\": 73\n      },\n      \"source\": {\n        \"filename\": \"1706.03762.pdf\",\n        \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n        \"mtime\": 1706775620.0\n      }\n    },\n    \"score\": 0.39190346002578735\n  },\n  {\n    \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n    \"metadata\": {\n      \"source_doc_id\": \"1706.03762.pdf14\",\n      \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n      \"mtime\": null,\n      \"page_number\": 14,\n      \"stats\": {\n        \"tiktokens\": 291,\n        \"chars\": 838,\n        \"lines\": 113\n      },\n      \"source\": {\n        \"filename\": \"1706.03762.pdf\",\n        \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n        \"mtime\": 1706775620.0\n      }\n    },\n    \"score\": 0.42808181047439575\n  }\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-7205/7206","correlationid":"c8f691e6-376b-4ad0-973a-f5c1a1882b57","xrequestid":"c8f691e6-376b-4ad0-973a-f5c1a1882b57","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"7945cce6-a17f-4710-86ce-d6fa6c9f7c1f","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"answer_the_question_with_context","context":{"request_id":"0xccf497116a5a912b29e55229bbac0ba8","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0xcda5d0e89f4747df","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x7fd179134fb9e709","start_time":"2024-02-05T15:00:17.680046Z","end_time":"2024-02-05T15:00:18.788089Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"AzureOpenAI.chat","node_name":"answer_the_question_with_context","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"prompt\": \"{{prompt_text}}\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"prompt_text\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\nuser: what's Attention? \\nassistant:\",\n  \"stream\": false\n}","output":"\"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-5386/5387","correlationid":"566b7a59-9c2a-4756-ae2f-446a7f2cd9b6","xrequestid":"566b7a59-9c2a-4756-ae2f-446a7f2cd9b6","agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame"}
{"specversion":"1.0","id":"37730a53-a3fe-425d-98c9-a0128b271d30","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"generate_prompt_context","context":{"request_id":"0xccf497116a5a912b29e55229bbac0ba8","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x3d101a004d88e6d0","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x7fd179134fb9e709","start_time":"2024-02-05T15:00:17.668465Z","end_time":"2024-02-05T15:00:17.672230Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"generate_prompt_context","node_name":"generate_prompt_context","flow_id":"","root_run_id":"","line_run_id":"6663adb5-bec1-4e57-b3fc-88b2ce82eb11","inputs":"{\n  \"search_result\": [\n    {\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf13\",\n        \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n        \"mtime\": null,\n        \"page_number\": 13,\n        \"stats\": {\n          \"tiktokens\": 304,\n          \"chars\": 835,\n          \"lines\": 113\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.39159566164016724\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf12\",\n        \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n        \"mtime\": null,\n        \"page_number\": 12,\n        \"stats\": {\n          \"tiktokens\": 254,\n          \"chars\": 833,\n          \"lines\": 73\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.39190346002578735\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf14\",\n        \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n        \"mtime\": null,\n        \"page_number\": 14,\n        \"stats\": {\n          \"tiktokens\": 291,\n          \"chars\": 838,\n          \"lines\": 113\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.42808181047439575\n    }\n  ]\n}","output":"\"Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-9664/9665","correlationid":"a8d25f2e-2089-4c40-8b8d-5288feaf4700","xrequestid":"a8d25f2e-2089-4c40-8b8d-5288feaf4700"}
{"specversion":"1.0","id":"6fee5e87-e702-40f5-9d5b-460e683b3f6e","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:22Z","data":[{"name":"promptflow.flow","context":{"request_id":"0xccf497116a5a912b29e55229bbac0ba8","trace_id":"0xccf497116a5a912b29e55229bbac0ba8","span_id":"0x7fd179134fb9e709","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":null,"start_time":"2024-02-05T15:00:13.789782Z","end_time":"2024-02-05T15:00:18.791564Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Flow","inputs":"{\n  \"question\": \"what's Attention?\",\n  \"chat_history\": []\n}","output":"{\n  \"output\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\"\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-867/868","correlationid":"ee3c0e82-7edf-480e-9c24-34e19f4f7f1d","xrequestid":"ee3c0e82-7edf-480e-9c24-34e19f4f7f1d","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"aeee92d8-c6f5-4948-bf4d-4a1b888fce9a","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:37Z","data":[{"name":"openai.resources.chat.completions.Completions.create","context":{"request_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","trace_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","span_id":"0x05e7abd1ef928fa1","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xa8317030c4283bfb","start_time":"2024-02-05T15:00:34.172071Z","end_time":"2024-02-05T15:00:34.829852Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.chat.completions.Completions.create","node_name":"modify_query_with_history","flow_id":"","root_run_id":"","line_run_id":"371856bd-457d-490c-8f4e-69f825b727d1","inputs":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Given the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history:\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"what's Attention?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Follow up Input: how to calculate Attention? \\nStandalone Question:\"\n    }\n  ],\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": null,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8oukUwRal4goGohKqXRV5UJkyL5V1\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"How can attention be calculated in machine learning models?\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707145234,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 10,\n    \"prompt_tokens\": 151,\n    \"total_tokens\": 161\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default","contentrange":"bytes 0-2622/2623","correlationid":"30c749b2-d62f-4f21-84b2-a60ac3590858","xrequestid":"30c749b2-d62f-4f21-84b2-a60ac3590858"}
{"specversion":"1.0","id":"ab359fb7-cb09-47e2-a701-285f7ba16c96","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:37Z","data":[{"name":"vector_lookup","context":{"request_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","trace_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","span_id":"0x76c01ab723e2980f","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xb1e57bf254752dd7","start_time":"2024-02-05T15:00:34.836506Z","end_time":"2024-02-05T15:00:35.383484Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"search","node_name":"vector_lookup","flow_id":"","root_run_id":"","line_run_id":"371856bd-457d-490c-8f4e-69f825b727d1","inputs":"{\n  \"mlindex_content\": \"embeddings:\\n  api_base: https:\/\/promptflow-ci-weu.openai.azure.com\/\\n  api_type: azure\\n  api_version: 2023-03-15-preview\\n  batch_size: '16'\\n  connection:\\n    id: \\n      \/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourceGroups\/promptflow\/providers\/Microsoft.MachineLearningServices\/workspaces\/pf-xp\/connections\/azure_open_ai_connection\\n  connection_type: workspace_connection\\n  deployment: text-embedding-ada-003\\n  dimension: 1536\\n  file_format_version: '2'\\n  kind: open_ai\\n  model: text-embedding-ada-002\\n  schema_version: '2'\\nindex:\\n  engine: langchain.vectorstores.FAISS\\n  kind: faiss\\n  method: FlatL2\\n  path: \\n    azureml:\/\/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourcegroups\/promptflow\/workspaces\/pf-xp\/datastores\/workspaceblobstore\/paths\/azureml\/f7ee0381-06fc-4566-820f-86ba174db987\/index\/\\nself:\\n  path: \\n    azureml:\/\/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourcegroups\/promptflow\/workspaces\/pf-xp\/datastores\/workspaceblobstore\/paths\/azureml\/f7ee0381-06fc-4566-820f-86ba174db987\/index\/\\n  asset_id: \\n    azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/serene-honey-h9lrkyczzk\/versions\/1\\n\",\n  \"queries\": \"How can attention be calculated in machine learning models?\",\n  \"query_type\": \"Vector\",\n  \"top_k\": 3\n}","output":"[\n  {\n    \"text\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n    \"metadata\": {\n      \"source_doc_id\": \"1706.03762.pdf3\",\n      \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n      \"mtime\": null,\n      \"page_number\": 3,\n      \"stats\": {\n        \"tiktokens\": 551,\n        \"chars\": 2502,\n        \"lines\": 35\n      },\n      \"source\": {\n        \"filename\": \"1706.03762.pdf\",\n        \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n        \"mtime\": 1706775620.0\n      }\n    },\n    \"score\": 0.3431832194328308\n  },\n  {\n    \"text\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n    \"metadata\": {\n      \"source_doc_id\": \"1706.03762.pdf0\",\n      \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n      \"mtime\": null,\n      \"page_number\": 0,\n      \"stats\": {\n        \"tiktokens\": 668,\n        \"chars\": 2874,\n        \"lines\": 50\n      },\n      \"source\": {\n        \"filename\": \"1706.03762.pdf\",\n        \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n        \"mtime\": 1706775620.0\n      }\n    },\n    \"score\": 0.3687279224395752\n  },\n  {\n    \"text\": \"Title: 1706.03762.pdfoutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni\\u2208Rdmodel\\u00d7dk,WK\\ni\\u2208Rdmodel\\u00d7dk,WV\\ni\\u2208Rdmodel\\u00d7dv\\nandWO\\u2208Rhdv\\u00d7dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel\/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\u2022In \\\"encoder-decoder attention\\\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\u2212\\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\\u221admodel.\\n5\",\n    \"metadata\": {\n      \"source_doc_id\": \"1706.03762.pdf4\",\n      \"chunk_hash\": \"b0907ffb4546970af003d72d3c66917f5f097f5428599ff50f68323e538f0da6\",\n      \"mtime\": null,\n      \"page_number\": 4,\n      \"stats\": {\n        \"tiktokens\": 743,\n        \"chars\": 3190,\n        \"lines\": 49\n      },\n      \"source\": {\n        \"filename\": \"1706.03762.pdf\",\n        \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n        \"mtime\": 1706775620.0\n      }\n    },\n    \"score\": 0.3867444396018982\n  }\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-13116/13117","correlationid":"958f5acc-5cd4-4f9d-a6d7-921e1c351d34","xrequestid":"958f5acc-5cd4-4f9d-a6d7-921e1c351d34"}
{"specversion":"1.0","id":"3f9e3ee4-c846-487a-85bf-10b161ed14c0","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:37Z","data":[{"name":"modify_query_with_history","context":{"request_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","trace_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","span_id":"0xa8317030c4283bfb","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xb1e57bf254752dd7","start_time":"2024-02-05T15:00:34.168947Z","end_time":"2024-02-05T15:00:34.831846Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"AzureOpenAI.chat","node_name":"modify_query_with_history","flow_id":"","root_run_id":"","line_run_id":"371856bd-457d-490c-8f4e-69f825b727d1","inputs":"{\n  \"prompt\": \"system: \\nGiven the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\n\\nuser:\\nFollow up Input: {{question}} \\nStandalone Question:\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"chat_history\": [\n    {\n      \"inputs\": {\n        \"question\": \"what's Attention?\"\n      },\n      \"outputs\": {\n        \"output\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\"\n      }\n    }\n  ],\n  \"question\": \"how to calculate Attention?\"\n}","output":"\"How can attention be calculated in machine learning models?\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-1917/1918","correlationid":"44528925-4948-4250-b927-a71711a8c699","xrequestid":"44528925-4948-4250-b927-a71711a8c699","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"7f7cd828-8b95-4a75-8731-f91b123059bf","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:37Z","data":[{"name":"generate_prompt_context","context":{"request_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","trace_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","span_id":"0xfa84e7b78cf74329","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xb1e57bf254752dd7","start_time":"2024-02-05T15:00:35.428012Z","end_time":"2024-02-05T15:00:35.432549Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"generate_prompt_context","node_name":"generate_prompt_context","flow_id":"","root_run_id":"","line_run_id":"371856bd-457d-490c-8f4e-69f825b727d1","inputs":"{\n  \"search_result\": [\n    {\n      \"text\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf3\",\n        \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n        \"mtime\": null,\n        \"page_number\": 3,\n        \"stats\": {\n          \"tiktokens\": 551,\n          \"chars\": 2502,\n          \"lines\": 35\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.3431832194328308\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf0\",\n        \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n        \"mtime\": null,\n        \"page_number\": 0,\n        \"stats\": {\n          \"tiktokens\": 668,\n          \"chars\": 2874,\n          \"lines\": 50\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.3687279224395752\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfoutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni\\u2208Rdmodel\\u00d7dk,WK\\ni\\u2208Rdmodel\\u00d7dk,WV\\ni\\u2208Rdmodel\\u00d7dv\\nandWO\\u2208Rhdv\\u00d7dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel\/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\u2022In \\\"encoder-decoder attention\\\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\u2212\\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\\u221admodel.\\n5\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf4\",\n        \"chunk_hash\": \"b0907ffb4546970af003d72d3c66917f5f097f5428599ff50f68323e538f0da6\",\n        \"mtime\": null,\n        \"page_number\": 4,\n        \"stats\": {\n          \"tiktokens\": 743,\n          \"chars\": 3190,\n          \"lines\": 49\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.3867444396018982\n    }\n  ]\n}","output":"\"Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfoutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni\\u2208Rdmodel\\u00d7dk,WK\\ni\\u2208Rdmodel\\u00d7dk,WV\\ni\\u2208Rdmodel\\u00d7dv\\nandWO\\u2208Rhdv\\u00d7dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel\/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\u2022In \\\"encoder-decoder attention\\\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\u2212\\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\\u221admodel.\\n5\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-21442/21443","correlationid":"e0273049-fad7-47de-a5f4-df0247d20d54","xrequestid":"e0273049-fad7-47de-a5f4-df0247d20d54","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"7a0b035b-1d4a-4a03-bccb-16190ba74e9a","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:37Z","data":[{"name":"openai.resources.embeddings.Embeddings.create","context":{"request_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","trace_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","span_id":"0xed3c5ea2433ffb9e","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xf853c85fdfd565fc","start_time":"2024-02-05T15:00:34.838300Z","end_time":"2024-02-05T15:00:35.375385Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.embeddings.Embeddings.create","node_name":"vector_lookup","flow_id":"","root_run_id":"","line_run_id":"371856bd-457d-490c-8f4e-69f825b727d1","inputs":"{\n  \"input\": [\n    [\n      4438,\n      649,\n      6666,\n      387,\n      16997,\n      304,\n      5780,\n      6975,\n      4211,\n      30\n    ]\n  ],\n  \"model\": \"text-embedding-ada-002\"\n}","output":"{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.03242553398013115,\n        0.016978833824396133,\n        0.04613117128610611,\n        -0.021240953356027603,\n        0.00824566837400198,\n        0.002475789748132229,\n        -0.00796013418585062,\n        -0.0012918678112328053,\n        -0.023204870522022247,\n        -0.016755977645516396,\n        0.03791335970163345,\n        0.04351261258125305,\n        -0.020502742379903793,\n        0.020837025716900826,\n        0.0047530983574688435,\n        -0.0031774374656379223,\n        0.022104518488049507,\n        0.022856658324599266,\n        0.001187404035590589,\n        -0.02186773531138897,\n        -0.041200485080480576,\n        0.01962524652481079,\n        0.0078695984557271,\n        -0.015780983492732048,\n        -0.018998464569449425,\n        0.016045624390244484,\n        0.018970608711242676,\n        -0.025335930287837982,\n        -0.009791730903089046,\n        -0.026352709159255028,\n        0.019374534487724304,\n        -0.0050490787252783775,\n        -0.010474227368831635,\n        -0.03468194976449013,\n        -0.018859179690480232,\n        0.015223843045532703,\n        0.020837025716900826,\n        -0.000933209084905684,\n        0.02447236329317093,\n        -0.016254551708698273,\n        0.03292695805430412,\n        0.030308401212096214,\n        -0.004157655406743288,\n        0.003031188389286399,\n        0.014360276982188225,\n        0.017744900658726692,\n        0.014019028283655643,\n        -0.030642686411738396,\n        -0.013642959296703339,\n        0.022633802145719528,\n        0.006379249971359968,\n        0.03782978653907776,\n        -0.025280214846134186,\n        -0.023232726380228996,\n        -0.0056514861062169075,\n        0.008433702401816845,\n        0.024528076872229576,\n        0.00983351655304432,\n        0.014318491332232952,\n        -0.012347609736025333,\n        0.0259487833827734,\n        -0.007270673755556345,\n        0.0008191695087589324,\n        0.01128904428333044,\n        -0.023441653698682785,\n        0.002096238313242793,\n        0.003130428958684206,\n        0.027299847453832626,\n        0.015836697071790695,\n        0.007890491746366024,\n        0.024792717769742012,\n        0.014931345358490944,\n        -0.0024096292909234762,\n        -0.01685347780585289,\n        0.03256481885910034,\n        -0.006233000662177801,\n        -0.014666703529655933,\n        -0.01817668415606022,\n        0.020057030022144318,\n        0.02334415540099144,\n        0.0003349367470946163,\n        0.001270104548893869,\n        -0.002421816810965538,\n        0.028971266001462936,\n        0.0025715481024235487,\n        -0.006772730033844709,\n        0.01924917846918106,\n        0.00851727370172739,\n        -0.0013676040107384324,\n        0.015154200606048107,\n        -0.0007469154661521316,\n        -0.021937377750873566,\n        0.02501557394862175,\n        0.00569327175617218,\n        0.0027856987435370684,\n        0.00991708692163229,\n        -0.012897784821689129,\n        0.03262053057551384,\n        -0.011344757862389088,\n        -0.009346019476652145,\n        -0.01649133674800396,\n        0.022564159706234932,\n        -0.050142575055360794,\n        -0.015683483332395554,\n        0.0019151679007336497,\n        -0.00038238067645579576,\n        0.004690420348197222,\n        -0.006922461092472076,\n        -0.012556537054479122,\n        0.015850625932216644,\n        -0.008635666221380234,\n        0.02835841290652752,\n        0.002049229573458433,\n        -0.03445909172296524,\n        -0.02185380645096302,\n        0.0013754387618973851,\n        -0.0021589165553450584,\n        -0.01778668537735939,\n        -0.03189624845981598,\n        -0.002952840644866228,\n        0.03039197251200676,\n        0.02133845165371895,\n        0.01722954586148262,\n        -0.022912371903657913,\n        0.0070687104016542435,\n        0.026088068261742592,\n        0.007632814347743988,\n        -0.017354903742671013,\n        -0.015377056784927845,\n        -0.026394495740532875,\n        -0.00027508774655871093,\n        0.02110166847705841,\n        -0.014025992713868618,\n        0.017480259761214256,\n        -0.010731903836131096,\n        0.006591659504920244,\n        0.011839219368994236,\n        0.009443518705666065,\n        -0.04482189193367958,\n        0.001659231842495501,\n        -0.0005701977061107755,\n        0.003271454945206642,\n        0.003680604510009289,\n        -0.009965837001800537,\n        -0.008496381342411041,\n        0.04774687439203262,\n        -0.0010742350714281201,\n        0.03092125616967678,\n        -0.0009828293696045876,\n        0.001983069349080324,\n        -0.003698015119880438,\n        -0.021784164011478424,\n        -0.0024183345958590508,\n        0.02372022531926632,\n        0.002155434340238571,\n        0.004714794922620058,\n        -0.0032975708600133657,\n        0.011456185951828957,\n        -0.0017802355578169227,\n        -0.020711669698357582,\n        0.017717042937874794,\n        0.019555604085326195,\n        -0.013064926490187645,\n        -0.018789537250995636,\n        0.031171968206763268,\n        0.02983483299612999,\n        0.024040579795837402,\n        -0.01906810700893402,\n        -0.00750049389898777,\n        -0.006769247818738222,\n        -0.006636927369982004,\n        0.03356767073273659,\n        -0.03002983145415783,\n        -0.003722389927133918,\n        -0.006525499280542135,\n        -0.0018228916451334953,\n        0.045824743807315826,\n        -0.012633143924176693,\n        -0.00638969661667943,\n        -0.023079514503479004,\n        0.00015506330237258226,\n        0.0115745784714818,\n        -0.005292827729135752,\n        0.006156394258141518,\n        -0.010787618346512318,\n        -0.013740458525717258,\n        0.0019430248066782951,\n        -0.01546062808483839,\n        0.003798996564000845,\n        -0.02057238481938839,\n        0.014485633000731468,\n        0.010348870418965816,\n        -0.022828800603747368,\n        -0.01853882521390915,\n        -0.6257793307304382,\n        -0.01268885750323534,\n        -0.008962985128164291,\n        -0.016226695850491524,\n        0.002757841721177101,\n        0.014610989950597286,\n        0.008001919835805893,\n        -0.006703087594360113,\n        -0.016588835045695305,\n        0.031171968206763268,\n        -0.0012178726028651,\n        -0.00945048313587904,\n        -0.007625850383192301,\n        -0.008545130491256714,\n        -0.006142465863376856,\n        -0.03273196145892143,\n        0.006877193693071604,\n        -0.03721693530678749,\n        -0.002716056304052472,\n        0.02557271346449852,\n        -0.04239833354949951,\n        0.029026979580521584,\n        -0.020488813519477844,\n        0.01870596595108509,\n        -0.013016177341341972,\n        -0.010697082616388798,\n        -0.001234412775374949,\n        -0.020990239456295967,\n        0.00547041604295373,\n        0.007389065809547901,\n        0.003456007456406951,\n        -0.006497642025351524,\n        0.010878153145313263,\n        0.01743847317993641,\n        0.03629765287041664,\n        -0.028079843148589134,\n        -0.01167207770049572,\n        0.02114345319569111,\n        -0.020238101482391357,\n        0.04838758707046509,\n        -0.014931345358490944,\n        -0.021268809214234352,\n        0.017647400498390198,\n        -0.010704047046601772,\n        -0.013092784211039543,\n        -0.004081048537045717,\n        0.004920240491628647,\n        0.009443518705666065,\n        -0.004286494106054306,\n        -0.030698399990797043,\n        -0.012222252786159515,\n        0.004610331263393164,\n        -0.006539427675306797,\n        -0.010704047046601772,\n        0.01316242665052414,\n        0.0016461737686768174,\n        0.015808839350938797,\n        0.0050490787252783775,\n        0.01706240512430668,\n        -0.03128339722752571,\n        -0.010397620499134064,\n        0.006609070114791393,\n        -0.014005100354552269,\n        -0.010620476678013802,\n        -0.008844593539834023,\n        0.015808839350938797,\n        0.002947617555037141,\n        0.020447028800845146,\n        0.0003941328322980553,\n        -0.032341960817575455,\n        0.005285863298922777,\n        0.01814882643520832,\n        -0.003840781981125474,\n        0.028093772009015083,\n        0.010759761556982994,\n        0.019221320748329163,\n        0.023789867758750916,\n        0.006313089746981859,\n        -0.001743673230521381,\n        0.004467564169317484,\n        -0.002143247053027153,\n        -0.01204118225723505,\n        -0.0008352743461728096,\n        -0.014005100354552269,\n        -0.0044466713443398476,\n        0.004471046384423971,\n        0.00443970737978816,\n        0.002522798487916589,\n        0.001045507495291531,\n        -0.00662996293976903,\n        0.012751535512506962,\n        0.028999123722314835,\n        -0.012772428803145885,\n        -0.05276113376021385,\n        0.0036701580975204706,\n        0.01165814884006977,\n        0.003628372447565198,\n        0.004941132850944996,\n        0.027007348835468292,\n        -0.023957008495926857,\n        -0.02206273376941681,\n        -0.020460957661271095,\n        0.0030851613264530897,\n        0.0032871244475245476,\n        0.031784821301698685,\n        -0.005334612913429737,\n        0.0031495806761085987,\n        0.014186170883476734,\n        0.012333680875599384,\n        -0.02983483299612999,\n        -0.01555812731385231,\n        0.0032575263176113367,\n        -0.006877193693071604,\n        0.014708489179611206,\n        0.011825291439890862,\n        -0.027898771688342094,\n        0.030419830232858658,\n        0.005745503585785627,\n        0.0027143151964992285,\n        0.01798168569803238,\n        0.007716385181993246,\n        0.0041681015864014626,\n        0.03259267657995224,\n        -0.03242553398013115,\n        -0.011644220910966396,\n        -0.0040392628870904446,\n        0.0037049793172627687,\n        -0.02261987328529358,\n        -0.003987031057476997,\n        0.004185512196272612,\n        0.010244406759738922,\n        -0.006459339056164026,\n        -0.01286296360194683,\n        -0.015279557555913925,\n        0.01408170722424984,\n        0.010070300661027431,\n        -0.02019631490111351,\n        -0.004941132850944996,\n        0.0012892561499029398,\n        -0.021394165232777596,\n        -0.018107041716575623,\n        0.01121243741363287,\n        0.024876289069652557,\n        -0.018775608390569687,\n        -0.04036477580666542,\n        -0.002329540438950062,\n        0.003774621756747365,\n        0.021199166774749756,\n        0.012347609736025333,\n        -0.015251700766384602,\n        0.014130456373095512,\n        -0.02831662818789482,\n        -0.025600571185350418,\n        0.0033637310843914747,\n        -0.028636982664465904,\n        -0.005310238339006901,\n        -0.009561911225318909,\n        -0.031784821301698685,\n        -0.01961131952702999,\n        -0.034347664564847946,\n        0.008921200409531593,\n        0.037133362144231796,\n        -0.02261987328529358,\n        0.00533113069832325,\n        -0.017647400498390198,\n        -0.010759761556982994,\n        -0.0036005156580358744,\n        0.027480917051434517,\n        -0.04855472967028618,\n        -0.018288111314177513,\n        0.013886707834899426,\n        -0.03292695805430412,\n        0.00448845699429512,\n        0.020126672461628914,\n        0.0005741150816902518,\n        0.02518271654844284,\n        -0.011128866113722324,\n        -0.007827813737094402,\n        -0.011247258633375168,\n        -0.021519523113965988,\n        0.01688133366405964,\n        0.022522373124957085,\n        -0.011184580624103546,\n        -0.006800586823374033,\n        0.02888769470155239,\n        0.019513819366693497,\n        0.02702127769589424,\n        0.0124938590452075,\n        -0.007584064733237028,\n        0.0022703444119542837,\n        0.002663824474439025,\n        0.0016209284076467156,\n        -0.019694888964295387,\n        0.008106383495032787,\n        0.0028727517928928137,\n        0.0019517301116138697,\n        0.01454134751111269,\n        -0.005035150330513716,\n        -0.003805960761383176,\n        0.030614828690886497,\n        0.010892082005739212,\n        0.030141260474920273,\n        0.026533780619502068,\n        -0.00798102654516697,\n        0.01232671644538641,\n        0.00907441321760416,\n        0.008886379189789295,\n        -0.02963983453810215,\n        0.0015390985645353794,\n        0.008677451871335506,\n        0.018274184316396713,\n        -0.004565063863992691,\n        -0.0037084612995386124,\n        -0.022536301985383034,\n        0.012890820391476154,\n        0.020837025716900826,\n        0.001690570847131312,\n        0.0038198893889784813,\n        -0.004648634698241949,\n        0.02484843321144581,\n        0.018483111634850502,\n        -0.0064837136305868626,\n        0.022633802145719528,\n        -0.0059091635048389435,\n        0.029918404296040535,\n        0.00407756632193923,\n        0.011240294203162193,\n        0.029556263238191605,\n        0.032119106501340866,\n        -0.02352522499859333,\n        -0.014332420192658901,\n        0.009346019476652145,\n        0.015042773447930813,\n        0.013218140229582787,\n        0.03596337139606476,\n        -0.014652775600552559,\n        0.00887941475957632,\n        -0.0011386543046683073,\n        0.03170125186443329,\n        -0.01923524960875511,\n        -0.01593419723212719,\n        0.003722389927133918,\n        -0.009868337772786617,\n        -0.025642355903983116,\n        0.028804125264286995,\n        -0.001045507495291531,\n        0.048220444470644,\n        0.014527418650686741,\n        0.015028844587504864,\n        -0.002865787595510483,\n        -0.01614312455058098,\n        -0.0019221320981159806,\n        -0.00402881670743227,\n        0.01739668846130371,\n        -0.0023991831112653017,\n        -0.0034995339810848236,\n        8.351655560545623e-05,\n        -0.008252632804214954,\n        0.013914564624428749,\n        0.025140929967164993,\n        0.01775882951915264,\n        0.02057238481938839,\n        0.0185527540743351,\n        -0.017299188300967216,\n        0.004255154635757208,\n        0.0024775308556854725,\n        -0.00890030711889267,\n        0.003868639003485441,\n        -0.001080328831449151,\n        0.001057695015333593,\n        -0.01473634596914053,\n        -0.014931345358490944,\n        -0.011734755709767342,\n        -0.01742454618215561,\n        0.02016845904290676,\n        0.005794253200292587,\n        -0.007660671602934599,\n        0.006361839361488819,\n        0.012479930184781551,\n        0.029305551201105118,\n        -0.025698071345686913,\n        -0.038247644901275635,\n        0.0075562079437077045,\n        -0.002284273039549589,\n        -0.009708159603178501,\n        -0.018650252372026443,\n        -0.008210847154259682,\n        0.017159903421998024,\n        -0.01230582408607006,\n        -0.005630593281239271,\n        -0.011463150382041931,\n        0.006664784159511328,\n        -0.01167207770049572,\n        -0.025447357445955276,\n        -0.018622396513819695,\n        0.003868639003485441,\n        0.015474556013941765,\n        0.009972801432013512,\n        -0.0019238732056692243,\n        -0.0296676903963089,\n        -0.004091495182365179,\n        0.000986311468295753,\n        -0.024374863132834435,\n        -0.01910989359021187,\n        0.01259135827422142,\n        0.00815513264387846,\n        -0.0021954788826406,\n        -0.02259201556444168,\n        -0.004909793846309185,\n        -0.029751261696219444,\n        0.025865212082862854,\n        0.003374177496880293,\n        0.003913906868547201,\n        -0.00026007110136561096,\n        0.026269137859344482,\n        0.013677780516445637,\n        0.002912796102464199,\n        0.014694560319185257,\n        0.02445843443274498,\n        0.0315062515437603,\n        0.0052440776489675045,\n        -0.029054837301373482,\n        -0.0020527117885649204,\n        0.014485633000731468,\n        0.031199825927615166,\n        0.0194441769272089,\n        -0.021463807672262192,\n        0.01390760112553835,\n        -0.041785482317209244,\n        0.012082967907190323,\n        -0.018385611474514008,\n        -0.010550834238529205,\n        0.03019697405397892,\n        0.002575030317530036,\n        -0.05691182613372803,\n        -0.008308346383273602,\n        0.0033585079945623875,\n        0.0067448727786540985,\n        0.03679908066987991,\n        0.0166584774851799,\n        0.010167799890041351,\n        -0.023567011579871178,\n        0.002545432187616825,\n        -0.0002733466972131282,\n        0.005574879702180624,\n        0.003680604510009289,\n        0.02206273376941681,\n        0.0067100515589118,\n        0.020642027258872986,\n        0.009777802042663097,\n        0.004749616142362356,\n        0.022689515724778175,\n        0.011128866113722324,\n        -0.006274786312133074,\n        0.015989910811185837,\n        0.028400197625160217,\n        -0.012201360426843166,\n        0.008475488051772118,\n        -0.025252358987927437,\n        0.037551216781139374,\n        0.014485633000731468,\n        -0.0007073063170537353,\n        0.013886707834899426,\n        -0.018288111314177513,\n        0.013350460678339005,\n        0.038609784096479416,\n        -0.010188693180680275,\n        0.004202922806143761,\n        -0.0083431676030159,\n        -0.019151678308844566,\n        0.031199825927615166,\n        0.017299188300967216,\n        0.0027195382863283157,\n        -0.0003904330951627344,\n        -0.012459037825465202,\n        -0.010801546275615692,\n        -0.02572592720389366,\n        0.002841412788257003,\n        0.011275115422904491,\n        0.006748354993760586,\n        -0.002576771192252636,\n        -0.020976310595870018,\n        0.004812294617295265,\n        -0.026993419975042343,\n        -0.036214083433151245,\n        -0.0018054809188470244,\n        0.021185237914323807,\n        0.00398006709292531,\n        0.0314783938229084,\n        -0.0030607865191996098,\n        -0.006978175137192011,\n        -0.026227353140711784,\n        -0.025642355903983116,\n        -0.0019987388513982296,\n        0.008092454634606838,\n        -0.03796907514333725,\n        -0.05983680859208107,\n        -0.006609070114791393,\n        0.02649199403822422,\n        0.025447357445955276,\n        0.014019028283655643,\n        0.006912014912813902,\n        -0.02817734144628048,\n        0.005762914195656776,\n        -0.017842400819063187,\n        -0.024890217930078506,\n        -0.02241094596683979,\n        -0.007465672679245472,\n        -0.0013162426184862852,\n        0.0015556386206299067,\n        -0.0194859616458416,\n        0.007841741666197777,\n        0.003078197129070759,\n        0.00653594546020031,\n        -0.015056701377034187,\n        0.014429919421672821,\n        0.03950120881199837,\n        -0.02094845473766327,\n        0.021004168316721916,\n        0.015599912963807583,\n        -0.0035761406179517508,\n        0.006410588975995779,\n        0.01812097057700157,\n        -0.022299518808722496,\n        0.010070300661027431,\n        -0.029584120959043503,\n        -0.01138654351234436,\n        -0.02129666693508625,\n        -0.01397027913480997,\n        0.0033114992547780275,\n        -0.010899046435952187,\n        0.007577100303024054,\n        0.01324599701911211,\n        -0.014750274829566479,\n        0.00224422849714756,\n        -0.00853816606104374,\n        0.004585956688970327,\n        -0.01039065606892109,\n        0.02554485760629177,\n        0.001240506418980658,\n        0.04245404899120331,\n        0.00684933690354228,\n        0.007235852535814047,\n        -0.004742652177810669,\n        0.004377028904855251,\n        -0.014708489179611206,\n        0.04490546137094498,\n        0.010042443871498108,\n        -0.006824961863458157,\n        -0.002970251254737377,\n        -0.003893014043569565,\n        -0.0037119435146450996,\n        -0.021213095635175705,\n        0.0035169445909559727,\n        -0.006354874931275845,\n        0.027271989732980728,\n        -0.0010315791005268693,\n        -0.03236981853842735,\n        -0.03223053365945816,\n        -0.025461286306381226,\n        -0.024932002648711205,\n        0.0037955145817250013,\n        -0.022341303527355194,\n        0.013350460678339005,\n        0.013461888767778873,\n        -0.013684744946658611,\n        -0.016268480569124222,\n        -0.012730643153190613,\n        0.033623382449150085,\n        -0.02484843321144581,\n        -0.01997346058487892,\n        0.03245339170098305,\n        0.004046227317303419,\n        0.02463950589299202,\n        -0.0011995915556326509,\n        0.025099145248532295,\n        -0.042342621833086014,\n        -0.029138408601284027,\n        0.0024914592504501343,\n        -0.0042969402857124805,\n        -0.001340617542155087,\n        0.01222921721637249,\n        0.03317767009139061,\n        0.02611592598259449,\n        0.0426211915910244,\n        0.010745832696557045,\n        0.014513489790260792,\n        0.014360276982188225,\n        -0.009854408912360668,\n        -0.004589438438415527,\n        0.016031695529818535,\n        -0.015711341053247452,\n        0.012312788516283035,\n        -0.020252030342817307,\n        0.01703454740345478,\n        -0.01983417384326458,\n        0.024583790451288223,\n        0.006842372473329306,\n        0.030893398448824883,\n        0.002928465837612748,\n        0.004937651101499796,\n        -0.010126015171408653,\n        -0.035211231559515,\n        -0.02239701710641384,\n        -0.008475488051772118,\n        0.007389065809547901,\n        -0.0026307441294193268,\n        0.026046283543109894,\n        -0.023037727922201157,\n        0.009972801432013512,\n        0.009116198867559433,\n        0.0231491569429636,\n        0.004081048537045717,\n        0.015878481790423393,\n        0.007528350688517094,\n        -0.011330829933285713,\n        0.0011726050870493054,\n        0.014443847350776196,\n        0.0314505398273468,\n        -0.010871189646422863,\n        -0.024152006953954697,\n        -0.01945810578763485,\n        0.004760062787681818,\n        0.02702127769589424,\n        0.007079157046973705,\n        0.010522976517677307,\n        0.024750933051109314,\n        0.005362470168620348,\n        -0.022912371903657913,\n        0.014214027673006058,\n        -0.018232397735118866,\n        -0.02352522499859333,\n        -0.00118131039198488,\n        -0.016379907727241516,\n        -0.036046940833330154,\n        -0.004108905792236328,\n        -0.0071871024556458,\n        0.014074742794036865,\n        0.010731903836131096,\n        0.010341905988752842,\n        -0.0037049793172627687,\n        0.004244708456099033,\n        -0.029584120959043503,\n        -0.021575236693024635,\n        0.02022417262196541,\n        0.002024854766204953,\n        0.0305869709700346,\n        -0.0076049575582146645,\n        0.03225839138031006,\n        0.0005266711814329028,\n        0.01611526682972908,\n        0.01546062808483839,\n        0.00016910061822272837,\n        -0.002303424524143338,\n        0.009979765862226486,\n        0.017299188300967216,\n        0.004700866527855396,\n        -0.02054452709853649,\n        -0.0333169549703598,\n        0.003123464761301875,\n        -0.005365951918065548,\n        -0.004286494106054306,\n        -0.020600242540240288,\n        0.011811362579464912,\n        0.004450153559446335,\n        -0.007932277396321297,\n        -0.014227956533432007,\n        -0.017842400819063187,\n        -0.01891489326953888,\n        0.03423623740673065,\n        -0.0049655078910291195,\n        0.0027456542011350393,\n        0.01000762265175581,\n        -0.009227626956999302,\n        -0.009680302813649178,\n        0.02445843443274498,\n        -0.0034403379540890455,\n        0.028943410143256187,\n        0.01703454740345478,\n        0.003194848308339715,\n        0.00689808651804924,\n        0.018608467653393745,\n        0.0008949057082645595,\n        0.020795240998268127,\n        0.011651184409856796,\n        -0.0010846814839169383,\n        -0.014917416498064995,\n        0.014513489790260792,\n        -0.012855999171733856,\n        -0.016379907727241516,\n        0.004185512196272612,\n        -0.003927835263311863,\n        -0.0006729203159920871,\n        0.03206339105963707,\n        -0.03551765903830528,\n        -0.021923448890447617,\n        0.019541677087545395,\n        -0.02649199403822422,\n        0.0024026650935411453,\n        -0.003466453868895769,\n        -0.0016740307910367846,\n        0.010975653305649757,\n        -0.006515052635222673,\n        -0.018970608711242676,\n        0.030865540727972984,\n        0.0036597116850316525,\n        -0.026408424600958824,\n        0.022856658324599266,\n        0.0055853258818387985,\n        0.008969949558377266,\n        -0.02665913663804531,\n        0.013942422345280647,\n        0.0015582501655444503,\n        -0.007904419675469398,\n        -0.005108274985104799,\n        0.007618885952979326,\n        0.01703454740345478,\n        0.0057141645811498165,\n        -0.006351393181830645,\n        -0.00853816606104374,\n        0.005738539155572653,\n        0.020976310595870018,\n        -0.016533121466636658,\n        -0.0011299489997327328,\n        0.016031695529818535,\n        -0.002251192694529891,\n        -0.022661659866571426,\n        4.6818237024126574e-05,\n        -0.013949385844171047,\n        -0.04276047646999359,\n        0.01174868457019329,\n        -0.02204880490899086,\n        -0.02924983575940132,\n        -0.00918584130704403,\n        0.0066613019444048405,\n        0.0064001427963376045,\n        0.013378318399190903,\n        -0.004241226240992546,\n        -0.027550559490919113,\n        0.020321672782301903,\n        0.01370563730597496,\n        -0.006072822958230972,\n        0.0143742049112916,\n        -0.011630292050540447,\n        -0.00518836360424757,\n        -0.0259209256619215,\n        0.022160233929753304,\n        -0.01647740788757801,\n        -0.0026081104297190905,\n        -0.029305551201105118,\n        0.008670487441122532,\n        0.015697412192821503,\n        -0.019903818145394325,\n        -0.012883856892585754,\n        -0.014290634542703629,\n        0.02461164817214012,\n        -0.008496381342411041,\n        0.008642630651593208,\n        0.01362903043627739,\n        0.002970251254737377,\n        -0.01724347472190857,\n        0.030141260474920273,\n        0.002757841721177101,\n        -0.029361264780163765,\n        -0.003017259994521737,\n        -0.0032975708600133657,\n        0.009241555817425251,\n        0.004505867604166269,\n        -0.0370219349861145,\n        -0.028581269085407257,\n        -0.0025837356224656105,\n        -0.004415332339704037,\n        -0.031227681785821915,\n        -0.036966223269701004,\n        -0.028469840064644814,\n        0.03301053121685982,\n        0.046047598123550415,\n        -0.008865485899150372,\n        -0.00989619456231594,\n        -0.020614169538021088,\n        -0.00908834207803011,\n        -0.010070300661027431,\n        -0.022174160927534103,\n        -0.009882265701889992,\n        -0.023929152637720108,\n        0.004760062787681818,\n        -0.015613840892910957,\n        0.02149166539311409,\n        -0.025085216388106346,\n        -0.0047705089673399925,\n        0.0029215014073997736,\n        0.02037738636136055,\n        -0.015599912963807583,\n        9.140573820332065e-05,\n        0.006455856841057539,\n        -0.0077512068673968315,\n        -0.029779119417071342,\n        -0.020084887742996216,\n        -0.0008283100905828178,\n        -0.0034333737567067146,\n        -0.006643891334533691,\n        0.0097429808229208,\n        0.006737908814102411,\n        -0.011365651153028011,\n        0.007563171908259392,\n        0.021073810756206512,\n        0.029556263238191605,\n        0.0026133335195481777,\n        0.01628240942955017,\n        -0.025280214846134186,\n        -0.010000658221542835,\n        0.005564433056861162,\n        -0.0019151679007336497,\n        -0.009283340536057949,\n        -0.005296309478580952,\n        0.03259267657995224,\n        -0.030280545353889465,\n        -9.608484106138349e-05,\n        -0.04055977240204811,\n        -0.0025071287527680397,\n        0.008524238131940365,\n        -0.009694231674075127,\n        0.03390195220708847,\n        0.03259267657995224,\n        -0.019555604085326195,\n        0.025489144027233124,\n        0.0009836998069658875,\n        -0.003118241438642144,\n        -0.02872055396437645,\n        0.012180467136204243,\n        0.000898387806955725,\n        -0.02078131213784218,\n        0.02703520655632019,\n        -0.031227681785821915,\n        -0.012361537665128708,\n        0.0012248369166627526,\n        -0.009332090616226196,\n        -0.006306125316768885,\n        -0.0022372642997652292,\n        -0.014214027673006058,\n        -0.012507786974310875,\n        -0.026993419975042343,\n        0.009617624804377556,\n        0.00241137039847672,\n        -0.026965564116835594,\n        0.016950976103544235,\n        -0.018413469195365906,\n        -0.002905831905081868,\n        -0.01704847626388073,\n        0.0039382814429700375,\n        -0.0002776993496809155,\n        -0.00897691398859024,\n        -0.022508446127176285,\n        0.013468853197991848,\n        -0.014945273287594318,\n        0.004387475550174713,\n        0.00832227524369955,\n        0.019875960424542427,\n        0.00513961398974061,\n        0.007107013836503029,\n        0.2261987328529358,\n        -0.016909191384911537,\n        -0.023093441501259804,\n        0.018580609932541847,\n        0.0011508418247103691,\n        0.010815475136041641,\n        0.024695219472050667,\n        0.009561911225318909,\n        0.007577100303024054,\n        0.010314049199223518,\n        0.023762010037899017,\n        -0.006943353917449713,\n        -0.03746764734387398,\n        -2.649678208399564e-05,\n        0.011268150992691517,\n        -0.0033463204745203257,\n        -0.03590765595436096,\n        -0.0370776504278183,\n        0.0018246326362714171,\n        2.875472273444757e-05,\n        0.012598322704434395,\n        0.0012039442081004381,\n        -0.013106712140142918,\n        -0.041395481675863266,\n        0.04111691191792488,\n        0.0008513791835866868,\n        0.013287782669067383,\n        0.0041681015864014626,\n        0.013942422345280647,\n        0.016226695850491524,\n        -0.019123822450637817,\n        0.020628098398447037,\n        -0.00953405350446701,\n        0.02225773222744465,\n        -0.026728779077529907,\n        -0.007653707172721624,\n        -0.01102440245449543,\n        -0.0048157768324017525,\n        0.04189690947532654,\n        0.012904749251902103,\n        0.011365651153028011,\n        -0.017925970256328583,\n        0.0021084256004542112,\n        -0.0002639884769450873,\n        -0.022689515724778175,\n        0.0012753276387229562,\n        0.0015730492305010557,\n        -0.0352390892803669,\n        0.006664784159511328,\n        0.02405450865626335,\n        -0.027453061193227768,\n        -0.0176056157797575,\n        0.02612985298037529,\n        0.0194441769272089,\n        -0.009589768014848232,\n        0.0013545459369197488,\n        -0.01611526682972908,\n        0.00899084284901619,\n        0.0278152022510767,\n        0.050504717975854874,\n        -0.016783835366368294,\n        0.002054452896118164,\n        0.0024670844431966543,\n        0.03866549953818321,\n        -8.825006079860032e-05,\n        -0.008872450329363346,\n        -0.014903487637639046,\n        0.020112745463848114,\n        0.01611526682972908,\n        -0.02872055396437645,\n        -0.0203634575009346,\n        -0.021199166774749756,\n        0.0074935294687747955,\n        -0.00908834207803011,\n        -0.04432046785950661,\n        -0.01817668415606022,\n        0.016895262524485588,\n        0.015182058326900005,\n        0.008196918293833733,\n        0.027202347293496132,\n        -0.008928163908421993,\n        -0.026547709479928017,\n        -0.00021393295901361853,\n        0.0007360338349826634,\n        -0.01519598625600338,\n        -0.017494188621640205,\n        0.0030990897212177515,\n        -0.016003839671611786,\n        -0.022188089787960052,\n        -0.00564800389111042,\n        0.00279962713830173,\n        -0.016769906505942345,\n        -0.02888769470155239,\n        -0.008482452481985092,\n        -0.01360813807696104,\n        -0.007465672679245472,\n        0.02447236329317093,\n        0.02002917416393757,\n        0.016352051869034767,\n        -0.0038303358014672995,\n        -0.024737004190683365,\n        0.03426409512758255,\n        0.033233385533094406,\n        -0.0007477860199287534,\n        0.008712273091077805,\n        -0.00504211476072669,\n        -0.008252632804214954,\n        0.002853600075468421,\n        -0.0012500822776928544,\n        -0.0013919788179919124,\n        -0.01924917846918106,\n        -0.04707830771803856,\n        0.00417158380150795,\n        -0.005766396410763264,\n        0.002787439851090312,\n        0.008273525163531303,\n        -0.00015647792315576226,\n        0.005480862222611904,\n        0.018483111634850502,\n        -0.013998135924339294,\n        0.0064663030207157135,\n        -0.03275981545448303,\n        0.0011438775109127164,\n        0.005491308402270079,\n        0.015042773447930813,\n        -0.023762010037899017,\n        -0.01020262110978365,\n        -0.008921200409531593,\n        -0.01720169000327587,\n        0.004798366222530603,\n        0.0025889587122946978,\n        -0.03312195837497711,\n        0.013120641000568867,\n        0.018037399277091026,\n        -0.012918678112328053,\n        -0.022299518808722496,\n        0.009304233826696873,\n        -0.029305551201105118,\n        0.009171913377940655,\n        -0.005923091899603605,\n        -0.008969949558377266,\n        -0.01416527759283781,\n        -0.0040845307521522045,\n        -0.01454134751111269,\n        0.008252632804214954,\n        -0.01462491787970066,\n        0.012027254328131676,\n        0.023190941661596298,\n        -0.00426908303052187,\n        -0.02167273499071598,\n        -0.016602763906121254,\n        -0.006811033468693495,\n        -0.00532764894887805,\n        -0.017898114398121834,\n        0.004241226240992546,\n        -0.0002594181860331446,\n        -0.009840480983257294,\n        -0.014290634542703629,\n        -0.0013449701946228743,\n        -0.01463884674012661,\n        -0.014889559708535671,\n        -0.01027922797948122,\n        -0.004516314249485731,\n        -0.004175066016614437,\n        -0.020795240998268127,\n        0.014973130077123642,\n        -0.1780618578195572,\n        0.006654337979853153,\n        0.03075411356985569,\n        -0.030057689175009727,\n        0.00897691398859024,\n        0.008886379189789295,\n        0.002056193770840764,\n        0.0029615459498018026,\n        -0.046576883643865585,\n        0.004498903173953295,\n        0.022828800603747368,\n        -0.03610265627503395,\n        -0.026979491114616394,\n        -0.022271661087870598,\n        0.000401532364776358,\n        -0.0015660850331187248,\n        -0.008259596303105354,\n        0.01500098779797554,\n        0.013998135924339294,\n        0.019569532945752144,\n        0.02944483608007431,\n        -0.009756909683346748,\n        0.007813884876668453,\n        0.0038964960258454084,\n        -0.00426560128107667,\n        -0.007382101379334927,\n        -0.01305099856108427,\n        0.031784821301698685,\n        0.00472524156793952,\n        -0.039751920849084854,\n        0.010418512858450413,\n        -0.0157252699136734,\n        0.010432441718876362,\n        0.005947466939687729,\n        -0.00860780943185091,\n        -0.0009749945602379739,\n        0.001659231842495501,\n        -0.03181267902255058,\n        -0.020962383598089218,\n        0.005386844743043184,\n        0.025322001427412033,\n        0.011135830543935299,\n        0.009582803584635258,\n        0.020990239456295967,\n        -0.007207995280623436,\n        0.0014172241790220141,\n        0.03459837660193443,\n        -0.021951306611299515,\n        0.021519523113965988,\n        -0.03451480716466904,\n        0.019736675545573235,\n        -0.00033406622242182493,\n        0.003480382263660431,\n        -0.02203487604856491,\n        0.002682975959032774,\n        0.014214027673006058,\n        0.016769906505942345,\n        0.026575565338134766,\n        -0.006170322652906179,\n        -0.019513819366693497,\n        -0.014457776211202145,\n        -0.019360605627298355,\n        0.03225839138031006,\n        -0.0017349679255858064,\n        -0.005902199074625969,\n        0.0028692695777863264,\n        0.01020262110978365,\n        0.017452402040362358,\n        -0.03682693839073181,\n        0.01324599701911211,\n        -0.0015922009479254484,\n        -0.013447960838675499,\n        -0.01910989359021187,\n        -0.012821177951991558,\n        0.002594181802123785,\n        0.03189624845981598,\n        -0.028024129569530487,\n        -0.004126316402107477,\n        -0.016797762364149094,\n        0.01614312455058098,\n        -0.020335599780082703,\n        0.03964049369096756,\n        -0.0008139463607221842,\n        0.02295415662229061,\n        -0.0035604711156338453,\n        -0.013141533359885216,\n        0.018789537250995636,\n        0.030113402754068375,\n        -0.008928163908421993,\n        -0.026742707937955856,\n        0.024695219472050667,\n        -0.022856658324599266,\n        -0.0052614882588386536,\n        -0.019917745143175125,\n        0.021380238234996796,\n        0.01943024806678295,\n        0.0078695984557271,\n        -0.01984810270369053,\n        0.03370695561170578,\n        0.003301053075119853,\n        -0.017494188621640205,\n        -0.005195328034460545,\n        -0.017633473500609398,\n        0.0033341331873089075,\n        0.009589768014848232,\n        0.005365951918065548,\n        -0.006922461092472076,\n        -0.013977243565022945,\n        0.04699473828077316,\n        -0.017354903742671013,\n        -0.011038331314921379,\n        -0.01232671644538641,\n        0.004666045308113098,\n        0.014861702919006348,\n        0.002273826627060771,\n        0.027843058109283447,\n        -0.014207063242793083,\n        0.011149759404361248,\n        0.017340974882245064,\n        0.011330829933285713,\n        0.05209256708621979,\n        -0.0034594896715134382,\n        -0.017522044479846954,\n        0.010244406759738922,\n        -0.020488813519477844,\n        -0.037328362464904785,\n        -0.12212502956390381,\n        -0.028636982664465904,\n        0.002160657662898302,\n        0.0015086299972608685,\n        -0.002303424524143338,\n        0.00620166165754199,\n        0.0008439796511083841,\n        0.02072559855878353,\n        -0.023580940440297127,\n        0.023636654019355774,\n        -0.014610989950597286,\n        -0.016630621626973152,\n        0.0034455610439181328,\n        -0.01704847626388073,\n        -0.004516314249485731,\n        -0.0006581213092431426,\n        -0.000348429981386289,\n        -0.009283340536057949,\n        0.0074099586345255375,\n        0.03039197251200676,\n        -0.0038964960258454084,\n        -0.027188418433070183,\n        0.0073263878002762794,\n        0.008914235979318619,\n        -0.010070300661027431,\n        -0.00916494894772768,\n        -0.021589165553450584,\n        0.013545460067689419,\n        -0.010745832696557045,\n        0.007256744895130396,\n        0.0213105957955122,\n        -0.029974117875099182,\n        0.006031037773936987,\n        0.007904419675469398,\n        9.76082737906836e-05,\n        0.014360276982188225,\n        -0.032704103738069534,\n        -0.009422625415027142,\n        0.0023713260889053345,\n        -0.018580609932541847,\n        -0.005059525370597839,\n        0.0008653076365590096,\n        0.0055191656574606895,\n        0.00943655427545309,\n        -0.0012082968605682254,\n        -0.00013340884470380843,\n        -0.0351276621222496,\n        0.02540557272732258,\n        0.007535315118730068,\n        -0.0143742049112916,\n        -0.02150559425354004,\n        -0.021547378972172737,\n        -0.026422351598739624,\n        -0.004861044231802225,\n        0.0014859961811453104,\n        -0.02484843321144581,\n        -0.0027439133264124393,\n        -0.0031704732682555914,\n        0.016895262524485588,\n        -0.004359618294984102,\n        -0.00426560128107667,\n        0.016560979187488556,\n        -0.02349736914038658,\n        0.04671616852283478,\n        0.05083899945020676,\n        -0.010258335620164871,\n        -0.011010474525392056,\n        -0.0051117572002112865,\n        -0.0046033672988414764,\n        -0.020126672461628914,\n        -0.017118118703365326,\n        0.021240953356027603,\n        -0.005735057406127453,\n        0.008983878418803215,\n        -0.027160562574863434,\n        -0.015404913574457169,\n        -0.012250109575688839,\n        0.0019221320981159806,\n        -0.02518271654844284,\n        0.008120311424136162,\n        -0.021324522793293,\n        -0.013935457915067673,\n        -0.012605286203324795,\n        0.004707830958068371,\n        -0.008364059962332249,\n        -0.013803137466311455,\n        -0.05022614821791649,\n        -0.030809827148914337,\n        -0.005553986877202988,\n        -0.037690501660108566,\n        -0.023121299222111702,\n        0.030475543811917305,\n        0.02245273068547249,\n        0.005905681289732456,\n        -0.01240332331508398,\n        -0.014875630848109722,\n        -0.013023141771554947,\n        -0.011553685180842876,\n        0.015585984103381634,\n        0.0024270399007946253,\n        0.004373547155410051,\n        0.0026115926448255777,\n        -0.022884514182806015,\n        0.018051328137516975,\n        0.011379579082131386,\n        0.0022477107122540474,\n        -0.0056340754963457584,\n        0.014388133771717548,\n        0.03799692913889885,\n        -0.007653707172721624,\n        -0.011762612499296665,\n        0.0015695671318098903,\n        -0.015613840892910957,\n        -0.004415332339704037,\n        -0.01979238912463188,\n        -0.0010176505893468857,\n        -0.0115815419703722,\n        -0.028246985748410225,\n        0.02707699127495289,\n        -0.029333407059311867,\n        0.018775608390569687,\n        0.006908532697707415,\n        0.004498903173953295,\n        0.012723678722977638,\n        0.01575312577188015,\n        0.008844593539834023,\n        -0.010446370579302311,\n        -0.01871989481151104,\n        0.0037293541245162487,\n        0.018761681392788887,\n        -0.02299594320356846,\n        -0.02372022531926632,\n        0.012765464372932911,\n        -0.001814186223782599,\n        0.0008888120064511895,\n        0.01397027913480997,\n        0.00282400194555521,\n        -0.01871989481151104,\n        0.014123492874205112,\n        0.02221594750881195,\n        0.027160562574863434,\n        -0.0115745784714818,\n        0.005310238339006901,\n        -0.02388736605644226,\n        -0.009213698096573353,\n        -0.021199166774749756,\n        0.012089932337403297,\n        0.009554946795105934,\n        -0.017118118703365326,\n        -0.013190283440053463,\n        0.011776541359722614,\n        0.0011665113270282745,\n        0.013830994255840778,\n        0.017299188300967216,\n        0.005052560940384865,\n        -0.012298859655857086,\n        -0.017744900658726692,\n        -0.008036741055548191,\n        0.009798695333302021,\n        -0.011240294203162193,\n        0.010432441718876362,\n        -0.01590633951127529,\n        0.025711998343467712,\n        0.0019325785106047988,\n        0.0033759186044335365,\n        -0.01008422952145338,\n        0.012619215063750744,\n        -0.0030573043040931225,\n        -0.006880675908178091,\n        0.020614169538021088,\n        -0.00750049389898777,\n        -0.03596337139606476,\n        -0.004147208761423826,\n        0.004592920653522015,\n        8.800522664387245e-06,\n        -0.007876562885940075,\n        0.02721627615392208,\n        -0.004133280366659164,\n        0.002930206712335348,\n        0.006375767756253481,\n        -0.024876289069652557,\n        0.031756963580846786,\n        0.006347910966724157,\n        -0.018190613016486168,\n        -0.004272565245628357,\n        0.00629567913711071,\n        0.04278833046555519,\n        0.012382430955767632,\n        -0.008015847764909267,\n        0.019402392208576202,\n        -0.004161137621849775,\n        -0.010961724445223808,\n        0.01742454618215561,\n        -0.0027682881336659193,\n        0.013552424497902393,\n        0.010126015171408653,\n        0.017633473500609398,\n        0.014875630848109722,\n        -0.016937047243118286,\n        0.01741061732172966,\n        0.02369236759841442,\n        0.01851096749305725,\n        0.010717975907027721,\n        0.010028515011072159,\n        -0.014186170883476734,\n        -0.010773689486086369,\n        -0.019694888964295387,\n        0.0027421722188591957,\n        0.0014868667349219322,\n        -0.016017766669392586,\n        -0.0019378017168492079,\n        0.040281202644109726,\n        0.022731302306056023,\n        0.01129600778222084,\n        0.021909520030021667,\n        -0.0035569891333580017,\n        -0.007799956481903791,\n        0.02111559547483921,\n        -0.013928493484854698,\n        0.0001116455823648721,\n        -0.020279886201024055,\n        0.00018085278861690313,\n        -0.004383993335068226,\n        -0.02150559425354004,\n        0.013538495637476444,\n        -0.0101817287504673,\n        0.04092191532254219,\n        0.01813489943742752,\n        0.007674599997699261,\n        -0.03295481577515602,\n        0.03487694635987282,\n        0.010711011476814747,\n        0.014485633000731468,\n        -0.0035831050481647253,\n        0.004049709532409906,\n        -0.01683954894542694,\n        -0.003823371371254325,\n        -0.013030105270445347,\n        0.0012648813426494598,\n        0.022536301985383034,\n        0.012291895225644112,\n        0.09142663329839706,\n        0.005985769908875227,\n        0.008691379800438881,\n        -0.03599122539162636,\n        -0.009659410454332829,\n        0.0021293184254318476,\n        0.013615102507174015,\n        0.0001742149906931445,\n        -0.04106120020151138,\n        -0.012821177951991558,\n        -0.002681235084310174,\n        0.01628240942955017,\n        -0.013447960838675499,\n        -0.029416978359222412,\n        0.0003438596904743463,\n        0.02034952864050865,\n        -0.009485304355621338,\n        0.02221594750881195,\n        -0.010864225216209888,\n        -0.016560979187488556,\n        0.027313776314258575,\n        -3.237286728108302e-05,\n        0.023302368819713593,\n        0.011741720139980316,\n        0.0019935155287384987,\n        0.008691379800438881,\n        0.008586916141211987,\n        -0.011031366884708405,\n        0.004749616142362356,\n        -0.046799737960100174,\n        0.026422351598739624,\n        0.013587245717644691,\n        -0.016630621626973152,\n        -0.0024618611205369234,\n        0.0006933778058737516,\n        0.005122203379869461,\n        -0.005560950841754675,\n        0.005146578419953585,\n        0.017452402040362358,\n        0.015878481790423393,\n        0.001499924692325294,\n        0.018497038632631302,\n        -0.012869928032159805,\n        -0.0101817287504673,\n        -0.03869335353374481,\n        0.0006285232957452536,\n        -0.0020701223984360695,\n        -0.020057030022144318,\n        -0.014931345358490944\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"ada\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"total_tokens\": 10\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-49321/49322","correlationid":"e0d7964c-53c8-49ce-b285-1e2740ff52b9","xrequestid":"e0d7964c-53c8-49ce-b285-1e2740ff52b9","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"6dccfd35-457c-421d-a3a8-5bff1c24f214","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:37Z","data":[{"name":"Prompt_variants","context":{"request_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","trace_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","span_id":"0x378f894483e371f1","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xb1e57bf254752dd7","start_time":"2024-02-05T15:00:35.437097Z","end_time":"2024-02-05T15:00:35.440283Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"render_template_jinja2","node_name":"Prompt_variants","flow_id":"","root_run_id":"","line_run_id":"371856bd-457d-490c-8f4e-69f825b727d1","inputs":"{\n  \"template\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n {{contexts}} \\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\nuser: {{question}} \\nassistant:\",\n  \"chat_history\": [\n    {\n      \"inputs\": {\n        \"question\": \"what's Attention?\"\n      },\n      \"outputs\": {\n        \"output\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\"\n      }\n    }\n  ],\n  \"contexts\": \"Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfoutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni\\u2208Rdmodel\\u00d7dk,WK\\ni\\u2208Rdmodel\\u00d7dk,WV\\ni\\u2208Rdmodel\\u00d7dv\\nandWO\\u2208Rhdv\\u00d7dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel\/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\u2022In \\\"encoder-decoder attention\\\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\u2212\\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\\u221admodel.\\n5\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n  \"question\": \"how to calculate Attention?\"\n}","output":"\"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfoutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni\\u2208Rdmodel\\u00d7dk,WK\\ni\\u2208Rdmodel\\u00d7dk,WV\\ni\\u2208Rdmodel\\u00d7dv\\nandWO\\u2208Rhdv\\u00d7dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel\/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\u2022In \\\"encoder-decoder attention\\\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\u2212\\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\\u221admodel.\\n5\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\n user: \\nwhat's Attention? \\nassistant: \\nAttention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf) \\nuser: how to calculate Attention? \\nassistant:\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-21811/21812","correlationid":"817eda7a-7f55-4dc3-b0dd-7672fc5c90ad","xrequestid":"817eda7a-7f55-4dc3-b0dd-7672fc5c90ad","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default"}
{"specversion":"1.0","id":"ee1fb79a-ff85-409a-834f-9ad3b35b3ad8","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:37Z","data":[{"name":"search","context":{"request_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","trace_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","span_id":"0xf853c85fdfd565fc","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x76c01ab723e2980f","start_time":"2024-02-05T15:00:34.837373Z","end_time":"2024-02-05T15:00:35.377829Z","status":{"status_code":"UNSET"},"attributes":{"span_type":"Retrieval","retrieval.query":"How can attention be calculated in machine learning models?","retrieval.documents":"[\n    {\n        \"document.content\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n        \"document.score\": 0.3431832194328308,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf3\",\n            \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n            \"mtime\": null,\n            \"page_number\": 3,\n            \"stats\": {\n                \"tiktokens\": 551,\n                \"chars\": 2502,\n                \"lines\": 35\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n        \"document.score\": 0.3687279224395752,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf0\",\n            \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n            \"mtime\": null,\n            \"page_number\": 0,\n            \"stats\": {\n                \"tiktokens\": 668,\n                \"chars\": 2874,\n                \"lines\": 50\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfoutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni\\u2208Rdmodel\\u00d7dk,WK\\ni\\u2208Rdmodel\\u00d7dk,WV\\ni\\u2208Rdmodel\\u00d7dv\\nandWO\\u2208Rhdv\\u00d7dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel\/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\u2022In \\\"encoder-decoder attention\\\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\u2212\\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\\u221admodel.\\n5\",\n        \"document.score\": 0.3867444396018982,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf4\",\n            \"chunk_hash\": \"b0907ffb4546970af003d72d3c66917f5f097f5428599ff50f68323e538f0da6\",\n            \"mtime\": null,\n            \"page_number\": 4,\n            \"stats\": {\n                \"tiktokens\": 743,\n                \"chars\": 3190,\n                \"lines\": 49\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    }\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-12052/12053","correlationid":"da6ec6c9-523d-4327-8196-fa9dd411b268","xrequestid":"da6ec6c9-523d-4327-8196-fa9dd411b268","modelversion":"default"}
{"specversion":"1.0","id":"4f5d4032-8544-4cc2-a691-1941d16a8bf8","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:42Z","data":[{"name":"answer_the_question_with_context","context":{"request_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","trace_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","span_id":"0x5ede77c145eebbff","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xb1e57bf254752dd7","start_time":"2024-02-05T15:00:35.443959Z","end_time":"2024-02-05T15:00:37.703542Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"AzureOpenAI.chat","node_name":"answer_the_question_with_context","flow_id":"","root_run_id":"","line_run_id":"371856bd-457d-490c-8f4e-69f825b727d1","inputs":"{\n  \"prompt\": \"{{prompt_text}}\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"prompt_text\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfoutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni\\u2208Rdmodel\\u00d7dk,WK\\ni\\u2208Rdmodel\\u00d7dk,WV\\ni\\u2208Rdmodel\\u00d7dv\\nandWO\\u2208Rhdv\\u00d7dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel\/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\u2022In \\\"encoder-decoder attention\\\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\u2212\\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\\u221admodel.\\n5\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\n user: \\nwhat's Attention? \\nassistant: \\nAttention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf) \\nuser: how to calculate Attention? \\nassistant:\",\n  \"stream\": false\n}","output":"\"There are different ways to calculate attention, but one commonly used method is Scaled Dot-Product Attention. In this method, the input consists of queries and keys of dimension dk, and values of dimension dv. The dot products of the query with all keys are computed, divided by the square root of dk, and a softmax function is applied to obtain the weights on the values. The attention function can be computed on a set of queries simultaneously, packed together into a matrix Q, with the keys and values also packed together into matrices K and V. The matrix of outputs is then computed as Attention(Q, K, V) = softmax(QKT\/\\u221adk)V. (Source: 1706.03762.pdf)\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-12002/12003","correlationid":"aae1de29-8b3a-467a-b2d1-ada6044e215e","xrequestid":"aae1de29-8b3a-467a-b2d1-ada6044e215e","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"ef3b5b3f-cc9f-48c3-a5cc-55b8455f62d1","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:42Z","data":[{"name":"openai.resources.chat.completions.Completions.create","context":{"request_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","trace_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","span_id":"0xfd838350b49af420","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x5ede77c145eebbff","start_time":"2024-02-05T15:00:35.446699Z","end_time":"2024-02-05T15:00:37.701347Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.chat.completions.Completions.create","node_name":"answer_the_question_with_context","flow_id":"","root_run_id":"","line_run_id":"371856bd-457d-490c-8f4e-69f825b727d1","inputs":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfoutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni\\u2208Rdmodel\\u00d7dk,WK\\ni\\u2208Rdmodel\\u00d7dk,WV\\ni\\u2208Rdmodel\\u00d7dv\\nandWO\\u2208Rhdv\\u00d7dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel\/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\u2022In \\\"encoder-decoder attention\\\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\u2212\\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\\u221admodel.\\n5\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history:\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"what's Attention?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf) \\nuser: how to calculate Attention? \\nassistant:\"\n    }\n  ],\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8oukVwdxBlTok7CeTJgzgw2kZXOlp\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"There are different ways to calculate attention, but one commonly used method is Scaled Dot-Product Attention. In this method, the input consists of queries and keys of dimension dk, and values of dimension dv. The dot products of the query with all keys are computed, divided by the square root of dk, and a softmax function is applied to obtain the weights on the values. The attention function can be computed on a set of queries simultaneously, packed together into a matrix Q, with the keys and values also packed together into matrices K and V. The matrix of outputs is then computed as Attention(Q, K, V) = softmax(QKT\/\\u221adk)V. (Source: 1706.03762.pdf)\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707145235,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 146,\n    \"prompt_tokens\": 2296,\n    \"total_tokens\": 2442\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-12903/12904","correlationid":"fb6a34b3-ed79-48e9-a9ca-6b2b040b6318","xrequestid":"fb6a34b3-ed79-48e9-a9ca-6b2b040b6318","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default"}
{"specversion":"1.0","id":"e1ab38e5-5aee-4f6c-b0b7-83f8a8d46f22","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:00:42Z","data":[{"name":"promptflow.flow","context":{"request_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","trace_id":"0x1f1ba2e1d4bdeb64c858cd9c0fc941cf","span_id":"0xb1e57bf254752dd7","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":null,"start_time":"2024-02-05T15:00:34.166870Z","end_time":"2024-02-05T15:00:37.707934Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Flow","inputs":"{\n  \"question\": \"how to calculate Attention?\",\n  \"chat_history\": [\n    {\n      \"inputs\": {\n        \"question\": \"what's Attention?\"\n      },\n      \"outputs\": {\n        \"output\": \"Attention is a mechanism used in machine learning models to focus on specific parts of input data when making predictions or generating output. It allows the model to selectively weigh different parts of the input, giving more importance to certain aspects and ignoring others. (Source: 1706.03762.pdf)\"\n      }\n    }\n  ]\n}","output":"{\n  \"output\": \"There are different ways to calculate attention, but one commonly used method is Scaled Dot-Product Attention. In this method, the input consists of queries and keys of dimension dk, and values of dimension dv. The dot products of the query with all keys are computed, divided by the square root of dk, and a softmax function is applied to obtain the weights on the values. The attention function can be computed on a set of queries simultaneously, packed together into a matrix Q, with the keys and values also packed together into matrices K and V. The matrix of outputs is then computed as Attention(Q, K, V) = softmax(QKT\/\\u221adk)V. (Source: 1706.03762.pdf)\"\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"correlationid":"1f352b4a-da5b-4e01-911d-c15ecc6cee94","xrequestid":"1f352b4a-da5b-4e01-911d-c15ecc6cee94","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-1692/1693"}
{"specversion":"1.0","id":"2ae5bde4-03dd-44f8-8295-72d4081598db","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:44Z","data":[{"name":"modify_query_with_history","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0x3e0f97e41b612f51","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x40962014af720857","start_time":"2024-02-05T15:55:42.182727Z","end_time":"2024-02-05T15:55:42.858312Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"AzureOpenAI.chat","node_name":"modify_query_with_history","flow_id":"","root_run_id":"","line_run_id":"747b1ca2-59f5-412e-8cd4-5ba466662175","inputs":"{\n  \"prompt\": \"system: \\nGiven the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\n\\nuser:\\nFollow up Input: {{question}} \\nStandalone Question:\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"chat_history\": [],\n  \"question\": \"how to calculate Attention?\"\n}","output":"\"How do you calculate attention?\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"correlationid":"548d4b00-3544-4dd7-8e73-e97a4471af07","xrequestid":"548d4b00-3544-4dd7-8e73-e97a4471af07","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-1436/1437"}
{"specversion":"1.0","id":"18868efd-2c92-4817-8ad9-bcf0b22063b6","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:44Z","data":[{"name":"openai.resources.chat.completions.Completions.create","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0x2f58bef3b12f8bee","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x3e0f97e41b612f51","start_time":"2024-02-05T15:55:42.186629Z","end_time":"2024-02-05T15:55:42.856948Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.chat.completions.Completions.create","node_name":"modify_query_with_history","flow_id":"","root_run_id":"","line_run_id":"747b1ca2-59f5-412e-8cd4-5ba466662175","inputs":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Given the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history:\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Follow up Input: how to calculate Attention? \\nStandalone Question:\"\n    }\n  ],\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": null,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8ovbqoSgaNmkLQ60aNL3kzPlKsvoC\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"How do you calculate attention?\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707148542,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 6,\n    \"prompt_tokens\": 79,\n    \"total_tokens\": 85\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"correlationid":"c93f1431-482c-4c8c-9503-065f53d3f937","xrequestid":"c93f1431-482c-4c8c-9503-065f53d3f937","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-2133/2134"}
{"specversion":"1.0","id":"5f69c036-05ce-4608-ae3e-8e086124afee","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:44Z","data":[{"name":"prepare_question_array","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0xe639855ba83c0d6e","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x40962014af720857","start_time":"2024-02-05T15:55:42.862647Z","end_time":"2024-02-05T15:55:42.863368Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"my_python_tool","node_name":"prepare_question_array","flow_id":"","root_run_id":"","line_run_id":"747b1ca2-59f5-412e-8cd4-5ba466662175","inputs":"{\n  \"modified_question\": \"How do you calculate attention?\",\n  \"original_question\": \"how to calculate Attention?\"\n}","output":"[\n  \"How do you calculate attention?\",\n  \"how to calculate Attention?\"\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-848/849","correlationid":"b4715a52-0375-430c-a820-75dd690a21f3","xrequestid":"b4715a52-0375-430c-a820-75dd690a21f3","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame"}
{"specversion":"1.0","id":"f1e31d41-2784-49b6-9e31-dd1d4a3c744f","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:49Z","data":[{"name":"openai.resources.embeddings.Embeddings.create","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0x26dac8a675a4780a","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x9b7433077fc39708","start_time":"2024-02-05T15:55:45.964884Z","end_time":"2024-02-05T15:55:46.537846Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.embeddings.Embeddings.create","node_name":"vector_lookup","flow_id":"","root_run_id":"","line_run_id":"747b1ca2-59f5-412e-8cd4-5ba466662175","inputs":"{\n  \"input\": [\n    [\n      4438,\n      656,\n      499,\n      11294,\n      6666,\n      30\n    ]\n  ],\n  \"model\": \"text-embedding-ada-002\"\n}","output":"{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.012303137220442295,\n        0.02535068802535534,\n        0.047507163137197495,\n        -0.029830709099769592,\n        0.018840447068214417,\n        0.009068318642675877,\n        -0.02173689566552639,\n        -0.021493269130587578,\n        -0.01464465819299221,\n        -0.018380263820290565,\n        0.031211258843541145,\n        0.040035951882600784,\n        -0.026920726522803307,\n        0.017784733325242996,\n        0.006977191660553217,\n        -0.007877255789935589,\n        0.02770574577152729,\n        0.018014824017882347,\n        0.0051736789755523205,\n        -0.026812447234988213,\n        -0.03481151908636093,\n        0.01078723929822445,\n        -0.010482706129550934,\n        0.003478444879874587,\n        -0.013108458369970322,\n        0.005759059451520443,\n        0.021831639111042023,\n        -0.01011049933731556,\n        -0.004425880964845419,\n        -0.039359211921691895,\n        0.025079993531107903,\n        -0.002368590794503689,\n        0.00022945723321754485,\n        -0.022386565804481506,\n        -0.014170940034091473,\n        0.015389072708785534,\n        0.003319410840049386,\n        -0.009291643276810646,\n        0.03004726581275463,\n        -0.0038608030881732702,\n        0.02996605634689331,\n        0.012519693933427334,\n        -0.002827082294970751,\n        0.009413456544280052,\n        -0.0005012107430957258,\n        0.013568641617894173,\n        -0.00496050575748086,\n        -0.040333718061447144,\n        -0.00713284220546484,\n        0.023374607786536217,\n        0.006733565125614405,\n        0.02693426050245762,\n        -0.02498525008559227,\n        -0.020410485565662384,\n        0.004084127489477396,\n        0.006557612679898739,\n        0.01721627078950405,\n        0.0015683454694226384,\n        0.011599327437579632,\n        -0.0020403717644512653,\n        0.011112074367702007,\n        -0.01668841391801834,\n        -0.007396770641207695,\n        0.01774412952363491,\n        -0.01192416250705719,\n        -0.001871186774224043,\n        0.018082499504089355,\n        0.005654164589941502,\n        -0.0050146449357271194,\n        0.0012934197438880801,\n        0.014563449658453465,\n        0.023063307628035545,\n        0.005650781095027924,\n        -0.012688878923654556,\n        0.04323016479611397,\n        -0.010184939950704575,\n        -0.01984202302992344,\n        -0.018962260335683823,\n        0.02950587309896946,\n        0.022508379071950912,\n        0.013101690448820591,\n        -0.0015065929619595408,\n        0.005782745312899351,\n        0.032781295478343964,\n        -0.01593046449124813,\n        -0.016214696690440178,\n        0.009514967910945415,\n        0.018434403464198112,\n        -0.008872064761817455,\n        0.010760169476270676,\n        -0.006889215670526028,\n        -0.03134660795331001,\n        0.019517188891768456,\n        0.01109853945672512,\n        0.011822652071714401,\n        0.017338084056973457,\n        -0.016174091026186943,\n        0.0222647525370121,\n        -0.021926382556557655,\n        -0.013446828350424767,\n        0.007295259740203619,\n        0.007390003185719252,\n        -0.047128189355134964,\n        -0.01336561981588602,\n        0.015158981084823608,\n        -0.00992101151496172,\n        -0.002862611087039113,\n        -0.010665426030755043,\n        -0.022657262161374092,\n        0.009406689554452896,\n        -0.016729017719626427,\n        0.03140074759721756,\n        -0.0002859227533917874,\n        -0.02000444196164608,\n        -0.023888930678367615,\n        0.014861215837299824,\n        0.0010066510876640677,\n        -0.01848854310810566,\n        -0.0348927266895771,\n        -0.020193928852677345,\n        0.0330519936978817,\n        -0.004205940291285515,\n        0.01570037379860878,\n        -0.00860136840492487,\n        0.013609246350824833,\n        0.042391009628772736,\n        0.017419293522834778,\n        -0.01820431277155876,\n        -0.01879984326660633,\n        -0.014807076193392277,\n        0.005799663718789816,\n        0.02869378589093685,\n        -0.0013551722513511777,\n        0.019517188891768456,\n        -0.01840733364224434,\n        0.017852406948804855,\n        0.022237684577703476,\n        0.008520159870386124,\n        -0.037545546889305115,\n        0.0004851381527259946,\n        0.0007169216987676919,\n        0.012066278606653214,\n        0.0073629338294267654,\n        -0.026514682918787003,\n        -0.0026866586413234472,\n        0.04428588226437569,\n        0.016620740294456482,\n        0.030317962169647217,\n        -0.008391578681766987,\n        -0.004324370063841343,\n        0.007944930344820023,\n        -0.003728838637471199,\n        0.0014862907119095325,\n        0.013724291697144508,\n        -0.004500322509557009,\n        0.014198009856045246,\n        -0.021994058042764664,\n        0.012248998507857323,\n        0.005468061193823814,\n        -0.02038341574370861,\n        0.0038743377663195133,\n        0.024430321529507637,\n        0.005495130550116301,\n        -0.018583286553621292,\n        0.024335578083992004,\n        0.0407397598028183,\n        0.01751403696835041,\n        -0.0016326358309015632,\n        -0.007579490542411804,\n        -0.007071935571730137,\n        -0.015267259441316128,\n        0.030101405456662178,\n        -0.030696935951709747,\n        0.013264108449220657,\n        -0.018447939306497574,\n        0.00577259436249733,\n        0.03895316645503044,\n        -0.01042179949581623,\n        0.0018170474795624614,\n        -0.029153969138860703,\n        -0.013737826608121395,\n        0.015389072708785534,\n        -0.0004513011372182518,\n        0.013717524707317352,\n        -0.01879984326660633,\n        -0.006730181630700827,\n        0.006726797670125961,\n        -0.006652356591075659,\n        -0.014428101480007172,\n        -0.021181968972086906,\n        0.015497351065278053,\n        0.023279864341020584,\n        -0.02068118192255497,\n        -0.02415962517261505,\n        -0.6518361568450928,\n        -0.0009592792484909296,\n        0.007606560364365578,\n        -0.022481311112642288,\n        0.0013543263776227832,\n        0.00522781815379858,\n        0.005058633163571358,\n        -0.0077283731661736965,\n        -0.013940848410129547,\n        0.03529876843094826,\n        0.01729748025536537,\n        -0.0034327649045735598,\n        0.002591914962977171,\n        -0.0034276891965419054,\n        0.006479787640273571,\n        -0.023807721212506294,\n        0.003319410840049386,\n        -0.028287742286920547,\n        -0.0033853929489851,\n        0.02861257642507553,\n        -0.04488141089677811,\n        0.019909696653485298,\n        -0.004946970846503973,\n        0.00705840066075325,\n        -0.0017028475413098931,\n        -0.0063512069173157215,\n        -0.013237038627266884,\n        -0.027204956859350204,\n        -0.009264573454856873,\n        0.023361071944236755,\n        0.0047777858562767506,\n        -0.005826733540743589,\n        0.017906546592712402,\n        0.02233242802321911,\n        0.045774709433317184,\n        -0.026528216898441315,\n        -0.005078935530036688,\n        0.013751361519098282,\n        -0.03954869881272316,\n        0.04285119101405144,\n        -0.014807076193392277,\n        -0.026081567630171776,\n        0.023685907945036888,\n        -0.011707605794072151,\n        -0.02061350643634796,\n        -0.025526640936732292,\n        0.008296835236251354,\n        -0.006530543323606253,\n        0.0027424897998571396,\n        -0.030507449060678482,\n        -0.01779826730489731,\n        0.004534159321337938,\n        -0.008905901573598385,\n        0.004507089965045452,\n        0.010340590961277485,\n        0.0045172409154474735,\n        0.0118497209623456,\n        0.017865942791104317,\n        0.01593046449124813,\n        -0.012621205300092697,\n        -0.017906546592712402,\n        0.011660234071314335,\n        -0.008621670305728912,\n        -0.007917860522866249,\n        -0.00554926972836256,\n        0.0074644447304308414,\n        0.004202556796371937,\n        0.041633058339357376,\n        0.010286451317369938,\n        -0.02430850826203823,\n        -0.009332248009741306,\n        0.0344325415790081,\n        -0.00912922527641058,\n        0.03267301991581917,\n        0.02618984691798687,\n        0.005400387104600668,\n        0.027827559038996696,\n        0.013189666904509068,\n        -0.009156295098364353,\n        0.012120417319238186,\n        -0.009508199989795685,\n        -0.004997726529836655,\n        -0.003945395350456238,\n        -0.012228695675730705,\n        -0.003031796310096979,\n        0.0051702954806387424,\n        -0.008831460028886795,\n        -0.0023364457301795483,\n        0.0065068574622273445,\n        -0.005342863965779543,\n        0.01042179949581623,\n        0.00773514062166214,\n        0.0013035708107054234,\n        -0.03313320130109787,\n        -0.003172219730913639,\n        0.014766471460461617,\n        -0.0032991087064146996,\n        0.005938395392149687,\n        0.033106133341789246,\n        -0.01365661807358265,\n        -0.021912848576903343,\n        -0.019869092851877213,\n        0.0013314863899722695,\n        -0.0005629632505588233,\n        0.032781295478343964,\n        -0.011051167733967304,\n        -0.01193093042820692,\n        0.01932770013809204,\n        0.04006301984190941,\n        -0.014306288212537766,\n        -0.00526842288672924,\n        0.0007820579339750111,\n        0.004043522756546736,\n        0.008222393691539764,\n        0.01586279086768627,\n        -0.024281438440084457,\n        0.03713950514793396,\n        0.011125609278678894,\n        0.002999651012942195,\n        0.003246661275625229,\n        0.0169861800968647,\n        -0.0030165696516633034,\n        0.030615728348493576,\n        -0.03640862554311752,\n        -0.009711221791803837,\n        0.017865942791104317,\n        0.009081853553652763,\n        -0.009041249752044678,\n        -0.014617589302361012,\n        -0.001679161679930985,\n        0.005420689005404711,\n        0.010367659851908684,\n        -0.010611286386847496,\n        -0.00913599319756031,\n        0.019652536138892174,\n        0.02543189749121666,\n        -0.029993126168847084,\n        -0.034161847084760666,\n        0.0003652282466646284,\n        -0.042391009628772736,\n        -0.019300632178783417,\n        0.010232312604784966,\n        0.014401031658053398,\n        -0.027651606127619743,\n        -0.023685907945036888,\n        -0.005454526282846928,\n        -0.011423375457525253,\n        0.01147074718028307,\n        0.002524241106584668,\n        0.0022129404824227095,\n        0.012763320468366146,\n        -0.02346935123205185,\n        -0.0242137648165226,\n        0.0042533124797046185,\n        -0.013825803063809872,\n        -0.019652536138892174,\n        -0.01475293654948473,\n        -0.018366729840636253,\n        -0.01774412952363491,\n        -0.03004726581275463,\n        0.018136637285351753,\n        0.028802063316106796,\n        -0.011795582249760628,\n        -0.002282306319102645,\n        -0.022305358201265335,\n        -0.0056304787285625935,\n        -0.011748210527002811,\n        0.022846750915050507,\n        -0.038899026811122894,\n        -0.020017975941300392,\n        0.0030402555130422115,\n        -0.019733745604753494,\n        -0.0009262881940230727,\n        0.014414566569030285,\n        -0.008398346602916718,\n        0.015822187066078186,\n        -0.009481130167841911,\n        -0.015713907778263092,\n        0.007870488800108433,\n        -0.016498927026987076,\n        0.012181323952972889,\n        0.015145446173846722,\n        -0.023753581568598747,\n        -0.001845808932557702,\n        0.028125323355197906,\n        0.020396949723362923,\n        0.01599813811480999,\n        0.008080278523266315,\n        -0.011802349239587784,\n        0.008669042028486729,\n        0.0033989278599619865,\n        0.005244736559689045,\n        -0.009623246267437935,\n        0.010299986228346825,\n        0.006513624452054501,\n        0.007295259740203619,\n        0.01804189383983612,\n        -0.0002029163297265768,\n        0.0016368654323741794,\n        0.02740797959268093,\n        0.03998181223869324,\n        0.01863742619752884,\n        0.023509955033659935,\n        -0.0056304787285625935,\n        0.016052277758717537,\n        0.017040319740772247,\n        0.006026371847838163,\n        -0.01758171059191227,\n        0.017771197482943535,\n        0.012323439121246338,\n        0.020288672298192978,\n        -0.01676962338387966,\n        -0.013189666904509068,\n        -0.006672658491879702,\n        0.005867337808012962,\n        0.020396949723362923,\n        0.0039995345287024975,\n        0.012512926943600178,\n        -0.002443032106384635,\n        0.02066764608025551,\n        0.04009009152650833,\n        -0.0026460543740540743,\n        0.02000444196164608,\n        -0.006324137561023235,\n        0.02376711741089821,\n        -0.007139609195291996,\n        0.020627042278647423,\n        0.03735605999827385,\n        0.008195323869585991,\n        -0.019882628694176674,\n        -0.01888105273246765,\n        -0.009075086563825607,\n        0.019977372139692307,\n        -0.0023499804083257914,\n        0.018840447068214417,\n        -0.012594135478138924,\n        0.0053327130153775215,\n        0.006638821680098772,\n        0.01591693051159382,\n        -0.0019422444747760892,\n        -0.0022095567546784878,\n        0.0169861800968647,\n        -0.0021486501209437847,\n        -0.021709825843572617,\n        0.013237038627266884,\n        0.015267259441316128,\n        0.04905012995004654,\n        0.02188577875494957,\n        0.027462119236588478,\n        -0.0008429645677097142,\n        -0.020017975941300392,\n        -0.0018356578657403588,\n        -0.001463450724259019,\n        0.009027714841067791,\n        -0.009420223534107208,\n        0.004899599123746157,\n        -0.001630097976885736,\n        -0.001567499595694244,\n        -0.007938162423670292,\n        0.03462202847003937,\n        0.020261602476239204,\n        0.017676454037427902,\n        0.016065813601017,\n        -0.009345782920718193,\n        -0.008817925117909908,\n        0.005177062936127186,\n        -0.015213120728731155,\n        0.0016816994175314903,\n        0.008709646761417389,\n        -0.0012511234963312745,\n        0.002627443987876177,\n        -0.01713506318628788,\n        -0.006594833452254534,\n        -0.00036628564703278244,\n        0.01570037379860878,\n        0.0034852121025323868,\n        -0.017933616414666176,\n        -0.009826268069446087,\n        0.010387962684035301,\n        0.01796068623661995,\n        -0.013074621558189392,\n        -0.023753581568598747,\n        0.01721627078950405,\n        0.0014710640534758568,\n        -0.011443677358329296,\n        -0.03245646134018898,\n        0.012986645102500916,\n        -0.0050755515694618225,\n        -0.009521734900772572,\n        -0.005468061193823814,\n        -0.013683686964213848,\n        -0.0004329022776801139,\n        0.011531653814017773,\n        -0.0227249376475811,\n        -0.01154518872499466,\n        -0.006604984402656555,\n        0.010543612763285637,\n        0.013094923458993435,\n        0.003246661275625229,\n        -0.03502807393670082,\n        0.004547694232314825,\n        -0.004408962558954954,\n        -0.02996605634689331,\n        -0.008046441711485386,\n        -0.004757483955472708,\n        -0.003097778419032693,\n        -0.008912668563425541,\n        -0.022927958518266678,\n        -0.016634274274110794,\n        -0.016701947897672653,\n        0.008391578681766987,\n        -0.0030520984437316656,\n        -0.0021960220765322447,\n        -0.006124499253928661,\n        0.01886751689016819,\n        0.0068655298091471195,\n        -0.007572723086923361,\n        -0.0006467098719440401,\n        0.02467394806444645,\n        0.02648761309683323,\n        -0.006222626194357872,\n        -0.01835319586098194,\n        0.002909983042627573,\n        -0.003187446389347315,\n        0.05630478635430336,\n        0.004899599123746157,\n        -0.013189666904509068,\n        0.019300632178783417,\n        -0.030669867992401123,\n        0.015158981084823608,\n        -0.02061350643634796,\n        -0.01779826730489731,\n        0.02338814176619053,\n        -0.013054318726062775,\n        -0.026243986561894417,\n        -0.021872244775295258,\n        0.002713728230446577,\n        -0.000576075108256191,\n        0.03134660795331001,\n        0.009054784663021564,\n        -0.001485444838181138,\n        -0.017378689721226692,\n        0.005552653688937426,\n        0.008851761929690838,\n        0.004402195103466511,\n        -0.002898140111938119,\n        0.01048947311937809,\n        0.012776855379343033,\n        0.010895517654716969,\n        0.005420689005404711,\n        -0.001796745345927775,\n        0.016106417402625084,\n        0.010997029021382332,\n        -0.004425880964845419,\n        0.013311480171978474,\n        0.013758128508925438,\n        -0.008317137137055397,\n        0.004889448173344135,\n        -0.023943068459630013,\n        0.021615082398056984,\n        0.012695646844804287,\n        0.0002912097843363881,\n        0.027367373928427696,\n        -0.01963900215923786,\n        0.008892366662621498,\n        0.029018620029091835,\n        -0.001747681642882526,\n        -0.007863721810281277,\n        -8.771188004175201e-05,\n        -0.01820431277155876,\n        0.00711253983899951,\n        0.01622823067009449,\n        0.00433790497481823,\n        0.008892366662621498,\n        -0.00528195733204484,\n        -0.012485857121646404,\n        -0.01697264425456524,\n        0.0011868331348523498,\n        0.001171606476418674,\n        0.009799198247492313,\n        -0.022833215072751045,\n        -0.012127185240387917,\n        0.00044411077396944165,\n        -0.03207748755812645,\n        -0.02980363927781582,\n        -0.00019625466666184366,\n        0.020261602476239204,\n        -0.004699960816651583,\n        0.011409840546548367,\n        -0.01916528306901455,\n        -0.009264573454856873,\n        -0.023293398320674896,\n        -0.009433758445084095,\n        -0.0098127331584692,\n        0.004087510984390974,\n        -0.017622316256165504,\n        -0.054301634430885315,\n        -0.004172103479504585,\n        0.026216916739940643,\n        0.013588943518698215,\n        0.013839337974786758,\n        0.005701536312699318,\n        -0.01863742619752884,\n        0.003826966043561697,\n        -0.009014179930090904,\n        -0.02612217329442501,\n        -0.015713907778263092,\n        -0.00420932425186038,\n        -0.006686193402856588,\n        0.017026783898472786,\n        0.007274957373738289,\n        0.016404183581471443,\n        0.0021994058042764664,\n        0.0028050881810486317,\n        -0.009630013257265091,\n        0.021912848576903343,\n        0.05178416147828102,\n        -0.034757379442453384,\n        0.03094056248664856,\n        0.01644478738307953,\n        -0.00524812052026391,\n        0.0077283731661736965,\n        0.01758171059191227,\n        -0.013419758528470993,\n        -0.005292108748108149,\n        -0.030534518882632256,\n        -0.008100580424070358,\n        0.002182487165555358,\n        -0.0005934165674261749,\n        0.00515337660908699,\n        0.001984540605917573,\n        0.016580134630203247,\n        0.010699262842535973,\n        -0.02566199004650116,\n        -0.009305178187787533,\n        -0.025486037135124207,\n        -0.017689989879727364,\n        0.0006843535811640322,\n        0.02942466549575329,\n        -0.005786128807812929,\n        0.02257605455815792,\n        0.01683729700744152,\n        -0.004090894479304552,\n        0.009548804722726345,\n        0.0019371688831597567,\n        -0.029560012742877007,\n        0.0359213724732399,\n        0.031130051240324974,\n        -0.011348933912813663,\n        -0.010360892862081528,\n        -0.010232312604784966,\n        -0.007159911561757326,\n        -0.008256230503320694,\n        0.002739106072112918,\n        0.0004214822838548571,\n        0.010252614505589008,\n        0.0007604868151247501,\n        -0.05224434658885002,\n        -0.02242717146873474,\n        -0.01840733364224434,\n        -0.036868806928396225,\n        0.0033853929489851,\n        -0.013561873696744442,\n        0.011335399001836777,\n        -0.002275539096444845,\n        -0.01551088597625494,\n        -0.02836894989013672,\n        -0.025039387866854668,\n        0.04428588226437569,\n        -0.027150817215442657,\n        -0.02617631107568741,\n        0.03161730244755745,\n        -0.01591693051159382,\n        0.012627972289919853,\n        -0.008472787216305733,\n        0.002600374398753047,\n        -0.04896892234683037,\n        -0.017256876453757286,\n        0.01490181963890791,\n        -0.03102177195250988,\n        -0.005962081253528595,\n        0.007762210443615913,\n        0.03992767259478569,\n        0.010272916406393051,\n        0.01992323249578476,\n        0.019530722871422768,\n        -0.00264097866602242,\n        0.01392731349915266,\n        -0.00354611873626709,\n        -0.003854035632684827,\n        0.012289602309465408,\n        -0.017554640769958496,\n        -0.0005079781403765082,\n        0.0016385572962462902,\n        0.007254655007272959,\n        -0.008073510602116585,\n        0.00906155165284872,\n        0.007484747096896172,\n        0.027651606127619743,\n        0.012309905141592026,\n        -0.013121993280947208,\n        -0.012580600567162037,\n        -0.022305358201265335,\n        -0.012844529934227467,\n        0.014306288212537766,\n        0.001734146848320961,\n        -0.012242230586707592,\n        0.008337439969182014,\n        -0.015808651223778725,\n        -0.00042761521763168275,\n        0.007917860522866249,\n        0.018299056217074394,\n        0.010191707871854305,\n        0.020424019545316696,\n        0.011694070883095264,\n        -0.021019551903009415,\n        0.006364741828292608,\n        0.010320288129150867,\n        0.042309798300266266,\n        0.0025225491262972355,\n        -0.022481311112642288,\n        -0.0012105191126465797,\n        0.003099470166489482,\n        0.019977372139692307,\n        0.015375537797808647,\n        0.0033143353648483753,\n        0.006232777610421181,\n        0.013764896430075169,\n        -0.017026783898472786,\n        0.005386852193623781,\n        -0.0063512069173157215,\n        -0.01984202302992344,\n        0.006239545065909624,\n        0.0036036416422575712,\n        -0.0236994419246912,\n        -0.010232312604784966,\n        -0.006016220431774855,\n        0.003243277547881007,\n        0.010699262842535973,\n        0.009589409455657005,\n        -0.004957122262567282,\n        0.0023618233390152454,\n        -0.011673768982291222,\n        -0.01494242437183857,\n        0.021290248259902,\n        -0.01053007785230875,\n        0.02785462699830532,\n        -0.010164638049900532,\n        0.019598396494984627,\n        -0.0028440007008612156,\n        0.011166214011609554,\n        -0.0021892546210438013,\n        0.013981453143060207,\n        0.002341521205380559,\n        0.009542036801576614,\n        0.015646234154701233,\n        0.0034885958302766085,\n        -0.0215744785964489,\n        -0.007153144106268883,\n        -0.001641941023990512,\n        0.011409840546548367,\n        -0.004084127489477396,\n        -0.029641222208738327,\n        0.014766471460461617,\n        0.010475939139723778,\n        -0.01616055704653263,\n        -0.03085935488343239,\n        -0.019977372139692307,\n        -0.010022522881627083,\n        0.04220151901245117,\n        0.00849309004843235,\n        0.004490171559154987,\n        0.01136246882379055,\n        -0.006601600907742977,\n        -0.026514682918787003,\n        0.020802995190024376,\n        0.00048429222078993917,\n        0.021845174953341484,\n        0.021006016060709953,\n        0.0064087300561368465,\n        -0.00463905418291688,\n        0.012336974032223225,\n        -0.004233010113239288,\n        0.017162133008241653,\n        0.008709646761417389,\n        -0.010442101396620274,\n        -0.032104555517435074,\n        0.02127671241760254,\n        -0.0035833395086228848,\n        -0.019977372139692307,\n        0.0074644447304308414,\n        0.0014093115460127592,\n        -0.012289602309465408,\n        0.029181038960814476,\n        -0.025865010917186737,\n        -0.019449513405561447,\n        -0.00015787080337759107,\n        -0.014604054391384125,\n        0.007938162423670292,\n        0.00666250754147768,\n        -0.01283099502325058,\n        0.006496706046164036,\n        -0.007593025453388691,\n        -0.029099829494953156,\n        0.02475515753030777,\n        -0.00703809829428792,\n        -0.0222647525370121,\n        0.012614437378942966,\n        0.005481595639139414,\n        0.034297194331884384,\n        -0.028125323355197906,\n        0.01283099502325058,\n        0.007146376650780439,\n        -0.004940203856676817,\n        -0.012973110191524029,\n        0.003084243508055806,\n        0.007302027195692062,\n        0.019057005643844604,\n        0.002065749606117606,\n        -0.0018864134326577187,\n        0.004798088222742081,\n        0.005874105263501406,\n        -0.017175666987895966,\n        0.0025073224678635597,\n        0.01279039029031992,\n        -0.013900244608521461,\n        -0.008926203474402428,\n        -0.008087045513093472,\n        -0.017256876453757286,\n        -0.05500544607639313,\n        0.02648761309683323,\n        -0.017771197482943535,\n        -0.01802835986018181,\n        -0.012966343201696873,\n        0.011274492368102074,\n        0.00359349069185555,\n        0.012248998507857323,\n        -0.004040139261633158,\n        -0.027380909770727158,\n        0.0224948450922966,\n        0.007322329096496105,\n        -0.0067504835315048695,\n        0.015984604135155678,\n        -0.012959575280547142,\n        0.012370811775326729,\n        -0.00679108826443553,\n        0.03578602150082588,\n        -0.003945395350456238,\n        0.023279864341020584,\n        -0.016133487224578857,\n        0.010293219238519669,\n        0.01576804742217064,\n        -0.017419293522834778,\n        -0.02137145586311817,\n        -0.026447007432579994,\n        0.02068118192255497,\n        -0.027218492701649666,\n        -0.00830360222607851,\n        0.016796693205833435,\n        0.0008370430441573262,\n        -0.012127185240387917,\n        0.03960283845663071,\n        0.010942889377474785,\n        -0.031292468309402466,\n        -0.018150173127651215,\n        -0.0052718063816428185,\n        0.003972465172410011,\n        0.014049126766622066,\n        -0.02942466549575329,\n        -0.010739867575466633,\n        -0.005298876203596592,\n        -0.00898711010813713,\n        -0.015307864174246788,\n        -0.03012847527861595,\n        -0.0155514907091856,\n        0.023509955033659935,\n        0.02640640363097191,\n        -0.014563449658453465,\n        -0.014238614588975906,\n        -0.015673303976655006,\n        -0.031806789338588715,\n        -0.01154518872499466,\n        -0.021290248259902,\n        -0.004625519271939993,\n        -0.0019388607470318675,\n        0.0014118492836132646,\n        -0.011971534229815006,\n        0.022305358201265335,\n        -0.019611932337284088,\n        -0.026704169809818268,\n        -0.018393799662590027,\n        0.0071260747499763966,\n        -0.005941779352724552,\n        -0.014238614588975906,\n        0.01222192868590355,\n        -0.02165568806231022,\n        -0.02437618374824524,\n        -0.034459613263607025,\n        -0.0013078005285933614,\n        -0.007836651988327503,\n        -0.023144515231251717,\n        0.016336509957909584,\n        0.00596546521410346,\n        -0.008520159870386124,\n        0.005627094767987728,\n        0.037247780710458755,\n        0.032266974449157715,\n        0.0031028538942337036,\n        0.0077283731661736965,\n        -0.023036237806081772,\n        0.006303835194557905,\n        0.005051865708082914,\n        -9.019678691402078e-05,\n        -0.01850207708775997,\n        0.01494242437183857,\n        0.021154899150133133,\n        -0.02309037744998932,\n        0.0030943946912884712,\n        -0.026392869651317596,\n        -0.012052743695676327,\n        0.0005422381218522787,\n        -0.00020608854538295418,\n        0.016620740294456482,\n        0.018177242949604988,\n        -0.023740047588944435,\n        0.03026382252573967,\n        0.016729017719626427,\n        -0.014766471460461617,\n        -0.03056158870458603,\n        0.019652536138892174,\n        -0.003579955780878663,\n        -0.02309037744998932,\n        0.019747279584407806,\n        -0.03464910015463829,\n        -0.005853802897036076,\n        0.012553530745208263,\n        -0.007437374908477068,\n        0.0007245350279845297,\n        0.008425415493547916,\n        0.0023059924133121967,\n        -0.021249642595648766,\n        -0.026230450719594955,\n        0.012743018567562103,\n        0.014807076193392277,\n        -0.02046462520956993,\n        0.004161952529102564,\n        -0.024660414084792137,\n        -0.005112772341817617,\n        -0.0011064702412113547,\n        0.006110964342951775,\n        0.005847035441547632,\n        -0.02581087313592434,\n        -0.008811158128082752,\n        0.01850207708775997,\n        -0.017635850235819817,\n        0.0028676867950707674,\n        0.008053208701312542,\n        0.016336509957909584,\n        -0.0003893371031153947,\n        0.012702413834631443,\n        0.2410278022289276,\n        -0.009968383237719536,\n        -0.01691850647330284,\n        0.032564740628004074,\n        -0.00314345839433372,\n        0.015456746332347393,\n        0.024362647905945778,\n        -0.0015886477194726467,\n        0.015023632906377316,\n        0.010124034248292446,\n        0.012418183498084545,\n        -0.02468748390674591,\n        -0.03237525373697281,\n        0.003657780820503831,\n        0.0018305822741240263,\n        -0.01576804742217064,\n        -0.027502723038196564,\n        -0.023807721212506294,\n        -0.005941779352724552,\n        -0.017825337126851082,\n        0.021019551903009415,\n        0.0029438200872391462,\n        -0.017378689721226692,\n        -0.019828489050269127,\n        0.032943714410066605,\n        -0.0035325840581208467,\n        0.001003267359919846,\n        -0.015727443620562553,\n        0.014725867658853531,\n        0.01128802727907896,\n        -0.012167789041996002,\n        0.0033887766767293215,\n        0.0006797009846195579,\n        0.01712152734398842,\n        -0.032185766845941544,\n        0.010557147674262524,\n        -0.013034016825258732,\n        -0.00698395911604166,\n        0.03004726581275463,\n        0.009670617990195751,\n        0.013352084904909134,\n        -0.003877721494063735,\n        -0.017486967146396637,\n        0.00401645340025425,\n        -0.012215160764753819,\n        0.012377578765153885,\n        0.013270875439047813,\n        -0.039656978100538254,\n        0.008553996682167053,\n        0.033106133341789246,\n        -0.020572902634739876,\n        -0.013264108449220657,\n        0.0074644447304308414,\n        0.020735319703817368,\n        -0.024186694994568825,\n        0.011037632822990417,\n        -4.348584479885176e-05,\n        -2.823275644914247e-05,\n        0.008330672048032284,\n        0.05495130643248558,\n        -0.007532118819653988,\n        0.016322974115610123,\n        -0.01563269831240177,\n        0.036192066967487335,\n        0.002378741977736354,\n        0.0025834557600319386,\n        -0.009501432999968529,\n        0.020112719386816025,\n        0.012932505458593369,\n        -0.03345803543925285,\n        -0.007951697334647179,\n        -0.028639646247029305,\n        -0.004297300241887569,\n        -0.026663564145565033,\n        -0.0368417389690876,\n        -0.006297067739069462,\n        0.013277643360197544,\n        0.01931416615843773,\n        0.023807721212506294,\n        0.02861257642507553,\n        -0.011802349239587784,\n        -0.029397595673799515,\n        0.00032991086482070386,\n        0.004818390589207411,\n        -0.01267534401267767,\n        -0.024132557213306427,\n        -0.015727443620562553,\n        -0.007335864007472992,\n        -0.013798733241856098,\n        0.005742141045629978,\n        0.018853982910513878,\n        -0.015565025620162487,\n        -0.03072400577366352,\n        -0.0019523955415934324,\n        -0.0034479915630072355,\n        -0.01622823067009449,\n        0.010760169476270676,\n        0.007579490542411804,\n        0.0020742088090628386,\n        0.01354833971709013,\n        -0.008438950404524803,\n        0.017825337126851082,\n        0.029316386207938194,\n        -0.0011428450234234333,\n        -0.008838227018713951,\n        -0.0028930644039064646,\n        -0.006368125323206186,\n        -0.002999651012942195,\n        0.0003277960349805653,\n        -0.022657262161374092,\n        -0.014468706212937832,\n        -0.03221283480525017,\n        0.005041714757680893,\n        -0.009893941693007946,\n        -0.004073976073414087,\n        0.001262120553292334,\n        0.0005836884374730289,\n        0.0041145808063447475,\n        0.019950302317738533,\n        -0.019260026514530182,\n        -0.0027103445027023554,\n        -0.021019551903009415,\n        0.009521734900772572,\n        0.007748675532639027,\n        0.01629590429365635,\n        -0.015876324847340584,\n        -0.014441636390984058,\n        -0.006730181630700827,\n        0.002938744379207492,\n        -0.005559421144425869,\n        0.0010997029021382332,\n        -0.01916528306901455,\n        0.005545886233448982,\n        0.001549735083244741,\n        -0.01721627078950405,\n        -0.00864197313785553,\n        0.0031180805526673794,\n        -0.019950302317738533,\n        0.0018897970439866185,\n        0.00013904896331951022,\n        -0.005941779352724552,\n        -0.023130981251597404,\n        0.013027248904109001,\n        -0.01154518872499466,\n        0.018759239464998245,\n        -0.02107369154691696,\n        0.014089731499552727,\n        0.0026223682798445225,\n        -0.0072072832845151424,\n        -0.02204819582402706,\n        -0.01743282750248909,\n        0.001085322117432952,\n        0.004730414133518934,\n        -0.01128125935792923,\n        -0.003332945518195629,\n        0.004940203856676817,\n        0.006523775868117809,\n        -0.034757379442453384,\n        0.01728394627571106,\n        -0.02309037744998932,\n        0.009149528108537197,\n        -0.001485444838181138,\n        -0.005911326035857201,\n        -0.002735722344368696,\n        -0.012282835319638252,\n        0.00985333789139986,\n        -0.17346204817295074,\n        0.018475007265806198,\n        0.030101405456662178,\n        -0.00924427155405283,\n        0.002194330096244812,\n        -0.01211365032941103,\n        0.004588298965245485,\n        0.009190131910145283,\n        -0.03540704771876335,\n        0.013453595340251923,\n        0.022021127864718437,\n        -0.027665140107274055,\n        -0.028423089534044266,\n        -0.029262246564030647,\n        0.011795582249760628,\n        -0.017933616414666176,\n        -0.0010785547783598304,\n        0.012932505458593369,\n        0.014455171301960945,\n        0.01691850647330284,\n        0.047588374465703964,\n        0.010401497595012188,\n        0.012844529934227467,\n        0.008175021968781948,\n        -0.014319823123514652,\n        -0.004138266667723656,\n        -0.018001290038228035,\n        0.01668841391801834,\n        0.016038743779063225,\n        -0.02663649618625641,\n        0.00015290098963305354,\n        -0.014292753301560879,\n        0.003431072924286127,\n        0.0037085365038365126,\n        -0.004046906717121601,\n        -0.004544310737401247,\n        -0.01561916433274746,\n        -0.02635226398706436,\n        -0.0068655298091471195,\n        0.012289602309465408,\n        0.027651606127619743,\n        0.0038980236276984215,\n        0.0071869813837111,\n        0.01970667578279972,\n        -0.02023453265428543,\n        -0.0001546985877212137,\n        0.01977434940636158,\n        -0.011673768982291222,\n        0.020505229011178017,\n        -0.027204956859350204,\n        0.00349197955802083,\n        0.002502246992662549,\n        0.0003045331104658544,\n        -0.02671770378947258,\n        -0.0034852121025323868,\n        -0.0012155945878475904,\n        0.014387497678399086,\n        0.0325918085873127,\n        -0.01563269831240177,\n        -0.01984202302992344,\n        -0.01659367047250271,\n        -0.015673303976655006,\n        0.03356631472706795,\n        -0.002700193552300334,\n        -0.006879064254462719,\n        -0.007349398918449879,\n        0.009149528108537197,\n        0.008161487057805061,\n        -0.03140074759721756,\n        0.018772773444652557,\n        -0.015145446173846722,\n        -0.024795761331915855,\n        -0.015565025620162487,\n        -0.022548984736204147,\n        -0.0004644129949156195,\n        0.021547408774495125,\n        -0.030832285061478615,\n        -0.007552420720458031,\n        -0.023753581568598747,\n        0.028423089534044266,\n        -0.014847680926322937,\n        0.031806789338588715,\n        -0.0017781349597498775,\n        0.014387497678399086,\n        -0.022900888696312904,\n        -0.0077757453545928,\n        0.01917881891131401,\n        0.030453309416770935,\n        -0.004943587351590395,\n        -0.026068033650517464,\n        0.02317158505320549,\n        -0.02363176830112934,\n        -0.011450444348156452,\n        -0.016566600650548935,\n        0.052271414548158646,\n        0.012634740211069584,\n        0.01901639997959137,\n        -0.00705840066075325,\n        0.02709667943418026,\n        -0.008838227018713951,\n        -0.01712152734398842,\n        -0.006171870976686478,\n        -0.02089773863554001,\n        0.006601600907742977,\n        0.009630013257265091,\n        -0.0038371169939637184,\n        0.005583107005804777,\n        -0.013216736726462841,\n        0.021980522200465202,\n        -0.0026172928046435118,\n        -0.025012318044900894,\n        -0.008892366662621498,\n        0.0143739627674222,\n        -0.004930052440613508,\n        -0.003092702943831682,\n        0.02407841756939888,\n        -0.004385276697576046,\n        -0.0056338622234761715,\n        -0.00029459348297677934,\n        0.030669867992401123,\n        0.0593365840613842,\n        0.010854912921786308,\n        -0.0222647525370121,\n        0.0007363779586739838,\n        -0.019043469801545143,\n        -0.03145488724112511,\n        -0.1149917021393776,\n        -0.01218809187412262,\n        0.01570037379860878,\n        0.007660699542611837,\n        0.009183364920318127,\n        0.007877255789935589,\n        0.006821541581302881,\n        -0.008628438226878643,\n        -0.00927810836583376,\n        0.02084359899163246,\n        -0.018529146909713745,\n        -0.013710756786167622,\n        -0.0043582068756222725,\n        -0.016580134630203247,\n        0.009893941693007946,\n        -0.005782745312899351,\n        0.017717059701681137,\n        -0.0210601557046175,\n        0.017554640769958496,\n        0.02068118192255497,\n        0.0042533124797046185,\n        -0.02635226398706436,\n        0.013771663419902325,\n        0.009582641534507275,\n        -0.020478159189224243,\n        -0.014658193103969097,\n        -0.01644478738307953,\n        0.013345316983759403,\n        -0.00028486535302363336,\n        0.004957122262567282,\n        0.004009685944765806,\n        -0.022075265645980835,\n        0.008276533335447311,\n        0.0013061086647212505,\n        -0.0037592919543385506,\n        0.008398346602916718,\n        -0.03240232169628143,\n        -0.005305643193423748,\n        0.0021351154427975416,\n        -0.026812447234988213,\n        -0.0007706379401497543,\n        0.007694536354392767,\n        -0.008533693850040436,\n        0.002871070522814989,\n        0.006293684244155884,\n        -0.0019777733832597733,\n        -0.03738313168287277,\n        0.015957534313201904,\n        -0.003307567909359932,\n        -0.00632075360044837,\n        -0.022075265645980835,\n        0.0013594019692391157,\n        -0.016810227185487747,\n        -0.015023632906377316,\n        -0.008337439969182014,\n        -0.043284304440021515,\n        -0.006473020184785128,\n        -0.006171870976686478,\n        0.011538420803844929,\n        0.0028575356118381023,\n        0.0020877434872090816,\n        0.015565025620162487,\n        -0.016566600650548935,\n        0.04742595553398132,\n        0.05570925399661064,\n        -0.006604984402656555,\n        -0.003738989820703864,\n        0.0014000063529238105,\n        0.007505048997700214,\n        -0.006828309036791325,\n        -0.01923295669257641,\n        0.013169365003705025,\n        -0.0046119848266243935,\n        0.005254887975752354,\n        -0.022359497845172882,\n        -0.013846104964613914,\n        -0.018894586712121964,\n        0.0011005487758666277,\n        -0.017689989879727364,\n        0.014157405123114586,\n        -0.02325279451906681,\n        -0.01479354128241539,\n        -0.008932971395552158,\n        0.004249928519129753,\n        0.0074644447304308414,\n        -0.015903394669294357,\n        -0.03600258007645607,\n        -0.02355056069791317,\n        0.004432648420333862,\n        -0.037843313068151474,\n        -0.022684331983327866,\n        0.03700415417551994,\n        0.02000444196164608,\n        0.014292753301560879,\n        -0.019882628694176674,\n        -0.005840267986059189,\n        -0.008743483573198318,\n        -0.004723646678030491,\n        0.018678029999136925,\n        0.0034615262411534786,\n        0.007667466998100281,\n        -0.00031447273795492947,\n        -0.031969208270311356,\n        0.012627972289919853,\n        0.015944000333547592,\n        0.007322329096496105,\n        0.003617176553234458,\n        0.00860136840492487,\n        0.012079812586307526,\n        -0.01576804742217064,\n        -0.00984656997025013,\n        -0.0010506391990929842,\n        -0.010997029021382332,\n        0.0033735500182956457,\n        -0.020572902634739876,\n        -0.00564401363953948,\n        -0.006297067739069462,\n        -0.023685907945036888,\n        0.024958180263638496,\n        -0.007538886275142431,\n        -0.006310602650046349,\n        0.017568176612257957,\n        -0.0014600669965147972,\n        0.008114115335047245,\n        -0.006056825164705515,\n        0.013940848410129547,\n        0.012668577022850513,\n        -0.01057068258523941,\n        0.006456101778894663,\n        0.006838459987193346,\n        -0.02468748390674591,\n        -0.020112719386816025,\n        0.015659768134355545,\n        -0.0032669634092599154,\n        0.011118842288851738,\n        0.023888930678367615,\n        0.010550379753112793,\n        -0.01185648888349533,\n        0.004239777568727732,\n        0.026622960343956947,\n        0.020045045763254166,\n        -0.030453309416770935,\n        0.00666250754147768,\n        -0.019043469801545143,\n        -0.007498281542211771,\n        -0.03283543512225151,\n        0.016417717561125755,\n        0.00970445480197668,\n        -0.007024563383311033,\n        -0.0014592211227864027,\n        0.013819035142660141,\n        -0.0008082815911620855,\n        0.014698797836899757,\n        0.004818390589207411,\n        0.003184062661603093,\n        -0.0011783739319071174,\n        -0.02263019233942032,\n        -0.02059997245669365,\n        -0.00451047345995903,\n        -0.007403538096696138,\n        0.009860104881227016,\n        -0.036029648035764694,\n        0.02912689931690693,\n        0.02475515753030777,\n        0.008472787216305733,\n        -0.024430321529507637,\n        0.023523490875959396,\n        -0.008181788958609104,\n        -0.01136246882379055,\n        0.017229806631803513,\n        -0.010868447832763195,\n        -0.03161730244755745,\n        -0.0037220711819827557,\n        0.00768100144341588,\n        0.0018491926603019238,\n        -0.0009102156036533415,\n        0.01728394627571106,\n        0.0126821119338274,\n        -0.005214283242821693,\n        0.008872064761817455,\n        -0.020437555387616158,\n        0.028774995356798172,\n        0.00852692686021328,\n        -0.016999714076519012,\n        -0.001177527941763401,\n        0.016566600650548935,\n        0.03651690110564232,\n        0.021696291863918304,\n        -0.010983494110405445,\n        0.019963836297392845,\n        -0.007890790700912476,\n        -0.004540926776826382,\n        0.0024328811559826136,\n        -0.013027248904109001,\n        0.010760169476270676,\n        -0.019896162673830986,\n        0.019882628694176674,\n        0.026068033650517464,\n        -0.02008564956486225,\n        0.010719564743340015,\n        0.0311571191996336,\n        0.011639932170510292,\n        -0.006456101778894663,\n        0.018908122554421425,\n        -0.006733565125614405,\n        -0.0069061340764164925,\n        -0.0065508452244102955,\n        0.007254655007272959,\n        0.004544310737401247,\n        -0.027001934126019478,\n        0.0023956603836268187,\n        0.025621384382247925,\n        0.02166922204196453,\n        0.015740977600216866,\n        0.02665003016591072,\n        0.007247888017445803,\n        0.0014253840781748295,\n        0.010557147674262524,\n        -0.003854035632684827,\n        0.0007016950403340161,\n        -0.014996563084423542,\n        -0.0041551850736141205,\n        -0.003468293696641922,\n        -0.0060906619764864445,\n        0.028856202960014343,\n        -0.010949657298624516,\n        0.031292468309402466,\n        -0.004320986568927765,\n        -0.0005451988545246422,\n        -0.02165568806231022,\n        0.03131953626871109,\n        -0.0004593374324031174,\n        -0.0010684035951271653,\n        -0.0032500450033694506,\n        -0.011267724446952343,\n        -0.022670798003673553,\n        -0.002789861522614956,\n        0.0016774698160588741,\n        -0.009122458286583424,\n        0.024890504777431488,\n        0.0033008004538714886,\n        0.05971555784344673,\n        -0.0032382020726799965,\n        0.004496939014643431,\n        -0.031211258843541145,\n        0.00524812052026391,\n        0.02030220627784729,\n        0.014170940034091473,\n        -0.030507449060678482,\n        -0.022075265645980835,\n        -0.013480665162205696,\n        0.0030148776713758707,\n        0.010482706129550934,\n        0.009230736643075943,\n        -0.016065813601017,\n        0.006117731798440218,\n        0.01781180314719677,\n        9.749289165483788e-05,\n        0.017825337126851082,\n        0.001195292454212904,\n        -0.015253724530339241,\n        0.02107369154691696,\n        -0.0035596536472439766,\n        0.01644478738307953,\n        0.005119539797306061,\n        -0.008865296840667725,\n        -0.011477514170110226,\n        0.02120903879404068,\n        -0.009508199989795685,\n        0.001542121754027903,\n        -0.05278573930263519,\n        0.008973575197160244,\n        0.011382770724594593,\n        -0.011897093616425991,\n        -0.005667699500918388,\n        -0.01939537562429905,\n        -0.001181757659651339,\n        -0.012418183498084545,\n        0.0012350509641692042,\n        0.018813379108905792,\n        0.027353839948773384,\n        0.008770553395152092,\n        0.014238614588975906,\n        -0.005349631421267986,\n        -0.014103266410529613,\n        -0.021939918398857117,\n        0.0001624176511541009,\n        0.007153144106268883,\n        -0.0018627274548634887,\n        -0.03743726760149002\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"ada\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 6,\n    \"total_tokens\": 6\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-49319/49320","correlationid":"13954189-b90b-41ac-9d72-a991f7637a08","xrequestid":"13954189-b90b-41ac-9d72-a991f7637a08","modelversion":"default"}
{"specversion":"1.0","id":"b9624e3d-31d4-474b-a93a-8f329b3c56b3","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:49Z","data":[{"name":"vector_lookup","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0x79a9517453e460b6","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x40962014af720857","start_time":"2024-02-05T15:55:42.865205Z","end_time":"2024-02-05T15:55:46.584158Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"search","node_name":"vector_lookup","flow_id":"","root_run_id":"","line_run_id":"747b1ca2-59f5-412e-8cd4-5ba466662175","inputs":"{\n  \"mlindex_content\": \"embeddings:\\n  api_base: https:\/\/promptflow-ci-weu.openai.azure.com\/\\n  api_type: azure\\n  api_version: 2023-03-15-preview\\n  batch_size: '16'\\n  connection:\\n    id: \\n      \/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourceGroups\/promptflow\/providers\/Microsoft.MachineLearningServices\/workspaces\/pf-xp\/connections\/azure_open_ai_connection\\n  connection_type: workspace_connection\\n  deployment: text-embedding-ada-003\\n  dimension: 1536\\n  file_format_version: '2'\\n  kind: open_ai\\n  model: text-embedding-ada-002\\n  schema_version: '2'\\nindex:\\n  engine: langchain.vectorstores.FAISS\\n  kind: faiss\\n  method: FlatL2\\n  path: \\n    azureml:\/\/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourcegroups\/promptflow\/workspaces\/pf-xp\/datastores\/workspaceblobstore\/paths\/azureml\/f7ee0381-06fc-4566-820f-86ba174db987\/index\/\\nself:\\n  path: \\n    azureml:\/\/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourcegroups\/promptflow\/workspaces\/pf-xp\/datastores\/workspaceblobstore\/paths\/azureml\/f7ee0381-06fc-4566-820f-86ba174db987\/index\/\\n  asset_id: \\n    azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/serene-honey-h9lrkyczzk\/versions\/1\\n\",\n  \"queries\": [\n    \"How do you calculate attention?\",\n    \"how to calculate Attention?\"\n  ],\n  \"query_type\": \"Vector\",\n  \"top_k\": 3\n}","output":"[\n  [\n    {\n      \"text\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf3\",\n        \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n        \"mtime\": null,\n        \"page_number\": 3,\n        \"stats\": {\n          \"tiktokens\": 551,\n          \"chars\": 2502,\n          \"lines\": 35\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.367725670337677\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf12\",\n        \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n        \"mtime\": null,\n        \"page_number\": 12,\n        \"stats\": {\n          \"tiktokens\": 254,\n          \"chars\": 833,\n          \"lines\": 73\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.4060562551021576\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf0\",\n        \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n        \"mtime\": null,\n        \"page_number\": 0,\n        \"stats\": {\n          \"tiktokens\": 668,\n          \"chars\": 2874,\n          \"lines\": 50\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.41361039876937866\n    }\n  ],\n  [\n    {\n      \"text\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf3\",\n        \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n        \"mtime\": null,\n        \"page_number\": 3,\n        \"stats\": {\n          \"tiktokens\": 551,\n          \"chars\": 2502,\n          \"lines\": 35\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.36372554302215576\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf12\",\n        \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n        \"mtime\": null,\n        \"page_number\": 12,\n        \"stats\": {\n          \"tiktokens\": 254,\n          \"chars\": 833,\n          \"lines\": 73\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.4135102331638336\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf0\",\n        \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n        \"mtime\": null,\n        \"page_number\": 0,\n        \"stats\": {\n          \"tiktokens\": 668,\n          \"chars\": 2874,\n          \"lines\": 50\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.4230053424835205\n    }\n  ]\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-19740/19741","correlationid":"3d3ae87b-0baa-4566-a4e8-6e07922579c4","xrequestid":"3d3ae87b-0baa-4566-a4e8-6e07922579c4","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"95f444c6-0cee-4d85-a51a-610cf4e9c94d","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:49Z","data":[{"name":"answer_the_question_with_context","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0xf1e251fb75ae4130","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x40962014af720857","start_time":"2024-02-05T15:55:46.682712Z","end_time":"2024-02-05T15:55:48.592951Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"AzureOpenAI.chat","node_name":"answer_the_question_with_context","flow_id":"","root_run_id":"","line_run_id":"747b1ca2-59f5-412e-8cd4-5ba466662175","inputs":"{\n  \"prompt\": \"{{prompt_text}}\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"prompt_text\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\nuser: how to calculate Attention? \\nassistant:\",\n  \"stream\": false\n}","output":"\"To calculate attention, we use a compatibility function of the query with the corresponding key, and then compute the dot products of the query with all keys, divide each by the square root of the dimension of the keys, and apply a softmax function to obtain the weights on the values. This is known as Scaled Dot-Product Attention. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf)\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-9117/9118","correlationid":"25d91133-2df4-4ea0-b061-c54dd1e06e9f","xrequestid":"25d91133-2df4-4ea0-b061-c54dd1e06e9f","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"0107f760-00f1-4b71-a5b2-29ec3f8fb3a6","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:49Z","data":[{"name":"search","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0x9b7433077fc39708","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x79a9517453e460b6","start_time":"2024-02-05T15:55:45.685802Z","end_time":"2024-02-05T15:55:46.554733Z","status":{"status_code":"UNSET"},"attributes":{"span_type":"Retrieval","retrieval.query":"How do you calculate attention?","retrieval.documents":"[\n    {\n        \"document.content\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n        \"document.score\": 0.367725670337677,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf3\",\n            \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n            \"mtime\": null,\n            \"page_number\": 3,\n            \"stats\": {\n                \"tiktokens\": 551,\n                \"chars\": 2502,\n                \"lines\": 35\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"document.score\": 0.4060562551021576,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf12\",\n            \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n            \"mtime\": null,\n            \"page_number\": 12,\n            \"stats\": {\n                \"tiktokens\": 254,\n                \"chars\": 833,\n                \"lines\": 73\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n        \"document.score\": 0.41361039876937866,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf0\",\n            \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n            \"mtime\": null,\n            \"page_number\": 0,\n            \"stats\": {\n                \"tiktokens\": 668,\n                \"chars\": 2874,\n                \"lines\": 50\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    }\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-9661/9662","correlationid":"01edda2e-c418-47cc-b903-83b584037dde","xrequestid":"01edda2e-c418-47cc-b903-83b584037dde","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"f25a184f-ae18-4c59-80df-1d75ef29e706","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:49Z","data":[{"name":"generate_prompt_context","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0x738af6353c39177f","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x40962014af720857","start_time":"2024-02-05T15:55:46.666888Z","end_time":"2024-02-05T15:55:46.673692Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"generate_prompt_context","node_name":"generate_prompt_context","flow_id":"","root_run_id":"","line_run_id":"747b1ca2-59f5-412e-8cd4-5ba466662175","inputs":"{\n  \"search_result\": [\n    [\n      {\n        \"text\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf3\",\n          \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n          \"mtime\": null,\n          \"page_number\": 3,\n          \"stats\": {\n            \"tiktokens\": 551,\n            \"chars\": 2502,\n            \"lines\": 35\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.367725670337677\n      },\n      {\n        \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf12\",\n          \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n          \"mtime\": null,\n          \"page_number\": 12,\n          \"stats\": {\n            \"tiktokens\": 254,\n            \"chars\": 833,\n            \"lines\": 73\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.4060562551021576\n      },\n      {\n        \"text\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf0\",\n          \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n          \"mtime\": null,\n          \"page_number\": 0,\n          \"stats\": {\n            \"tiktokens\": 668,\n            \"chars\": 2874,\n            \"lines\": 50\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.41361039876937866\n      }\n    ],\n    [\n      {\n        \"text\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf3\",\n          \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n          \"mtime\": null,\n          \"page_number\": 3,\n          \"stats\": {\n            \"tiktokens\": 551,\n            \"chars\": 2502,\n            \"lines\": 35\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.36372554302215576\n      },\n      {\n        \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf12\",\n          \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n          \"mtime\": null,\n          \"page_number\": 12,\n          \"stats\": {\n            \"tiktokens\": 254,\n            \"chars\": 833,\n            \"lines\": 73\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.4135102331638336\n      },\n      {\n        \"text\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf0\",\n          \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n          \"mtime\": null,\n          \"page_number\": 0,\n          \"stats\": {\n            \"tiktokens\": 668,\n            \"chars\": 2874,\n            \"lines\": 50\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.4230053424835205\n      }\n    ]\n  ]\n}","output":"\"Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-25808/25809","correlationid":"c7537bbc-c3b6-4fd6-832c-fac6dc9b2d28","xrequestid":"c7537bbc-c3b6-4fd6-832c-fac6dc9b2d28","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame"}
{"specversion":"1.0","id":"7321c4e0-c272-4be2-ace7-f97b0aa85ea5","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:49Z","data":[{"name":"promptflow.flow","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0x40962014af720857","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":null,"start_time":"2024-02-05T15:55:42.179765Z","end_time":"2024-02-05T15:55:48.596399Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Flow","inputs":"{\n  \"question\": \"how to calculate Attention?\",\n  \"chat_history\": []\n}","output":"{\n  \"output\": \"To calculate attention, we use a compatibility function of the query with the corresponding key, and then compute the dot products of the query with all keys, divide each by the square root of the dimension of the keys, and apply a softmax function to obtain the weights on the values. This is known as Scaled Dot-Product Attention. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf)\"\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","contentrange":"bytes 0-1069/1070","correlationid":"3d2be86a-949f-4157-838f-c40f2f2b0343","xrequestid":"3d2be86a-949f-4157-838f-c40f2f2b0343"}
{"specversion":"1.0","id":"3b1a78ad-523c-4b00-ac79-3513e3071dbd","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:49Z","data":[{"name":"openai.resources.embeddings.Embeddings.create","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0x1592958a3e244518","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x8291424e1e95e3eb","start_time":"2024-02-05T15:55:45.968341Z","end_time":"2024-02-05T15:55:46.575390Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.embeddings.Embeddings.create","node_name":"vector_lookup","flow_id":"","root_run_id":"","line_run_id":"747b1ca2-59f5-412e-8cd4-5ba466662175","inputs":"{\n  \"input\": [\n    [\n      5269,\n      311,\n      11294,\n      63120,\n      30\n    ]\n  ],\n  \"model\": \"text-embedding-ada-002\"\n}","output":"{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.017377326264977455,\n        0.022515706717967987,\n        0.038735486567020416,\n        -0.031620804220438004,\n        0.013290619477629662,\n        0.011088456027209759,\n        -0.030547956004738808,\n        -0.03201606497168541,\n        -0.014398759230971336,\n        -0.01929716020822525,\n        0.036250993609428406,\n        0.03712621331214905,\n        -0.023094480857253075,\n        0.01828077808022499,\n        0.011886034160852432,\n        -0.00854044035077095,\n        0.022642754018306732,\n        0.019268928095698357,\n        0.0041643469594419,\n        -0.02919277921319008,\n        -0.03032209351658821,\n        0.018930133432149887,\n        -0.004185521509498358,\n        0.009330960921943188,\n        -0.009535648860037327,\n        -0.005360714625567198,\n        0.020440591499209404,\n        -0.021047597751021385,\n        -0.01067202165722847,\n        -0.03834022581577301,\n        0.030717352405190468,\n        -0.006567669101059437,\n        0.008985107764601707,\n        -0.03399236500263214,\n        -0.013777635991573334,\n        0.011293144896626472,\n        0.004443146288394928,\n        -0.010792011395096779,\n        0.03478288650512695,\n        -0.0020715862046927214,\n        0.02279803529381752,\n        0.013241211883723736,\n        -0.001865133410319686,\n        0.010516740381717682,\n        0.004616072867065668,\n        0.010721429251134396,\n        -0.009443892166018486,\n        -0.042151663452386856,\n        -0.012909475713968277,\n        0.018746620044112206,\n        0.0011796042090281844,\n        0.02919277921319008,\n        -0.028896333649754524,\n        -0.01562688872218132,\n        0.007502882741391659,\n        0.004429030232131481,\n        0.01990416646003723,\n        0.00505721103399992,\n        0.005646572448313236,\n        -0.005830085836350918,\n        0.008808652870357037,\n        -0.017109114676713943,\n        -0.010947291739284992,\n        0.017094997689127922,\n        -0.009112155996263027,\n        -0.003910251427441835,\n        0.01716557890176773,\n        0.0035714569967240095,\n        -0.006542965769767761,\n        0.006528849247843027,\n        0.02330622635781765,\n        0.02723059430718422,\n        0.010304993949830532,\n        -0.020257078111171722,\n        0.04508787766098976,\n        -0.006433563306927681,\n        -0.01499164942651987,\n        -0.019932400435209274,\n        0.023786185309290886,\n        0.017010299488902092,\n        0.022261610254645348,\n        -0.002971508540213108,\n        0.003172667697072029,\n        0.02987036667764187,\n        -0.010954350233078003,\n        -0.01499164942651987,\n        0.004121997859328985,\n        0.02492961660027504,\n        -0.009846210479736328,\n        0.007721687667071819,\n        0.000765375210903585,\n        -0.03972363471984863,\n        0.022007515653967857,\n        0.01212601363658905,\n        0.011342551559209824,\n        0.018083147704601288,\n        -0.013798810541629791,\n        0.02689179964363575,\n        -0.01853487268090248,\n        -0.011547240428626537,\n        0.01024147029966116,\n        0.002911513904109597,\n        -0.04020359367132187,\n        -0.008942758664488792,\n        0.009761511348187923,\n        -0.014109372161328793,\n        -0.002883280860260129,\n        -0.007876968011260033,\n        -0.024463774636387825,\n        0.0029026910196989775,\n        -0.006955871358513832,\n        0.029926832765340805,\n        0.0031126728281378746,\n        -0.01528809405863285,\n        -0.01895836554467678,\n        0.00729819480329752,\n        0.0010340285953134298,\n        -0.012217770330607891,\n        -0.031564339995384216,\n        -0.027978764846920967,\n        0.022092213854193687,\n        0.003924367483705282,\n        0.013699996285140514,\n        -0.006486499682068825,\n        0.0009016870171763003,\n        0.0438738651573658,\n        0.015302211046218872,\n        -0.014906950294971466,\n        -0.029813900589942932,\n        -0.021880466490983963,\n        0.012006023898720741,\n        0.032834816724061966,\n        0.018732503056526184,\n        0.01288830116391182,\n        -0.021640488877892494,\n        0.022402774542570114,\n        0.02093466743826866,\n        0.003017387120053172,\n        -0.041191745549440384,\n        -0.0014769315021112561,\n        0.00021042303706053644,\n        0.019777119159698486,\n        0.003945542499423027,\n        -0.02494373358786106,\n        -0.0044607920572161674,\n        0.0447208508849144,\n        0.023842651396989822,\n        0.03986480087041855,\n        -0.0131847457960248,\n        -0.005219550337642431,\n        0.004556077998131514,\n        -0.00823693722486496,\n        0.00542423827573657,\n        0.012570681050419807,\n        0.00032820701017044485,\n        0.007188792340457439,\n        -0.022346310317516327,\n        0.012436575256288052,\n        -0.0011372549924999475,\n        -0.014179954305291176,\n        0.0016489755362272263,\n        0.01520339585840702,\n        0.006091239862143993,\n        -0.023842651396989822,\n        0.02313682995736599,\n        0.034528788179159164,\n        0.003687917487695813,\n        -0.0037090920377522707,\n        -0.00659943139180541,\n        -0.008702779188752174,\n        -0.015852751210331917,\n        0.03255248814821243,\n        -0.0202429611235857,\n        0.008582789450883865,\n        -0.011716637760400772,\n        0.012429516762495041,\n        0.041022345423698425,\n        -0.003769086906686425,\n        -0.0048242900520563126,\n        -0.022487474605441093,\n        -0.009401543065905571,\n        0.013474132865667343,\n        0.0046231308951973915,\n        0.012238944880664349,\n        -0.019494790583848953,\n        -0.0009334489586763084,\n        0.0044749085791409016,\n        -0.009796802885830402,\n        -0.013770578429102898,\n        -0.03131024166941643,\n        0.013424725271761417,\n        0.014328177087008953,\n        -0.018421942368149757,\n        -0.02178165316581726,\n        -0.6247367262840271,\n        -0.0111802127212286,\n        0.001736320904456079,\n        -0.018986599519848824,\n        0.004591369070112705,\n        0.005311307031661272,\n        0.004820760805159807,\n        -0.010516740381717682,\n        -0.020271195098757744,\n        0.03788850083947182,\n        0.009676813147962093,\n        0.001716028549708426,\n        -0.005286603234708309,\n        -0.0007706689066253603,\n        0.006271224468946457,\n        -0.02347562462091446,\n        -0.003035032656043768,\n        -0.028501072898507118,\n        0.003146199509501457,\n        0.029221011325716972,\n        -0.04923810809850693,\n        0.016290361061692238,\n        -0.014059964567422867,\n        0.020454708486795425,\n        0.0035573404747992754,\n        -0.012994173914194107,\n        -0.017956100404262543,\n        -0.022572172805666924,\n        -0.0006833234801888466,\n        0.017899634316563606,\n        0.0028109343256801367,\n        -0.004083177540451288,\n        0.026538889855146408,\n        0.024915499612689018,\n        0.050254493951797485,\n        -0.03712621331214905,\n        -0.014963416382670403,\n        0.015838634222745895,\n        -0.030180929228663445,\n        0.04534197598695755,\n        -0.008561614900827408,\n        -0.02245924063026905,\n        0.022501589730381966,\n        -0.011476658284664154,\n        -0.026087163016200066,\n        -0.020172379910945892,\n        0.008039306849241257,\n        -0.015669237822294235,\n        0.0016207427252084017,\n        -0.03306068107485771,\n        -0.013580006547272205,\n        -0.0010807892540469766,\n        -0.0032503081019967794,\n        0.0012113661505281925,\n        0.004866639617830515,\n        0.0001760142476996407,\n        0.012224828824400902,\n        0.014906950294971466,\n        0.013728228397667408,\n        -0.010361460037529469,\n        -0.0160362645983696,\n        0.014081139117479324,\n        -0.0025727194733917713,\n        -0.0111802127212286,\n        -0.009987374767661095,\n        -0.00012175421579740942,\n        0.0019163053948432207,\n        0.045031413435935974,\n        0.014723436906933784,\n        -0.024124979972839355,\n        -0.008582789450883865,\n        0.032580722123384476,\n        -0.015273978002369404,\n        0.0333147756755352,\n        0.02612951211631298,\n        0.0011081397533416748,\n        0.02452024072408676,\n        0.00930272787809372,\n        -0.0037479123566299677,\n        0.019494790583848953,\n        -0.004129055887460709,\n        -0.007040569558739662,\n        -0.015683354809880257,\n        -0.016262127086520195,\n        -0.0016780906589701772,\n        0.007559348363429308,\n        -0.0031479639001190662,\n        -0.0012545977951958776,\n        -0.0005095149390399456,\n        -0.0028374025132507086,\n        0.01301534939557314,\n        -0.002323917346075177,\n        0.00338264973834157,\n        -0.04474908486008644,\n        0.008420450612902641,\n        0.01689736731350422,\n        -0.010185004211962223,\n        0.0016330945072695613,\n        0.034105297178030014,\n        -0.006768828257918358,\n        -0.010841418989002705,\n        -0.010566147975623608,\n        -0.007097035646438599,\n        0.003285599173977971,\n        0.04006243124604225,\n        -0.008293403312563896,\n        -0.021372275426983833,\n        0.01836547628045082,\n        0.03913074731826782,\n        -0.019678303971886635,\n        -0.011631938628852367,\n        -0.0009669754654169083,\n        0.003490287344902754,\n        0.012605972588062286,\n        0.016699736937880516,\n        -0.028049347922205925,\n        0.035855732858181,\n        0.01139901764690876,\n        0.004400797188282013,\n        0.009556823410093784,\n        0.013947033323347569,\n        -0.0008447801228612661,\n        0.029475107789039612,\n        -0.03729560971260071,\n        -0.011321377009153366,\n        0.018210195004940033,\n        0.01161076407879591,\n        -0.01785728521645069,\n        -0.01989005133509636,\n        -0.0076581635512411594,\n        0.004136114381253719,\n        0.00311620207503438,\n        -0.015273978002369404,\n        -0.01036851853132248,\n        0.022586289793252945,\n        0.013304735533893108,\n        -0.02305213175714016,\n        -0.02773878537118435,\n        -0.00182454870082438,\n        -0.038904882967472076,\n        -0.024209678173065186,\n        0.012415400706231594,\n        0.011631938628852367,\n        -0.02978566847741604,\n        -0.026849450543522835,\n        -0.012365993112325668,\n        -0.010749662294983864,\n        0.0162338949739933,\n        0.0008712484268471599,\n        0.0015122225740924478,\n        0.004626660142093897,\n        -0.02168283797800541,\n        -0.022939199581742287,\n        0.004040828440338373,\n        -0.012288352474570274,\n        -0.02714589610695839,\n        -0.009959141723811626,\n        -0.016685621812939644,\n        -0.018054915592074394,\n        -0.031056147068738937,\n        0.0111802127212286,\n        0.029221011325716972,\n        -0.015683354809880257,\n        -0.0021739304065704346,\n        -0.022656871005892754,\n        -0.0023856768384575844,\n        -0.011561356484889984,\n        0.02364502102136612,\n        -0.04381740093231201,\n        -0.023772068321704865,\n        0.00379731971770525,\n        -0.023094480857253075,\n        0.005872434936463833,\n        0.014356410130858421,\n        -0.009359193034470081,\n        0.012358934618532658,\n        -0.015161046758294106,\n        -0.01911364682018757,\n        0.009874443523585796,\n        -0.019706537947058678,\n        0.01870427094399929,\n        0.017363209277391434,\n        -0.02628479339182377,\n        -0.0026856509502977133,\n        0.021880466490983963,\n        0.025762485340237617,\n        0.016177428886294365,\n        0.005554815288633108,\n        -0.014949300326406956,\n        0.006500616203993559,\n        -0.007397009525448084,\n        0.012711845338344574,\n        -0.01715146377682686,\n        0.006581785622984171,\n        0.005491291638463736,\n        0.017814936116337776,\n        0.01588098518550396,\n        0.003631451865658164,\n        0.0021827530581504107,\n        0.026609471067786217,\n        0.048108793795108795,\n        0.015132813714444637,\n        0.030547956004738808,\n        0.003536165924742818,\n        0.017094997689127922,\n        0.018421942368149757,\n        0.007742862217128277,\n        -0.01564100570976734,\n        0.01226012036204338,\n        0.0045137288980185986,\n        0.010022665373980999,\n        -0.01973477005958557,\n        -0.018548989668488503,\n        -0.009867385029792786,\n        0.007375834975391626,\n        0.01706676371395588,\n        0.008194588124752045,\n        0.010149713605642319,\n        -0.0027791722677648067,\n        0.015895100310444832,\n        0.028049347922205925,\n        -0.00897099170833826,\n        0.017546722665429115,\n        -0.0021739304065704346,\n        0.02511312998831272,\n        -0.006927638314664364,\n        0.0229815486818552,\n        0.03980833292007446,\n        0.011780161410570145,\n        -0.013594122603535652,\n        -0.023277994245290756,\n        -0.008392217569053173,\n        0.013467074371874332,\n        0.0011998965637758374,\n        0.0291080791503191,\n        -0.012323644012212753,\n        0.0032273689284920692,\n        0.0059041972272098064,\n        0.01725027896463871,\n        0.0023944994900375605,\n        -0.0047819409519433975,\n        0.02039824239909649,\n        -0.000605241977609694,\n        -0.019579488784074783,\n        0.008653371594846249,\n        0.009782686829566956,\n        0.04649952054023743,\n        0.02442142553627491,\n        0.030463257804512978,\n        0.006532378029078245,\n        -0.01497753243893385,\n        -0.0028726935852319,\n        -0.011935441754758358,\n        0.016869135200977325,\n        -0.012203654274344444,\n        0.008469858206808567,\n        -0.001971006626263261,\n        0.008681604638695717,\n        -0.0032344271894544363,\n        0.04618896171450615,\n        0.014864601194858551,\n        0.022656871005892754,\n        0.022741569206118584,\n        -0.013572948053479195,\n        -0.00702292425557971,\n        0.00512073514983058,\n        -0.006662955041974783,\n        0.002666240790858865,\n        0.00021395215298980474,\n        -0.0043019820004701614,\n        0.006348864641040564,\n        -0.01698206551373005,\n        0.00256036757491529,\n        -0.005039565730839968,\n        0.01648799143731594,\n        0.004002008121460676,\n        -0.021485207602381706,\n        -0.00613006018102169,\n        0.01809726469218731,\n        0.011109630577266216,\n        -0.019748887047171593,\n        -0.033004213124513626,\n        0.011215504258871078,\n        0.00037805564352311194,\n        -0.0017177931731566787,\n        -0.025070780888199806,\n        0.021160529926419258,\n        -0.001196367433294654,\n        -0.005127793177962303,\n        -0.0020398241467773914,\n        -0.0021862820722162724,\n        0.00308444001711905,\n        0.011886034160852432,\n        -0.021824002265930176,\n        -0.0150763476267457,\n        -0.01182956900447607,\n        0.011631938628852367,\n        0.01571158692240715,\n        0.009203912690281868,\n        -0.04071178659796715,\n        0.0007433182909153402,\n        -0.003927896730601788,\n        -0.03438762575387955,\n        -0.009161563590168953,\n        -0.013424725271761417,\n        -0.004397267941385508,\n        -0.007495824713259935,\n        -0.027851717546582222,\n        -0.0107567198574543,\n        -0.015005765482783318,\n        -0.0002408615982858464,\n        -0.00713232671841979,\n        -0.001181368832476437,\n        0.00025497801834717393,\n        0.020186495035886765,\n        -0.0018210195703431964,\n        -0.014356410130858421,\n        0.003456760896369815,\n        0.025226062163710594,\n        0.027795251458883286,\n        0.0016030971892178059,\n        -0.02902338095009327,\n        0.004143172409385443,\n        -0.00015351618640124798,\n        0.05155320465564728,\n        0.001986887538805604,\n        -0.017391443252563477,\n        0.02065233699977398,\n        -0.018746620044112206,\n        0.021725187078118324,\n        -0.016953833401203156,\n        -0.01921246200799942,\n        0.030971448868513107,\n        -0.01289535965770483,\n        -0.026934148743748665,\n        -0.027357641607522964,\n        0.006511203479021788,\n        0.0016525046667084098,\n        0.03438762575387955,\n        0.006885289214551449,\n        -0.0016322123119607568,\n        -0.013692937791347504,\n        0.01140607614070177,\n        0.005830085836350918,\n        0.0041537596844136715,\n        -0.00479958625510335,\n        0.01319180428981781,\n        0.015542189590632915,\n        0.013636471703648567,\n        -0.002689179964363575,\n        0.004139643162488937,\n        0.01809726469218731,\n        0.013227095827460289,\n        0.0014822252560406923,\n        0.013756461441516876,\n        0.022134562954306602,\n        -0.007559348363429308,\n        0.00569950882345438,\n        -0.021301694214344025,\n        0.026524772867560387,\n        0.011843685060739517,\n        0.0024315551854670048,\n        0.027611738070845604,\n        -0.022049864754080772,\n        0.006631193216890097,\n        0.029249243438243866,\n        -0.0076722800731658936,\n        -0.010234411805868149,\n        -0.004069061018526554,\n        -0.015782169997692108,\n        0.00738289300352335,\n        0.00542423827573657,\n        0.00844868365675211,\n        0.017094997689127922,\n        -0.0011301967315375805,\n        -0.007827560417354107,\n        -0.014088197611272335,\n        0.002641536993905902,\n        6.016025508870371e-05,\n        0.0072699617594480515,\n        -0.033258311450481415,\n        -0.01630447804927826,\n        0.003132082987576723,\n        -0.0301526952534914,\n        -0.028444606810808182,\n        -0.004658421967178583,\n        0.033597104251384735,\n        -0.0015501605812460184,\n        0.009189796634018421,\n        -0.011159038171172142,\n        -0.023179179057478905,\n        -0.026058930903673172,\n        -0.011730753816664219,\n        -0.009585056453943253,\n        0.0015360440593212843,\n        -0.011638997122645378,\n        -0.058442022651433945,\n        0.0011054929345846176,\n        0.025381341576576233,\n        0.012429516762495041,\n        0.002512724604457617,\n        0.009316843934357166,\n        -0.01922657899558544,\n        0.007651105523109436,\n        -0.018252544105052948,\n        -0.030265627428889275,\n        -0.014031731523573399,\n        -0.004129055887460709,\n        -0.003934955224394798,\n        0.02476021833717823,\n        0.013128280639648438,\n        0.00613006018102169,\n        0.00981091894209385,\n        0.00844868365675211,\n        -0.0175043735653162,\n        0.019791236147284508,\n        0.050677984952926636,\n        -0.044862017035484314,\n        0.024731986224651337,\n        0.006250049453228712,\n        -0.0005809793365187943,\n        0.009281553328037262,\n        0.015175162814557552,\n        -0.011808394454419613,\n        -0.01928304322063923,\n        -0.03280658274888992,\n        -0.0014125253073871136,\n        -0.00018362388073001057,\n        0.0003968040400650352,\n        0.000392833782825619,\n        0.0010710841743275523,\n        0.02178165316581726,\n        0.005417180247604847,\n        -0.030943214893341064,\n        -0.0116037055850029,\n        -0.016883250325918198,\n        -0.008053423836827278,\n        -0.0006356805097311735,\n        0.026002464815974236,\n        -0.009345076978206635,\n        0.023038014769554138,\n        0.01383410207927227,\n        -0.011130805127322674,\n        0.020525289699435234,\n        0.003066794481128454,\n        -0.02790818363428116,\n        0.03721091151237488,\n        0.028741052374243736,\n        -0.004841935820877552,\n        -0.003255601739510894,\n        -0.009740336798131466,\n        -0.0069629293866455555,\n        -0.003698504762724042,\n        0.005187788046896458,\n        0.00480664474889636,\n        0.005216021090745926,\n        0.0005518642137758434,\n        -0.04669715091586113,\n        -0.01998886652290821,\n        -0.0219369325786829,\n        -0.044862017035484314,\n        0.005028978455811739,\n        -0.007679338101297617,\n        0.008858060464262962,\n        -0.007213496137410402,\n        -0.017560839653015137,\n        -0.02132992632687092,\n        -0.015993915498256683,\n        0.04796763136982918,\n        -0.0215134397149086,\n        -0.021386392414569855,\n        0.021315809339284897,\n        -0.014963416382670403,\n        0.011300202459096909,\n        -0.01541514229029417,\n        0.0016966185066848993,\n        -0.05711507797241211,\n        -0.017179695889353752,\n        0.00782050285488367,\n        -0.028825750574469566,\n        -0.002980331424623728,\n        0.007467591669410467,\n        0.03766263648867607,\n        0.0068817599676549435,\n        0.02553662285208702,\n        0.01784316822886467,\n        -0.0064935581758618355,\n        0.013304735533893108,\n        -0.012965941801667213,\n        -0.0004296688421163708,\n        0.007481708191335201,\n        -0.028119929134845734,\n        -0.0022621580865234137,\n        0.002833873499184847,\n        0.007040569558739662,\n        0.00025299290427938104,\n        0.010502624325454235,\n        0.008985107764601707,\n        0.026651820167899132,\n        0.024957848712801933,\n        -0.009288610890507698,\n        -0.014723436906933784,\n        -0.015852751210331917,\n        -0.02220514602959156,\n        0.017645537853240967,\n        -0.0025374284014105797,\n        -0.020680570974946022,\n        -0.0015651591820642352,\n        -0.0170808807015419,\n        0.008145180530846119,\n        0.017363209277391434,\n        0.027258828282356262,\n        0.015485724434256554,\n        0.016699736937880516,\n        0.013177688233554363,\n        -0.021908700466156006,\n        0.0010110893053933978,\n        0.005336010828614235,\n        0.047431208193302155,\n        -0.0011504890862852335,\n        -0.0196218378841877,\n        0.0014354644808918238,\n        -0.00032136935624293983,\n        0.011638997122645378,\n        0.014328177087008953,\n        0.0009793273638933897,\n        0.0027244710363447666,\n        0.012845952063798904,\n        -0.01758907176554203,\n        0.006370039191097021,\n        -0.012987116351723671,\n        -0.016332710161805153,\n        -0.000704498088452965,\n        0.00969092920422554,\n        -0.015824519097805023,\n        -0.003892605658620596,\n        -0.0010057956678792834,\n        0.004256103653460741,\n        0.011674287728965282,\n        0.010643788613379002,\n        -0.005293661262840033,\n        0.007354660425335169,\n        -0.0196218378841877,\n        -0.01828077808022499,\n        0.031705502420663834,\n        -0.016163313761353493,\n        0.029588038101792336,\n        -0.013325910083949566,\n        0.015528073534369469,\n        -0.007615814451128244,\n        0.009712104685604572,\n        -0.007284078281372786,\n        0.01480813603848219,\n        0.00012418047117535025,\n        0.009542707353830338,\n        0.014201128855347633,\n        0.008865118026733398,\n        -0.02629891037940979,\n        -0.010749662294983864,\n        0.0006630311254411936,\n        0.012351877056062222,\n        -0.010686137713491917,\n        -0.036843884736299515,\n        0.01541514229029417,\n        0.013057698495686054,\n        0.0006378862308338284,\n        -0.02878340147435665,\n        -0.019268928095698357,\n        -0.020115913823246956,\n        0.045116111636161804,\n        0.005819498561322689,\n        -0.002373324939981103,\n        0.009281553328037262,\n        -0.014638738706707954,\n        -0.029305709525942802,\n        0.016262127086520195,\n        -0.007474650163203478,\n        0.02562132105231285,\n        0.025141362100839615,\n        0.0029662149026989937,\n        -0.0002285097143612802,\n        0.01161076407879591,\n        -0.007636989001184702,\n        0.009239204227924347,\n        0.006715891882777214,\n        -0.006080652587115765,\n        -0.03204429894685745,\n        0.019254811108112335,\n        0.0022321606520563364,\n        -0.020638221874833107,\n        0.0030562072061002254,\n        0.005265428684651852,\n        -0.008949817158281803,\n        0.027851717546582222,\n        -0.03740854188799858,\n        -0.026651820167899132,\n        0.002630949718877673,\n        -0.011441366747021675,\n        0.014864601194858551,\n        0.00501839118078351,\n        -0.015570422634482384,\n        0.0017821993678808212,\n        -0.004788998980075121,\n        -0.029221011325716972,\n        0.0227839183062315,\n        -7.857778996367415e-07,\n        -0.010072072967886925,\n        0.010735545307397842,\n        0.007615814451128244,\n        0.028402257710695267,\n        -0.022049864754080772,\n        0.013968207873404026,\n        -0.0019551257137209177,\n        0.001117844833061099,\n        -0.009570939466357231,\n        0.009027457796037197,\n        0.0002395381743554026,\n        0.017772585153579712,\n        -0.0007247905014082789,\n        -0.005830085836350918,\n        -0.0004755472473334521,\n        0.012231887318193913,\n        -0.020214729011058807,\n        0.005452471319586039,\n        0.016417408362030983,\n        -0.005205433815717697,\n        -0.013580006547272205,\n        -0.0029362174682319164,\n        -0.016925599426031113,\n        -0.04988746717572212,\n        0.02202163077890873,\n        -0.015782169997692108,\n        -0.018464291468262672,\n        -0.012443633750081062,\n        0.015782169997692108,\n        0.00930272787809372,\n        0.013290619477629662,\n        -0.0007256727549247444,\n        -0.02398381568491459,\n        0.021315809339284897,\n        0.00613006018102169,\n        -0.007989899255335331,\n        0.010192062705755234,\n        -0.02135816030204296,\n        0.009359193034470081,\n        -0.00909803993999958,\n        0.03382296860218048,\n        -0.004189050756394863,\n        0.028726935386657715,\n        -0.019932400435209274,\n        0.014003499411046505,\n        0.014307002536952496,\n        -0.026171863079071045,\n        -0.015570422634482384,\n        -0.030180929228663445,\n        0.028656354174017906,\n        -0.026242444291710854,\n        -0.01067202165722847,\n        0.019395975396037102,\n        0.007785211317241192,\n        -0.011871918104588985,\n        0.03938484191894531,\n        0.01140607614070177,\n        -0.033597104251384735,\n        -0.006906463764607906,\n        -0.004647834692150354,\n        -0.0068817599676549435,\n        0.01169546227902174,\n        -0.023108595982193947,\n        -0.00938742607831955,\n        0.0035326366778463125,\n        -0.006362981162965298,\n        -0.013445899821817875,\n        -0.030519722029566765,\n        -0.003642039140686393,\n        0.018676036968827248,\n        0.03224192559719086,\n        -0.014010556973516941,\n        -0.01268361322581768,\n        -0.02391323260962963,\n        -0.031959597021341324,\n        0.0006930285017006099,\n        -0.03235485777258873,\n        0.00039570117951370776,\n        0.005575989838689566,\n        0.004877226892858744,\n        -0.015824519097805023,\n        0.023334460332989693,\n        -0.0107567198574543,\n        -0.02672240324318409,\n        -0.019918283447623253,\n        0.012295410968363285,\n        -0.00794049259275198,\n        -0.01169546227902174,\n        0.009330960921943188,\n        -0.01426465343683958,\n        -0.02476021833717823,\n        -0.030180929228663445,\n        -0.012930650264024734,\n        -0.007728745695203543,\n        -0.026496540755033493,\n        0.013163571245968342,\n        0.005897138733416796,\n        -0.017800819128751755,\n        0.007644047029316425,\n        0.03825552761554718,\n        0.027470573782920837,\n        -0.0034620545338839293,\n        -0.0008125770254991949,\n        -0.01826666109263897,\n        0.006832352373749018,\n        0.0011496067745611072,\n        -0.009359193034470081,\n        -0.01741967536509037,\n        0.01438464317470789,\n        0.02809169702231884,\n        -0.029503339901566505,\n        0.00153075042180717,\n        -0.033343009650707245,\n        -0.011250794865190983,\n        0.00580185279250145,\n        -0.006250049453228712,\n        0.023546205833554268,\n        0.021386392414569855,\n        -0.019410092383623123,\n        0.026002464815974236,\n        0.017786702141165733,\n        -0.019480673596262932,\n        -0.03006799705326557,\n        0.0150763476267457,\n        -0.00866043008863926,\n        -0.024830801412463188,\n        0.018294893205165863,\n        -0.03498051315546036,\n        -0.007502882741391659,\n        0.01895836554467678,\n        -0.0015669238055124879,\n        -0.0005289249820634723,\n        0.009366251528263092,\n        0.003843198297545314,\n        -0.007065273355692625,\n        -0.02902338095009327,\n        0.011928384192287922,\n        0.02440730854868889,\n        -0.023165062069892883,\n        0.001280183787457645,\n        -0.019410092383623123,\n        -0.013375317677855492,\n        -0.003100321162492037,\n        -0.0031126728281378746,\n        0.012478924356400967,\n        -0.03128201141953468,\n        -0.013565889559686184,\n        0.014356410130858421,\n        -0.01148371584713459,\n        0.006468854378908873,\n        0.0007322898600250483,\n        0.016290361061692238,\n        0.002720942022278905,\n        0.007644047029316425,\n        0.2311706691980362,\n        -0.013805869035422802,\n        -0.020539406687021255,\n        0.023362692445516586,\n        -0.010128539055585861,\n        0.015782169997692108,\n        0.025635438039898872,\n        -0.0038537855725735426,\n        0.014123489148914814,\n        0.008251053281128407,\n        0.008794535882771015,\n        -0.0227839183062315,\n        -0.03430292755365372,\n        0.0001040535353240557,\n        -0.004873697645962238,\n        -0.015782169997692108,\n        -0.03907427936792374,\n        -0.02399793267250061,\n        -0.015132813714444637,\n        -0.026228327304124832,\n        0.02511312998831272,\n        0.005420709494501352,\n        -0.02092055045068264,\n        -0.024364959448575974,\n        0.030293859541416168,\n        -0.0075522903352975845,\n        0.004023182671517134,\n        -0.021231111139059067,\n        0.01674208603799343,\n        0.018478408455848694,\n        -0.023094480857253075,\n        -0.0011090220650658011,\n        0.0027721140068024397,\n        0.01480813603848219,\n        -0.037352073937654495,\n        0.00794049259275198,\n        -0.010410867631435394,\n        -0.0035979251842945814,\n        0.026835333555936813,\n        0.009585056453943253,\n        0.00018571928376331925,\n        -0.0011460777604952455,\n        -0.016699736937880516,\n        0.010135597549378872,\n        -0.020609987899661064,\n        0.015838634222745895,\n        0.018478408455848694,\n        -0.042462222278118134,\n        0.007220554165542126,\n        0.025988347828388214,\n        -0.025494273751974106,\n        -0.016770320013165474,\n        0.015979798510670662,\n        0.023687370121479034,\n        -0.021894583478569984,\n        0.013297677971422672,\n        0.0036561554297804832,\n        -0.0027350583113729954,\n        0.013862335123121738,\n        0.060418322682380676,\n        -0.016248011961579323,\n        0.013290619477629662,\n        -0.025804834440350533,\n        0.02639772556722164,\n        0.003719679545611143,\n        -0.007061744574457407,\n        -0.013474132865667343,\n        0.014850485138595104,\n        0.01319886278361082,\n        -0.02604481391608715,\n        -0.00497251283377409,\n        -0.02835990861058235,\n        -0.001187544665299356,\n        -0.00930978637188673,\n        -0.042321059852838516,\n        -0.01331885252147913,\n        0.004517257679253817,\n        0.018393708392977715,\n        0.008731012232601643,\n        0.02477433532476425,\n        -0.009260378777980804,\n        -0.03099968098104,\n        0.005787736736238003,\n        0.002673299051821232,\n        -0.02347562462091446,\n        -0.02227572724223137,\n        -0.01956537365913391,\n        0.0007172911427915096,\n        -0.01288830116391182,\n        0.010312052443623543,\n        0.01929716020822525,\n        -0.018407825380563736,\n        -0.024026164785027504,\n        -0.00037562940269708633,\n        -0.007954608649015427,\n        -0.010587322525680065,\n        0.009930908679962158,\n        0.009147446602582932,\n        0.0034444089978933334,\n        0.010495565831661224,\n        -0.01268361322581768,\n        0.016022149473428726,\n        0.023941466584801674,\n        0.0028126987162977457,\n        -0.008787478320300579,\n        -0.0024739045184105635,\n        -0.009535648860037327,\n        -0.002406851388514042,\n        0.006031244993209839,\n        -0.026411840692162514,\n        -0.01596568338572979,\n        -0.0259459987282753,\n        0.00037673223414458334,\n        -0.007721687667071819,\n        -0.005106618627905846,\n        -0.005830085836350918,\n        -0.0002737264148890972,\n        0.0040055373683571815,\n        0.018294893205165863,\n        -0.026341259479522705,\n        0.005117205902934074,\n        -0.021654604002833366,\n        0.01310710608959198,\n        0.011787219904363155,\n        0.027371758595108986,\n        -0.015810402110219002,\n        -0.0282610934227705,\n        -0.01204131543636322,\n        0.0071711465716362,\n        -0.007968724705278873,\n        0.009246261790394783,\n        -0.027131779119372368,\n        0.006941754836589098,\n        -0.0015122225740924478,\n        -0.0080181322991848,\n        -0.0012369522592052817,\n        -0.003490287344902754,\n        -0.021485207602381706,\n        0.004492553882300854,\n        0.006620605941861868,\n        -0.0014839897630736232,\n        -0.02978566847741604,\n        0.017702003940939903,\n        -0.009401543065905571,\n        0.020779386162757874,\n        -0.013756461441516876,\n        0.022346310317516327,\n        0.004037299193441868,\n        -0.006857056170701981,\n        -0.01716557890176773,\n        -0.02987036667764187,\n        0.0005342186777852476,\n        0.007891084998846054,\n        -0.006271224468946457,\n        0.005452471319586039,\n        0.0075099412351846695,\n        -0.0007521410589106381,\n        -0.035827502608299255,\n        0.016417408362030983,\n        -0.010770836845040321,\n        0.001510458067059517,\n        -0.005840673111379147,\n        -0.0037408540956676006,\n        0.0012837129179388285,\n        -0.011017873883247375,\n        0.00041246446198783815,\n        -0.18238428235054016,\n        0.01749025657773018,\n        0.031790200620889664,\n        -0.017913751304149628,\n        0.005494820419698954,\n        -0.012591855600476265,\n        0.006853526923805475,\n        0.013029465451836586,\n        -0.03224192559719086,\n        0.011039048433303833,\n        0.022063981741666794,\n        -0.021160529926419258,\n        -0.026143629103899002,\n        -0.022558055818080902,\n        0.0080181322991848,\n        -0.006948812864720821,\n        -0.004577252548187971,\n        0.024350842460989952,\n        0.014123489148914814,\n        0.01613507978618145,\n        0.03551694005727768,\n        -0.0018351359758526087,\n        0.01664327085018158,\n        0.008385160006582737,\n        -0.013022406958043575,\n        0.00202747224830091,\n        -0.01733497716486454,\n        0.016671504825353622,\n        0.010079131461679935,\n        -0.023701487109065056,\n        0.000368130044080317,\n        -0.01852075755596161,\n        0.006624135188758373,\n        0.004305511247366667,\n        -0.006966458633542061,\n        -0.007319369353353977,\n        -0.019607722759246826,\n        -0.020680570974946022,\n        -0.01674208603799343,\n        0.01895836554467678,\n        0.02433672547340393,\n        0.006430034060031176,\n        0.010834360495209694,\n        0.0134953074157238,\n        -0.01298005785793066,\n        0.007284078281372786,\n        0.017617305740714073,\n        -0.0198194682598114,\n        0.020525289699435234,\n        -0.017349092289805412,\n        0.015513957478106022,\n        -0.0037761451676487923,\n        0.00760169792920351,\n        -0.025734253227710724,\n        -7.675809320062399e-05,\n        -0.00576303293928504,\n        0.00896393321454525,\n        0.03260895609855652,\n        -0.008836885914206505,\n        -0.01997474953532219,\n        -0.023503856733441353,\n        -0.018294893205165863,\n        0.04347860440611839,\n        0.0023839122150093317,\n        -0.00037629110738635063,\n        -0.009408600628376007,\n        0.002454494358971715,\n        0.015161046758294106,\n        -0.028472840785980225,\n        0.02398381568491459,\n        -0.010799068957567215,\n        -0.030773818492889404,\n        -0.016840901225805283,\n        -0.022501589730381966,\n        0.007954608649015427,\n        0.01922657899558544,\n        -0.027724670246243477,\n        -0.009408600628376007,\n        -0.01887366734445095,\n        0.02893868274986744,\n        -0.0210899468511343,\n        0.03867901861667633,\n        0.00047069473657757044,\n        0.021216996014118195,\n        -0.023292111232876778,\n        -0.010128539055585861,\n        0.01826666109263897,\n        0.024364959448575974,\n        -0.0010507918195798993,\n        -0.021584022790193558,\n        0.03800142928957939,\n        -0.0333147756755352,\n        -0.012267177924513817,\n        -0.010227354243397713,\n        0.04952043667435646,\n        0.007651105523109436,\n        0.018323127180337906,\n        -0.01216130517423153,\n        0.027287060394883156,\n        -0.004859581124037504,\n        -0.00857573188841343,\n        0.00013079754717182368,\n        -0.02176753617823124,\n        0.005505407694727182,\n        0.0037902616895735264,\n        -0.003218546276912093,\n        0.003811436239629984,\n        -0.012613031081855297,\n        0.017899634316563606,\n        0.001433699973858893,\n        -0.021922817453742027,\n        -0.007135855499655008,\n        0.013502365909516811,\n        0.0021121709141880274,\n        -0.0015669238055124879,\n        0.020299427211284637,\n        -0.010065015405416489,\n        0.0008231643587350845,\n        -0.0013357672141864896,\n        0.04074001684784889,\n        0.05045212432742119,\n        0.01331885252147913,\n        -0.018323127180337906,\n        -0.0011363726807758212,\n        -0.01725027896463871,\n        -0.03150787204504013,\n        -0.10609909147024155,\n        -0.013149455189704895,\n        0.019523022696375847,\n        0.012754195369780064,\n        0.009959141723811626,\n        0.0005650983657687902,\n        0.0067229499109089375,\n        0.00208217347972095,\n        -0.013735286891460419,\n        0.0301526952534914,\n        -0.010989640839397907,\n        -0.014666971750557423,\n        -0.0032326625660061836,\n        -0.013431783765554428,\n        0.009083922952413559,\n        0.0013939974596723914,\n        0.018732503056526184,\n        -0.023800302296876907,\n        0.020680570974946022,\n        0.02141462452709675,\n        0.00013565007247962058,\n        -0.026002464815974236,\n        0.02048294059932232,\n        0.0056006936356425285,\n        -0.025564854964613914,\n        -0.014067023061215878,\n        -0.014483457431197166,\n        0.014963416382670403,\n        0.000726996164303273,\n        0.01681266911327839,\n        0.012133072130382061,\n        -0.02065233699977398,\n        0.01289535965770483,\n        0.0012051902012899518,\n        0.005141909699887037,\n        0.011081397533416748,\n        -0.035912200808525085,\n        -0.002553309313952923,\n        -0.0010357931023463607,\n        -0.02142874151468277,\n        0.005014861933887005,\n        0.010079131461679935,\n        -0.008081656880676746,\n        0.004792528226971626,\n        0.017560839653015137,\n        -0.011173155158758163,\n        -0.03125377744436264,\n        0.014511690475046635,\n        -0.004877226892858744,\n        -0.01361529715359211,\n        -0.015443375334143639,\n        -2.220856367785018e-05,\n        -0.019395975396037102,\n        -0.017292628064751625,\n        -0.005784207489341497,\n        -0.035291075706481934,\n        -0.006588844116777182,\n        -0.010742603801190853,\n        0.012605972588062286,\n        0.005695979576557875,\n        0.010961408726871014,\n        0.011130805127322674,\n        -0.015542189590632915,\n        0.052202560007572174,\n        0.05192023143172264,\n        -0.013537656515836716,\n        -0.0022233377676457167,\n        -0.0009158034226857126,\n        0.012711845338344574,\n        -0.005004274658858776,\n        -0.018845435231924057,\n        0.012309527024626732,\n        -0.0025215474888682365,\n        0.005000745411962271,\n        -0.02118876203894615,\n        -0.010869651101529598,\n        -0.023249760270118713,\n        -0.0027650557458400726,\n        -0.020200612023472786,\n        0.015852751210331917,\n        -0.03808613121509552,\n        -0.018309010192751884,\n        -0.003917309455573559,\n        0.007947550155222416,\n        0.006737066432833672,\n        -0.014448166824877262,\n        -0.03198783099651337,\n        -0.03447232395410538,\n        0.013135338202118874,\n        -0.03712621331214905,\n        -0.022840384393930435,\n        0.0369003489613533,\n        0.021880466490983963,\n        0.01170252077281475,\n        -0.01956537365913391,\n        -0.00723467068746686,\n        -0.0032097233925014734,\n        0.001431935466825962,\n        0.013311794027686119,\n        0.0005964192096143961,\n        0.0012528331717476249,\n        -0.0027085901238024235,\n        -0.04088118299841881,\n        0.013255327939987183,\n        0.012944767251610756,\n        0.008314577862620354,\n        -0.003927896730601788,\n        0.015979798510670662,\n        0.016516223549842834,\n        -0.006948812864720821,\n        -0.022092213854193687,\n        -0.009330960921943188,\n        -0.018139613792300224,\n        -0.004940750543028116,\n        -0.020680570974946022,\n        -0.002664476167410612,\n        -0.004905459471046925,\n        -0.026157746091485023,\n        0.022741569206118584,\n        -0.006370039191097021,\n        0.0005033389898017049,\n        0.023376809433102608,\n        0.004136114381253719,\n        0.013255327939987183,\n        -0.0021492266096174717,\n        0.021640488877892494,\n        0.009458008222281933,\n        -0.01139195915311575,\n        0.006048890296369791,\n        0.007439359091222286,\n        -0.01965007185935974,\n        -0.011773102916777134,\n        0.015655120834708214,\n        0.0002375530602876097,\n        0.006754712201654911,\n        0.035291075706481934,\n        0.008886292576789856,\n        -0.019748887047171593,\n        0.0020715862046927214,\n        0.020539406687021255,\n        0.01413054671138525,\n        -0.03557340428233147,\n        0.0031620804220438004,\n        -0.014335235580801964,\n        -0.009189796634018421,\n        -0.03348417207598686,\n        0.03083028458058834,\n        0.003624393604695797,\n        -0.0010984347900375724,\n        0.0029503339901566505,\n        0.02227572724223137,\n        -0.0026027169078588486,\n        0.010707312263548374,\n        0.002930923830717802,\n        0.01140607614070177,\n        -0.0012881242437288165,\n        -0.025889534503221512,\n        -0.01438464317470789,\n        -0.003800848964601755,\n        -0.009712104685604572,\n        0.015513957478106022,\n        -0.023348575457930565,\n        0.031649038195610046,\n        0.012796544469892979,\n        0.005463058594614267,\n        -0.018464291468262672,\n        0.020864084362983704,\n        -0.0029573922511190176,\n        -0.014723436906933784,\n        0.02125934511423111,\n        -0.013869392685592175,\n        -0.030632654204964638,\n        -0.015852751210331917,\n        0.007153501268476248,\n        0.004725475329905748,\n        0.0016154489712789655,\n        0.020553523674607277,\n        0.012288352474570274,\n        -0.006652367766946554,\n        0.017349092289805412,\n        -0.014328177087008953,\n        0.028246978297829628,\n        0.01226012036204338,\n        -0.017052648589015007,\n        -0.013092989102005959,\n        0.022332193329930305,\n        0.04531374201178551,\n        0.02579071931540966,\n        -0.013085931539535522,\n        0.02015826292335987,\n        -0.013290619477629662,\n        -0.002099819015711546,\n        -0.0009096274734474719,\n        -0.011053165420889854,\n        0.00794049259275198,\n        -0.027047080919146538,\n        0.028741052374243736,\n        0.027724670246243477,\n        -0.01497753243893385,\n        -0.000180646195076406,\n        0.030604422092437744,\n        0.013417666777968407,\n        -0.005540698766708374,\n        0.021118180826306343,\n        -0.019494790583848953,\n        -0.015245744958519936,\n        -0.008886292576789856,\n        0.006708833388984203,\n        0.0021686365362256765,\n        -0.027301177382469177,\n        -0.0016233895439654589,\n        0.011808394454419613,\n        0.022501589730381966,\n        0.01782905124127865,\n        0.03145140781998634,\n        0.014236420392990112,\n        -0.00947918277233839,\n        0.007262903731316328,\n        -0.00695940013974905,\n        0.002793288789689541,\n        -0.016022149473428726,\n        -0.01139195915311575,\n        -0.0010304994648322463,\n        -0.010770836845040321,\n        0.0282610934227705,\n        -0.00422787107527256,\n        0.04107881337404251,\n        0.0009316843934357166,\n        3.367127192177577e-06,\n        -0.020864084362983704,\n        0.03492404893040657,\n        -0.0030420906841754913,\n        0.0007750802906230092,\n        -0.004566665273159742,\n        -0.010043839924037457,\n        -0.026185978204011917,\n        -0.0024597879964858294,\n        0.0033085383474826813,\n        -0.010417926125228405,\n        0.025494273751974106,\n        -0.0036702719517052174,\n        0.09186972677707672,\n        -0.004305511247366667,\n        0.005664217751473188,\n        -0.04141760617494583,\n        0.0037761451676487923,\n        0.017885517328977585,\n        0.008688663132488728,\n        -0.03390766680240631,\n        -0.018563106656074524,\n        -0.014405817724764347,\n        0.00697704590857029,\n        0.0019886521622538567,\n        0.00981091894209385,\n        -0.015005765482783318,\n        0.00405494449660182,\n        0.014695203863084316,\n        0.005533640738576651,\n        0.018294893205165863,\n        -0.0004104793188162148,\n        -0.01152606587857008,\n        0.033681802451610565,\n        -0.006377097684890032,\n        0.0229815486818552,\n        0.01203425694257021,\n        -0.006719421129673719,\n        -0.013523540459573269,\n        0.024873150512576103,\n        -0.007891084998846054,\n        -0.002165107522159815,\n        -0.05432002618908882,\n        0.0013825278729200363,\n        0.00695940013974905,\n        -0.002512724604457617,\n        -0.011455483734607697,\n        -0.016276244074106216,\n        -0.007054686080664396,\n        -0.019000714644789696,\n        0.00140899617690593,\n        0.031733736395835876,\n        0.0322701595723629,\n        0.00681117782369256,\n        0.027879949659109116,\n        -0.004291394725441933,\n        -0.01234481856226921,\n        -0.020878201350569725,\n        3.8351474358933046e-05,\n        -0.00011232488031964749,\n        -0.009338018484413624,\n        -0.03664625436067581\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"ada\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 5\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-49288/49289","correlationid":"cc51d225-664b-4bba-9967-3827af766268","xrequestid":"cc51d225-664b-4bba-9967-3827af766268","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame"}
{"specversion":"1.0","id":"4a7ce252-8fc4-41de-938e-e3325c911bbb","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:49Z","data":[{"name":"Prompt_variants","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0x71bb72f614ce5b14","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x40962014af720857","start_time":"2024-02-05T15:55:46.677872Z","end_time":"2024-02-05T15:55:46.680543Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"render_template_jinja2","node_name":"Prompt_variants","flow_id":"","root_run_id":"","line_run_id":"747b1ca2-59f5-412e-8cd4-5ba466662175","inputs":"{\n  \"template\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n {{contexts}} \\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\nuser: {{question}} \\nassistant:\",\n  \"chat_history\": [],\n  \"contexts\": \"Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n  \"question\": \"how to calculate Attention?\"\n}","output":"\"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\nuser: how to calculate Attention? \\nassistant:\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","contentrange":"bytes 0-16279/16280","correlationid":"500548ff-4f06-458a-8af9-0c9e0310976e","xrequestid":"500548ff-4f06-458a-8af9-0c9e0310976e","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"761c71a1-2812-4a58-97d1-b15e735ed337","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:49Z","data":[{"name":"search","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0x8291424e1e95e3eb","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x79a9517453e460b6","start_time":"2024-02-05T15:55:45.686252Z","end_time":"2024-02-05T15:55:46.576680Z","status":{"status_code":"UNSET"},"attributes":{"span_type":"Retrieval","retrieval.query":"how to calculate Attention?","retrieval.documents":"[\n    {\n        \"document.content\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n        \"document.score\": 0.36372554302215576,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf3\",\n            \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n            \"mtime\": null,\n            \"page_number\": 3,\n            \"stats\": {\n                \"tiktokens\": 551,\n                \"chars\": 2502,\n                \"lines\": 35\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"document.score\": 0.4135102331638336,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf12\",\n            \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n            \"mtime\": null,\n            \"page_number\": 12,\n            \"stats\": {\n                \"tiktokens\": 254,\n                \"chars\": 833,\n                \"lines\": 73\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n        \"document.score\": 0.4230053424835205,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf0\",\n            \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n            \"mtime\": null,\n            \"page_number\": 0,\n            \"stats\": {\n                \"tiktokens\": 668,\n                \"chars\": 2874,\n                \"lines\": 50\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    }\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-9658/9659","correlationid":"19c7f1fa-198a-4370-848f-7d079b4b5413","xrequestid":"19c7f1fa-198a-4370-848f-7d079b4b5413","modelversion":"default"}
{"specversion":"1.0","id":"461672aa-5937-4db2-bd07-921c176e35a3","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:55:49Z","data":[{"name":"openai.resources.chat.completions.Completions.create","context":{"request_id":"0x5b050648eab9bb1d00da1700e21352d2","trace_id":"0x5b050648eab9bb1d00da1700e21352d2","span_id":"0xe18ea4b343287f66","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xf1e251fb75ae4130","start_time":"2024-02-05T15:55:46.685331Z","end_time":"2024-02-05T15:55:48.592139Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.chat.completions.Completions.create","node_name":"answer_the_question_with_context","flow_id":"","root_run_id":"","line_run_id":"747b1ca2-59f5-412e-8cd4-5ba466662175","inputs":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\nuser: how to calculate Attention? \\nassistant:\"\n    }\n  ],\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8ovbvvrrN7PfSuwQl04rrwb1ni2Og\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"To calculate attention, we use a compatibility function of the query with the corresponding key, and then compute the dot products of the query with all keys, divide each by the square root of the dimension of the keys, and apply a softmax function to obtain the weights on the values. This is known as Scaled Dot-Product Attention. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf)\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707148547,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 119,\n    \"prompt_tokens\": 1735,\n    \"total_tokens\": 1854\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"xrequestid":"206a5dad-2108-4919-a36a-c02d899ba197","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-9911/9912","correlationid":"206a5dad-2108-4919-a36a-c02d899ba197"}
{"specversion":"1.0","id":"d43a31fa-30a9-47d9-acf3-b6d83efed44d","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"openai.resources.chat.completions.Completions.create","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0xa1c01237a2bfc843","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x6555b4a6692fe2cd","start_time":"2024-02-05T15:56:04.366469Z","end_time":"2024-02-05T15:56:05.066954Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.chat.completions.Completions.create","node_name":"modify_query_with_history","flow_id":"","root_run_id":"","line_run_id":"d13da436-d3ad-4905-9aae-2dfc0cb2b538","inputs":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Given the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history:\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"how to calculate Attention?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"To calculate attention, we use a compatibility function of the query with the corresponding key, and then compute the dot products of the query with all keys, divide each by the square root of the dimension of the keys, and apply a softmax function to obtain the weights on the values. This is known as Scaled Dot-Product Attention. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf)\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Follow up Input: how to improve Attension calculation efficiency? \\nStandalone Question:\"\n    }\n  ],\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": null,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8ovcCg3bbyX7xxWkJCTFAgpZZpDu9\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"What are some ways to improve the efficiency of Attention calculation?\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707148564,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 12,\n    \"prompt_tokens\": 216,\n    \"total_tokens\": 228\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default","contentrange":"bytes 0-2856/2857","correlationid":"1f973ead-525f-4d68-9eb0-336e86a1c875","xrequestid":"1f973ead-525f-4d68-9eb0-336e86a1c875"}
{"specversion":"1.0","id":"184ca24d-b77f-4a15-8485-f6f41d4ef1fb","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"search","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0xcc74e3c1dde67fce","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x2b2a902ea98f7623","start_time":"2024-02-05T15:56:05.081516Z","end_time":"2024-02-05T15:56:05.629362Z","status":{"status_code":"UNSET"},"attributes":{"span_type":"Retrieval","retrieval.query":"how to improve Attension calculation efficiency?","retrieval.documents":"[\n    {\n        \"document.content\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n        \"document.score\": 0.403403103351593,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf3\",\n            \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n            \"mtime\": null,\n            \"page_number\": 3,\n            \"stats\": {\n                \"tiktokens\": 551,\n                \"chars\": 2502,\n                \"lines\": 35\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"document.score\": 0.42596590518951416,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf12\",\n            \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n            \"mtime\": null,\n            \"page_number\": 12,\n            \"stats\": {\n                \"tiktokens\": 254,\n                \"chars\": 833,\n                \"lines\": 73\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n        \"document.score\": 0.4357953667640686,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf0\",\n            \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n            \"mtime\": null,\n            \"page_number\": 0,\n            \"stats\": {\n                \"tiktokens\": 668,\n                \"chars\": 2874,\n                \"lines\": 50\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    }\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-9678/9679","correlationid":"24bdfde7-2fc2-4f4c-8e78-329153c2a4f8","xrequestid":"24bdfde7-2fc2-4f4c-8e78-329153c2a4f8","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"0f0b8447-0319-4ef8-a09c-d574684b7442","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"generate_prompt_context","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0x950ff8d24b8480aa","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x4d0bbb09563f095d","start_time":"2024-02-05T15:56:05.754652Z","end_time":"2024-02-05T15:56:05.761406Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"generate_prompt_context","node_name":"generate_prompt_context","flow_id":"","root_run_id":"","line_run_id":"d13da436-d3ad-4905-9aae-2dfc0cb2b538","inputs":"{\n  \"search_result\": [\n    [\n      {\n        \"text\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf3\",\n          \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n          \"mtime\": null,\n          \"page_number\": 3,\n          \"stats\": {\n            \"tiktokens\": 551,\n            \"chars\": 2502,\n            \"lines\": 35\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.3891408443450928\n      },\n      {\n        \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf12\",\n          \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n          \"mtime\": null,\n          \"page_number\": 12,\n          \"stats\": {\n            \"tiktokens\": 254,\n            \"chars\": 833,\n            \"lines\": 73\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.39398106932640076\n      },\n      {\n        \"text\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf0\",\n          \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n          \"mtime\": null,\n          \"page_number\": 0,\n          \"stats\": {\n            \"tiktokens\": 668,\n            \"chars\": 2874,\n            \"lines\": 50\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.42212480306625366\n      }\n    ],\n    [\n      {\n        \"text\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf3\",\n          \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n          \"mtime\": null,\n          \"page_number\": 3,\n          \"stats\": {\n            \"tiktokens\": 551,\n            \"chars\": 2502,\n            \"lines\": 35\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.403403103351593\n      },\n      {\n        \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf12\",\n          \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n          \"mtime\": null,\n          \"page_number\": 12,\n          \"stats\": {\n            \"tiktokens\": 254,\n            \"chars\": 833,\n            \"lines\": 73\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.42596590518951416\n      },\n      {\n        \"text\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf0\",\n          \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n          \"mtime\": null,\n          \"page_number\": 0,\n          \"stats\": {\n            \"tiktokens\": 668,\n            \"chars\": 2874,\n            \"lines\": 50\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.4357953667640686\n      }\n    ]\n  ]\n}","output":"\"Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-25809/25810","correlationid":"3a727cd7-3ede-4cd6-a7e9-22a007feaf8e","xrequestid":"3a727cd7-3ede-4cd6-a7e9-22a007feaf8e","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default"}
{"specversion":"1.0","id":"049df3c1-0d4a-41f1-a7ff-315fd90fab81","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"promptflow.flow","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0x4d0bbb09563f095d","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":null,"start_time":"2024-02-05T15:56:04.360491Z","end_time":"2024-02-05T15:56:08.088375Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Flow","inputs":"{\n  \"question\": \"how to improve Attension calculation efficiency?\",\n  \"chat_history\": [\n    {\n      \"inputs\": {\n        \"question\": \"how to calculate Attention?\"\n      },\n      \"outputs\": {\n        \"output\": \"To calculate attention, we use a compatibility function of the query with the corresponding key, and then compute the dot products of the query with all keys, divide each by the square root of the dimension of the keys, and apply a softmax function to obtain the weights on the values. This is known as Scaled Dot-Product Attention. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf)\"\n      }\n    }\n  ]\n}","output":"{\n  \"output\": \"One way to improve the efficiency of attention calculation is to use dot-product attention, which is faster and more space-efficient than additive attention. Additionally, multi-head attention can be used to linearly project the queries, keys, and values multiple times with different, learned linear projections to different dimensions, and then perform the attention function in parallel on each of these projected versions. This can improve the model's performance while also being more parallelizable and requiring less time to train. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf)\"\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-1951/1952","correlationid":"77f91f85-d366-494c-adf2-49e6afd65cbf","xrequestid":"77f91f85-d366-494c-adf2-49e6afd65cbf","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"aaa66009-9852-433c-8e09-a6fcbdce6bf4","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"modify_query_with_history","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0x6555b4a6692fe2cd","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x4d0bbb09563f095d","start_time":"2024-02-05T15:56:04.362997Z","end_time":"2024-02-05T15:56:05.070253Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"AzureOpenAI.chat","node_name":"modify_query_with_history","flow_id":"","root_run_id":"","line_run_id":"d13da436-d3ad-4905-9aae-2dfc0cb2b538","inputs":"{\n  \"prompt\": \"system: \\nGiven the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\n\\nuser:\\nFollow up Input: {{question}} \\nStandalone Question:\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"chat_history\": [\n    {\n      \"inputs\": {\n        \"question\": \"how to calculate Attention?\"\n      },\n      \"outputs\": {\n        \"output\": \"To calculate attention, we use a compatibility function of the query with the corresponding key, and then compute the dot products of the query with all keys, divide each by the square root of the dimension of the keys, and apply a softmax function to obtain the weights on the values. This is known as Scaled Dot-Product Attention. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf)\"\n      }\n    }\n  ],\n  \"question\": \"how to improve Attension calculation efficiency?\"\n}","output":"\"What are some ways to improve the efficiency of Attention calculation?\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-2151/2152","correlationid":"06fcc328-46ce-44dc-b307-15dd2a2b4d0f","xrequestid":"06fcc328-46ce-44dc-b307-15dd2a2b4d0f","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"851f9b76-238d-49ef-a1d0-9fcc840a4b2f","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"openai.resources.embeddings.Embeddings.create","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0x5df96e44e9d6f838","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x79d5c32fe460520a","start_time":"2024-02-05T15:56:05.082313Z","end_time":"2024-02-05T15:56:05.654850Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.embeddings.Embeddings.create","node_name":"vector_lookup","flow_id":"","root_run_id":"","line_run_id":"d13da436-d3ad-4905-9aae-2dfc0cb2b538","inputs":"{\n  \"input\": [\n    [\n      3923,\n      527,\n      1063,\n      5627,\n      311,\n      7417,\n      279,\n      15374,\n      315,\n      63120,\n      22702,\n      30\n    ]\n  ],\n  \"model\": \"text-embedding-ada-002\"\n}","output":"{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.018112102523446083,\n        0.034980811178684235,\n        0.03368215635418892,\n        -0.019258787855505943,\n        0.009746826253831387,\n        0.02544812671840191,\n        -0.018802877515554428,\n        -0.02584877610206604,\n        -0.017766715958714485,\n        -0.024356702342629433,\n        0.024384334683418274,\n        0.029012521728873253,\n        0.008800464682281017,\n        0.0286809504032135,\n        0.008620863780379295,\n        -0.011598100885748863,\n        0.026677705347537994,\n        0.0123165063560009,\n        0.010389246046543121,\n        -0.011004035361111164,\n        -0.03520185872912407,\n        -0.0008729660767130554,\n        0.002260559005662799,\n        -0.014354290440678596,\n        -0.0007326525519602001,\n        -0.006634887307882309,\n        0.018374597653746605,\n        -0.01565294712781906,\n        0.0033157167490571737,\n        -0.03525712341070175,\n        0.041225410997867584,\n        -0.004483125638216734,\n        0.0006549404351972044,\n        -0.02280936762690544,\n        -0.010147474706172943,\n        0.014533892273902893,\n        0.0037336351815611124,\n        -0.012592815794050694,\n        0.030559856444597244,\n        -0.017932502552866936,\n        0.022961338981986046,\n        0.012005657888948917,\n        -0.004987390711903572,\n        0.008828096091747284,\n        0.007163329981267452,\n        0.022933708503842354,\n        0.0010931503493338823,\n        -0.01903774030506611,\n        -0.02746518701314926,\n        0.016661476343870163,\n        0.014098703861236572,\n        0.02594548463821411,\n        -0.013055634684860706,\n        -0.011446130461990833,\n        -0.008323830552399158,\n        0.001347009907476604,\n        0.018885770812630653,\n        0.019023925065994263,\n        -0.0035851188004016876,\n        -0.007253130432218313,\n        0.005353501066565514,\n        -0.0017234819242730737,\n        0.0002428503503324464,\n        0.009898795746266842,\n        -0.011100743897259235,\n        -0.003913236781954765,\n        0.004894135985523462,\n        -0.002600765321403742,\n        0.01838841289281845,\n        -0.007930089719593525,\n        0.02325146459043026,\n        0.01424376666545868,\n        -0.015639130026102066,\n        -0.009947150014340878,\n        0.03614131361246109,\n        -0.020087717100977898,\n        -0.019148264080286026,\n        -0.013663516379892826,\n        0.025807328522205353,\n        0.01595688797533512,\n        0.006990636233240366,\n        -0.00019147401326335967,\n        -0.0030135030392557383,\n        0.029344093054533005,\n        0.005353501066565514,\n        -0.000526715419255197,\n        0.02257450483739376,\n        0.011314883828163147,\n        -0.007170237600803375,\n        0.0047870660200715065,\n        0.007888643071055412,\n        -0.014450998976826668,\n        0.023127123713493347,\n        0.009947150014340878,\n        0.010803710669279099,\n        0.01677200011909008,\n        -0.007204776164144278,\n        0.019493652507662773,\n        -0.024038946256041527,\n        -0.010617201216518879,\n        0.004973575007170439,\n        0.003833797527477145,\n        -0.047525275498628616,\n        -0.01609504222869873,\n        -0.004027214366942644,\n        -0.01463059987872839,\n        -0.004914859309792519,\n        -0.005571094807237387,\n        -0.013442468829452991,\n        0.008006074465811253,\n        -0.01630227454006672,\n        0.017352251335978508,\n        0.011280344799160957,\n        -0.016053594648838043,\n        0.00034409196814522147,\n        0.0073429313488304615,\n        0.012952018529176712,\n        -0.008019889704883099,\n        -0.041225410997867584,\n        -0.026235608384013176,\n        0.02689875289797783,\n        0.016882523894309998,\n        0.02757571078836918,\n        -0.006669426336884499,\n        -0.0031516579911112785,\n        0.023942237719893456,\n        0.020916646346449852,\n        -0.004776704590767622,\n        -0.021952807903289795,\n        -0.020571259781718254,\n        0.00895934272557497,\n        0.02942698635160923,\n        0.012095458805561066,\n        0.01304181944578886,\n        -0.03108484484255314,\n        -0.0014704858185723424,\n        0.01656476780772209,\n        0.006507094018161297,\n        -0.02551720291376114,\n        -0.0031274808570742607,\n        0.010188921354711056,\n        0.02642902545630932,\n        0.0025610458105802536,\n        -0.015873994678258896,\n        -0.011736256070435047,\n        0.04713844135403633,\n        0.010513585060834885,\n        0.03188614174723625,\n        0.004168822895735502,\n        -0.0016354082617908716,\n        -0.006220422685146332,\n        -0.016426613554358482,\n        0.003578210948035121,\n        0.007377469912171364,\n        0.007370562292635441,\n        0.0015119323506951332,\n        -0.023196201771497726,\n        0.0066245258785784245,\n        -0.012323413975536823,\n        -0.0374675989151001,\n        0.01192967314273119,\n        0.006524363532662392,\n        0.011149098165333271,\n        -0.00046066014328971505,\n        0.02028113417327404,\n        0.01815355010330677,\n        0.0068352119997143745,\n        -0.005619449075311422,\n        0.005318962503224611,\n        -0.0060408213175833225,\n        -0.0046281879767775536,\n        0.01930023543536663,\n        -0.010320168919861317,\n        0.009636301547288895,\n        -0.012523738667368889,\n        -0.0018668175907805562,\n        0.032853227108716965,\n        -0.00015984950005076826,\n        -0.010651740245521069,\n        -0.02881910465657711,\n        0.006161706987768412,\n        -0.0035212221555411816,\n        0.011266529560089111,\n        0.002329636365175247,\n        -0.00990570429712534,\n        -0.010803710669279099,\n        0.01606741175055504,\n        -0.007957720197737217,\n        -0.01391910295933485,\n        -0.035754479467868805,\n        0.00754325557500124,\n        -0.006966459099203348,\n        -0.025158001109957695,\n        -0.020239688456058502,\n        -0.6512067914009094,\n        -0.018512751907110214,\n        0.011515208519995213,\n        -0.022325826808810234,\n        0.006320585031062365,\n        0.011328699067234993,\n        0.004759435076266527,\n        0.007626148406416178,\n        -0.01883050799369812,\n        0.02358303591609001,\n        0.013663516379892826,\n        0.00803370587527752,\n        -0.008164952509105206,\n        -0.01418850477784872,\n        0.004255170002579689,\n        -0.030836166813969612,\n        0.0018581829499453306,\n        -0.021303480491042137,\n        -0.006821396294981241,\n        0.0030359532684087753,\n        -0.03130589425563812,\n        0.0015257478225976229,\n        -0.01319378986954689,\n        0.0070009976625442505,\n        0.010658647865056992,\n        -0.02128966525197029,\n        0.0033329862635582685,\n        -0.019327865913510323,\n        -0.010340891778469086,\n        0.027313217520713806,\n        0.008413631469011307,\n        0.0006683241808786988,\n        0.019286420196294785,\n        0.027078352868556976,\n        0.047027915716171265,\n        -0.019258787855505943,\n        -0.018941031768918037,\n        0.027630973607301712,\n        -0.035754479467868805,\n        0.04548058286309242,\n        -0.022256748750805855,\n        -0.016177935525774956,\n        0.027934914454817772,\n        -0.016647661104798317,\n        -0.01494835689663887,\n        -0.003172381082549691,\n        0.011349421925842762,\n        -0.003196558216586709,\n        0.0031516579911112785,\n        -0.04114251956343651,\n        0.005695434287190437,\n        -0.003882151795551181,\n        -0.019466020166873932,\n        0.004117014817893505,\n        0.006085721775889397,\n        0.00104825000744313,\n        0.013663516379892826,\n        0.012116181664168835,\n        0.004476217553019524,\n        -0.004845781717449427,\n        -0.010285629890859127,\n        0.02484024502336979,\n        -0.02182846888899803,\n        -0.01174316368997097,\n        -0.02926119975745678,\n        0.0006078814039938152,\n        -0.007653779815882444,\n        0.02402513101696968,\n        0.008392907679080963,\n        -0.026802044361829758,\n        -0.005021929275244474,\n        0.01418850477784872,\n        -0.012157628312706947,\n        0.025254709646105766,\n        0.03111247532069683,\n        -0.006697057280689478,\n        0.02879147417843342,\n        -0.00855178665369749,\n        -0.014865463599562645,\n        0.004548748955130577,\n        0.00044943505781702697,\n        -0.004963213577866554,\n        -0.027216508984565735,\n        -0.019548913463950157,\n        -0.007184052839875221,\n        0.02338961884379387,\n        -0.007184052839875221,\n        -0.02180083841085434,\n        -0.0017329801339656115,\n        -0.0008310878765769303,\n        0.013380298390984535,\n        0.007778118830174208,\n        -0.0023210018407553434,\n        -0.047027915716171265,\n        0.019424574449658394,\n        -0.006852481514215469,\n        -0.00899388175457716,\n        0.018913401290774345,\n        0.03125062957406044,\n        -0.005356954876333475,\n        -0.01518321968615055,\n        -0.024411965161561966,\n        -0.010437600314617157,\n        -0.012551369145512581,\n        0.020709414035081863,\n        -0.0016984413377940655,\n        -0.00830310769379139,\n        0.01197802647948265,\n        0.04144645854830742,\n        -0.005598725751042366,\n        -0.0018823600839823484,\n        -0.008800464682281017,\n        0.00043540369370020926,\n        0.010741541162133217,\n        0.0010983311804011464,\n        -0.03567158803343773,\n        0.012157628312706947,\n        0.006859389133751392,\n        -0.012074735015630722,\n        0.004165369085967541,\n        0.007743580266833305,\n        0.0006463057361543179,\n        0.020571259781718254,\n        -0.04501085355877876,\n        0.0030014144722372293,\n        0.010064582340419292,\n        -0.0001065303513314575,\n        -0.0003052359097637236,\n        -0.012724063359200954,\n        -0.012095458805561066,\n        -0.008199491538107395,\n        -0.002226020209491253,\n        -0.013242144137620926,\n        -0.0010275267995893955,\n        0.01953509822487831,\n        0.006717780139297247,\n        -0.00712188333272934,\n        -0.010347799398005009,\n        0.0005146268522366881,\n        -0.03304664418101311,\n        -0.022629767656326294,\n        0.001903083291836083,\n        0.02528234012424946,\n        -0.041556984186172485,\n        -0.021151509135961533,\n        0.004994298331439495,\n        -0.0007728037890046835,\n        0.014893095009028912,\n        0.000305020046653226,\n        -0.008731387555599213,\n        0.007791934534907341,\n        -0.00872447993606329,\n        -0.014975987374782562,\n        -0.00476634269580245,\n        -0.001241666846908629,\n        -0.01994956284761429,\n        -0.012240521609783173,\n        -0.03288085758686066,\n        -0.021814653649926186,\n        -0.03962281718850136,\n        0.018346965312957764,\n        0.021234402433037758,\n        -0.017089756205677986,\n        0.0029461525846272707,\n        -0.015514791011810303,\n        -0.018070656806230545,\n        -0.006147891748696566,\n        0.020170610398054123,\n        -0.045231904834508896,\n        -0.022602135315537453,\n        0.005270608235150576,\n        -0.025088923051953316,\n        0.004600557032972574,\n        0.026042193174362183,\n        -0.010361614637076855,\n        0.016108857467770576,\n        -0.017531853169202805,\n        -0.024964584037661552,\n        0.004973575007170439,\n        -0.01116982102394104,\n        0.01649569161236286,\n        0.022823184728622437,\n        -0.015804916620254517,\n        -0.007639964111149311,\n        0.03586500510573387,\n        0.009159667417407036,\n        0.018181180581450462,\n        -0.005709249991923571,\n        -0.005781780928373337,\n        0.016108857467770576,\n        0.011605008505284786,\n        0.012475384399294853,\n        -0.00624459981918335,\n        -0.011674086563289165,\n        -0.0025437765289098024,\n        0.017421329393982887,\n        0.006545086856931448,\n        -0.0006139256875030696,\n        -0.003289812710136175,\n        0.011073112487792969,\n        0.04147408902645111,\n        0.012862217612564564,\n        0.01684107817709446,\n        -0.023306725546717644,\n        0.019493652507662773,\n        0.008289291523396969,\n        0.012399399653077126,\n        -0.027727682143449783,\n        -0.007377469912171364,\n        0.0180568415671587,\n        -0.006113352719694376,\n        -0.020004823803901672,\n        -0.007128790952265263,\n        -0.013974364846944809,\n        -0.003262181766331196,\n        0.010555031709372997,\n        0.0056712571531534195,\n        0.0239560529589653,\n        -0.005329323932528496,\n        0.015873994678258896,\n        0.023831713944673538,\n        -0.011971118859946728,\n        0.03221771493554115,\n        -0.01032707653939724,\n        -0.011888226494193077,\n        0.008904080837965012,\n        0.02180083841085434,\n        0.037605755031108856,\n        0.02325146459043026,\n        -0.014001995325088501,\n        -0.023790268227458,\n        -0.00516353826969862,\n        0.013967457227408886,\n        0.013711870647966862,\n        0.026277055963873863,\n        0.006762680597603321,\n        0.010693186894059181,\n        0.008413631469011307,\n        0.016274644061923027,\n        0.007936997339129448,\n        -0.001714847283437848,\n        -0.00437950948253274,\n        -0.001650950638577342,\n        0.002709562424570322,\n        0.015804916620254517,\n        0.002201843075454235,\n        0.06537488102912903,\n        0.020460736006498337,\n        0.013656608760356903,\n        0.022021885961294174,\n        -0.020391657948493958,\n        -0.007315299939364195,\n        -0.008012982085347176,\n        0.0065416330471634865,\n        -0.012523738667368889,\n        -0.0025437765289098024,\n        0.025669174268841743,\n        0.013000372797250748,\n        -0.0018996293656527996,\n        0.041225410997867584,\n        0.014865463599562645,\n        0.017338436096906662,\n        0.017573298886418343,\n        -0.017296988517045975,\n        -0.020156795158982277,\n        -0.0008246118668466806,\n        0.0006061544991098344,\n        -0.005612541455775499,\n        0.015155589208006859,\n        -0.002258832100778818,\n        -0.011542838998138905,\n        -0.019355496391654015,\n        0.006952643860131502,\n        -0.032853227108716965,\n        0.01744895987212658,\n        0.019106818363070488,\n        -0.028929628431797028,\n        0.001288294093683362,\n        0.014768755063414574,\n        0.025365233421325684,\n        -0.012440845370292664,\n        -0.02182846888899803,\n        0.013048727065324783,\n        0.0074396394193172455,\n        -0.013760224916040897,\n        -0.02898489125072956,\n        -0.005636718589812517,\n        0.021842284128069878,\n        -4.6249500883277506e-05,\n        -0.007943904958665371,\n        0.00017215391562785953,\n        0.002833901671692729,\n        -0.013117804192006588,\n        -0.009657025337219238,\n        -0.004210269544273615,\n        -0.02021205611526966,\n        0.02568298950791359,\n        0.016274644061923027,\n        0.004155007656663656,\n        -0.014229951426386833,\n        0.001432493212632835,\n        0.017932502552866936,\n        -0.029150675982236862,\n        -0.013331945054233074,\n        0.0029772373382002115,\n        0.010914234444499016,\n        0.004528025630861521,\n        -0.02460538223385811,\n        0.006897381506860256,\n        -0.01609504222869873,\n        -0.00013005985238123685,\n        -0.013608254492282867,\n        0.01656476780772209,\n        0.001359098474495113,\n        0.01649569161236286,\n        -0.0032000120263546705,\n        -0.011618824675679207,\n        0.007018267177045345,\n        0.029951974749565125,\n        0.04863051325082779,\n        -0.00240044086240232,\n        -0.016965417191386223,\n        0.0036438344977796078,\n        0.012461569160223007,\n        0.049183133989572525,\n        0.006983728613704443,\n        -0.020391657948493958,\n        0.01197802647948265,\n        -0.0023866253904998302,\n        -0.004728350322693586,\n        -0.019921932369470596,\n        -0.019659437239170074,\n        0.03492555022239685,\n        -0.020336396992206573,\n        -0.01710357330739498,\n        -0.013725685887038708,\n        0.012060919776558876,\n        -0.0074327317997813225,\n        0.025227079167962074,\n        0.016288459300994873,\n        -7.161386747611687e-05,\n        -0.021787023171782494,\n        0.005502017680555582,\n        -0.0003976269799750298,\n        0.008835003711283207,\n        0.020267318934202194,\n        0.0173798818141222,\n        0.00300314137712121,\n        0.024149470031261444,\n        0.020018640905618668,\n        0.011453038081526756,\n        0.017490405589342117,\n        0.019825223833322525,\n        -0.017573298886418343,\n        0.014533892273902893,\n        0.014975987374782562,\n        -0.01029253751039505,\n        0.013352667912840843,\n        -0.022864630445837975,\n        0.028294116258621216,\n        0.004306978080421686,\n        0.01351845357567072,\n        0.024950768798589706,\n        -0.0047870660200715065,\n        0.006524363532662392,\n        0.02591785229742527,\n        -0.0002944425505120307,\n        0.0002605514309834689,\n        0.011936580762267113,\n        -0.004386417102068663,\n        0.00042828009463846684,\n        0.005716157611459494,\n        -0.020640337839722633,\n        -0.013587530702352524,\n        5.7258715969510376e-05,\n        -0.015445713885128498,\n        -0.006144437473267317,\n        0.014098703861236572,\n        0.011812240816652775,\n        0.014437183737754822,\n        -0.023817898705601692,\n        -0.019410759210586548,\n        0.01439573708921671,\n        -0.024384334683418274,\n        -0.0415017232298851,\n        0.007819565013051033,\n        0.011867502704262733,\n        0.002799362875521183,\n        0.014216136187314987,\n        -0.0013513272861018777,\n        -0.006558902096003294,\n        -0.018913401290774345,\n        -0.020433105528354645,\n        0.004113561008125544,\n        -0.007874827831983566,\n        -0.007308392319828272,\n        -0.0458950474858284,\n        -0.005636718589812517,\n        0.011225082911550999,\n        0.0165371373295784,\n        0.017932502552866936,\n        0.0018184634391218424,\n        -0.01331122126430273,\n        -0.0009083682671189308,\n        -0.016854893416166306,\n        -0.032549288123846054,\n        -0.015404267236590385,\n        -0.03078090399503708,\n        0.004224085249006748,\n        -0.0059579284861683846,\n        0.0032570010516792536,\n        -0.006406932137906551,\n        -0.00434497045353055,\n        0.004673088435083628,\n        -0.001097467727959156,\n        -0.003015229944139719,\n        0.04890682175755501,\n        -0.027658604085445404,\n        0.030863797292113304,\n        0.004051391500979662,\n        0.0006225603865459561,\n        0.013055634684860706,\n        0.0011432315222918987,\n        -0.018222626298666,\n        -0.013304313644766808,\n        -0.014713493175804615,\n        -0.009359992109239101,\n        0.0014739397447556257,\n        -0.005426032468676567,\n        0.0005271471454761922,\n        0.007854104042053223,\n        0.009152759797871113,\n        0.02865331992506981,\n        -0.02898489125072956,\n        -0.0046212803572416306,\n        -0.006196245551109314,\n        -0.0032172815408557653,\n        -0.006800673436373472,\n        0.02318238653242588,\n        0.016647661104798317,\n        0.03685971722006798,\n        -0.0018063748721033335,\n        -0.011915856972336769,\n        0.010299445129930973,\n        -0.001650950638577342,\n        -0.0217732060700655,\n        0.019825223833322525,\n        0.015141773037612438,\n        -0.00682485057041049,\n        -0.016481876373291016,\n        0.004811243154108524,\n        -0.015058880671858788,\n        0.010720817372202873,\n        0.02196662314236164,\n        -0.022629767656326294,\n        0.02790728211402893,\n        0.008814280852675438,\n        -0.056809280067682266,\n        -0.02278173714876175,\n        -0.020667968317866325,\n        -0.017048310488462448,\n        0.02696782909333706,\n        -0.02051599696278572,\n        0.01197802647948265,\n        0.008392907679080963,\n        -0.003781989449635148,\n        -0.009049143642187119,\n        -0.018512751907110214,\n        0.05003969371318817,\n        -0.028100699186325073,\n        -0.01812591776251793,\n        0.014616784639656544,\n        -0.0062756845727562904,\n        0.019286420196294785,\n        -0.0003641675866674632,\n        -0.007025174796581268,\n        -0.04907260835170746,\n        -0.01957654394209385,\n        -0.02584877610206604,\n        -0.027023091912269592,\n        -0.0024574296548962593,\n        0.007660687435418367,\n        0.03412425145506859,\n        0.01579110138118267,\n        0.027105985209345818,\n        0.004914859309792519,\n        -0.011598100885748863,\n        0.008717572316527367,\n        -0.006710872519761324,\n        3.8559239328606054e-05,\n        0.004728350322693586,\n        -0.018167365342378616,\n        -0.012109274044632912,\n        -0.0026249424554407597,\n        0.009511962532997131,\n        -0.022933708503842354,\n        0.003940867725759745,\n        -0.002552411053329706,\n        0.020598890259861946,\n        0.020156795158982277,\n        -0.0021034078672528267,\n        -0.01602596417069435,\n        -0.011453038081526756,\n        -0.02423236332833767,\n        -0.005871581844985485,\n        0.00040237605571746826,\n        0.0021034078672528267,\n        -0.007943904958665371,\n        -0.027589526027441025,\n        0.004569472279399633,\n        0.011231990531086922,\n        0.03920835256576538,\n        0.015597684308886528,\n        0.004821605049073696,\n        0.004372601397335529,\n        -0.009954058565199375,\n        0.0029185216408222914,\n        0.01883050799369812,\n        0.04136356711387634,\n        -0.0069871824234724045,\n        -0.019991008564829826,\n        -0.027520449832081795,\n        0.023541588336229324,\n        0.03067038021981716,\n        0.02503366209566593,\n        -0.006534724961966276,\n        0.01697923243045807,\n        0.007591609843075275,\n        -0.011218175292015076,\n        0.0048561436124145985,\n        0.007598517462611198,\n        -0.0025144186802208424,\n        0.0050737373530864716,\n        0.005118637811392546,\n        -0.024646827951073647,\n        -0.00781265739351511,\n        -0.0075087170116603374,\n        0.022615952417254448,\n        0.015431898646056652,\n        0.005567640997469425,\n        -0.01859564520418644,\n        0.02254687435925007,\n        -0.01572202332317829,\n        -0.004023760557174683,\n        0.020695598796010017,\n        -0.006897381506860256,\n        0.019106818363070488,\n        -0.0004956306074745953,\n        0.003250093199312687,\n        -0.005481294356286526,\n        0.009670840576291084,\n        0.009325453080236912,\n        0.0022104778327047825,\n        0.017697637900710106,\n        0.011197451502084732,\n        0.022284379228949547,\n        0.019728515297174454,\n        -0.027257954701781273,\n        -0.021538343280553818,\n        0.022629767656326294,\n        0.004983936902135611,\n        -0.018609460443258286,\n        -0.030919058248400688,\n        0.015127957798540592,\n        0.019314050674438477,\n        0.020101532340049744,\n        -0.014368105679750443,\n        -0.021137693896889687,\n        -0.027313217520713806,\n        0.03691498190164566,\n        0.017200279980897903,\n        0.007045898120850325,\n        0.0025904036592692137,\n        -0.014299028553068638,\n        -0.030532225966453552,\n        0.01970088481903076,\n        -0.016039779409766197,\n        0.01539045199751854,\n        0.014029626734554768,\n        0.007639964111149311,\n        -0.0063827550038695335,\n        0.019825223833322525,\n        -0.0008755564922466874,\n        0.014098703861236572,\n        0.003730181371793151,\n        -0.002447067992761731,\n        -0.03130589425563812,\n        0.011301067657768726,\n        -0.0023935330100357533,\n        -0.006406932137906551,\n        -0.0008764199446886778,\n        -0.016205566003918648,\n        -0.0027890014462172985,\n        0.03111247532069683,\n        -0.03246639296412468,\n        -0.0320519283413887,\n        0.018070656806230545,\n        -0.030007237568497658,\n        0.01579110138118267,\n        -0.01017510611563921,\n        -0.004431317560374737,\n        0.019396943971514702,\n        0.0008367004338651896,\n        -0.00824784580618143,\n        0.024038946256041527,\n        0.013532268814742565,\n        -0.021690314635634422,\n        0.018264073878526688,\n        0.002878802129998803,\n        0.02355540543794632,\n        -0.02624942548573017,\n        0.01778053119778633,\n        0.001873725326731801,\n        -0.03614131361246109,\n        -0.011784610338509083,\n        0.006973366718739271,\n        0.013014188036322594,\n        0.005332777742296457,\n        0.017490405589342117,\n        -0.01218525879085064,\n        0.006755772978067398,\n        0.0039028748869895935,\n        -0.016191750764846802,\n        0.012779325246810913,\n        0.03277033567428589,\n        -0.006524363532662392,\n        -0.015666762366890907,\n        -0.017656192183494568,\n        -0.02196662314236164,\n        -0.036997873336076736,\n        0.020322579890489578,\n        -0.03185851126909256,\n        -0.012427030131220818,\n        0.001960072200745344,\n        0.01059647835791111,\n        0.014699677936732769,\n        0.02011534944176674,\n        -0.0018996293656527996,\n        -0.026940198615193367,\n        0.0046281879767775536,\n        0.013753317296504974,\n        -0.0009023239836096764,\n        -0.004255170002579689,\n        -0.004604010842740536,\n        -0.006807581055909395,\n        -0.010534308850765228,\n        0.03719129040837288,\n        -0.018844323232769966,\n        0.011680994182825089,\n        -0.016744369640946388,\n        0.014464814215898514,\n        0.009187298826873302,\n        -0.022450165823101997,\n        -0.021579790860414505,\n        -0.03387557342648506,\n        0.010485954582691193,\n        -0.02460538223385811,\n        -0.015459529124200344,\n        0.021925177425146103,\n        0.005453663412481546,\n        -0.007446547504514456,\n        0.016260826960206032,\n        0.004030668176710606,\n        -0.01331122126430273,\n        -0.028059253469109535,\n        0.009470515884459019,\n        0.003431421471759677,\n        0.021317295730113983,\n        -0.02828030101954937,\n        -0.0067903115414083,\n        -0.016108857467770576,\n        -0.00739819323644042,\n        -0.028459902852773666,\n        -0.0024867875035852194,\n        0.007142606656998396,\n        0.00048138335114344954,\n        0.04072805494070053,\n        -0.015818731859326363,\n        -0.015804916620254517,\n        -0.012592815794050694,\n        -0.045673999935388565,\n        -0.008931712247431278,\n        0.0005124682211317122,\n        -0.003864882281050086,\n        -0.009056051261723042,\n        0.011093835346400738,\n        -0.004904497880488634,\n        0.03431766852736473,\n        -0.012385583482682705,\n        -0.009228745475411415,\n        -0.012088551186025143,\n        0.023900792002677917,\n        -0.0217732060700655,\n        -0.001094877370633185,\n        0.00011343809455865994,\n        -0.020916646346449852,\n        -0.012952018529176712,\n        -0.007923181168735027,\n        -0.013449376448988914,\n        -0.004300069995224476,\n        -0.0016958509804680943,\n        0.0031706541776657104,\n        0.013642792589962482,\n        -0.016882523894309998,\n        0.018650906160473824,\n        0.03580974042415619,\n        0.026111269369721413,\n        -0.007992259226739407,\n        -0.005975198000669479,\n        -0.018982479348778725,\n        0.003937413915991783,\n        0.009214929305016994,\n        -0.008814280852675438,\n        -0.013235236518085003,\n        0.019866669550538063,\n        0.0106310173869133,\n        -0.02514418587088585,\n        -0.0048699588514864445,\n        -0.03310190513730049,\n        -0.005394947715103626,\n        0.01391910295933485,\n        -0.019521282985806465,\n        0.004106653388589621,\n        0.0036438344977796078,\n        -0.020239688456058502,\n        0.007709041703492403,\n        0.007598517462611198,\n        -0.010472138412296772,\n        -0.008068243972957134,\n        0.003049768740311265,\n        -0.00754325557500124,\n        -0.015887809917330742,\n        0.01518321968615055,\n        -0.027492817491292953,\n        -0.004006491042673588,\n        0.01700686477124691,\n        -0.01533519010990858,\n        -0.006272230762988329,\n        0.004103199578821659,\n        -0.0031171191949397326,\n        -0.0008341100183315575,\n        -0.036832086741924286,\n        0.0020947731100022793,\n        -0.010651740245521069,\n        -0.015321374870836735,\n        0.00918039120733738,\n        -0.01691015623509884,\n        -0.009346176870167255,\n        -0.019562728703022003,\n        -0.01539045199751854,\n        -0.0072876689955592155,\n        -0.021980438381433487,\n        0.0017372973961755633,\n        0.007577794604003429,\n        -0.02105480246245861,\n        0.009318545460700989,\n        -0.011632639914751053,\n        0.01246847677975893,\n        0.011736256070435047,\n        0.020198240876197815,\n        0.23497380316257477,\n        -0.007329115644097328,\n        -0.024038946256041527,\n        0.033792681992053986,\n        -0.012689524330198765,\n        0.023210017010569572,\n        0.03749522939324379,\n        -0.015915440395474434,\n        -0.009677748195827007,\n        0.016813447698950768,\n        0.015141773037612438,\n        -0.017863424494862556,\n        -0.04175040125846863,\n        -0.00037344987504184246,\n        0.012143813073635101,\n        -0.014602969400584698,\n        -0.04349115118384361,\n        -0.022312011569738388,\n        -0.013297406025230885,\n        0.003695642575621605,\n        0.03230060636997223,\n        -0.0021880276035517454,\n        -0.013684239238500595,\n        -0.01609504222869873,\n        0.03738470748066902,\n        -0.023679744452238083,\n        0.02044692076742649,\n        0.009746826253831387,\n        0.014768755063414574,\n        0.008904080837965012,\n        -0.0239560529589653,\n        0.001896175555884838,\n        0.0035160414408892393,\n        0.016813447698950768,\n        -0.024950768798589706,\n        -0.00754325557500124,\n        -0.00918039120733738,\n        -0.003899421077221632,\n        0.025323787704110146,\n        0.005726519040763378,\n        0.0016794450348243117,\n        -0.0031913775019347668,\n        -0.03730181232094765,\n        -0.005633264780044556,\n        -0.0015879174461588264,\n        0.019493652507662773,\n        0.01143922284245491,\n        -0.020032456144690514,\n        0.006286046467721462,\n        0.015500975772738457,\n        -0.024964584037661552,\n        0.0070562600158154964,\n        0.02430144138634205,\n        0.011425407603383064,\n        -0.012413214892148972,\n        0.002155215945094824,\n        -0.00141090655233711,\n        0.00369909661822021,\n        0.021994255483150482,\n        0.03702550381422043,\n        -0.010651740245521069,\n        0.004306978080421686,\n        -0.0028010900132358074,\n        0.028100699186325073,\n        -0.0066176182590425014,\n        -0.0006968186353333294,\n        -0.023444881662726402,\n        0.01819499582052231,\n        0.020557444542646408,\n        -0.01132179144769907,\n        0.00224847043864429,\n        -0.01819499582052231,\n        0.00600628275424242,\n        -0.004003037232905626,\n        -0.040507007390260696,\n        -0.010043858550488949,\n        0.024784982204437256,\n        0.015376636758446693,\n        0.006434563081711531,\n        0.0165371373295784,\n        -0.008717572316527367,\n        -0.01724172756075859,\n        0.0015127958031371236,\n        0.0004852689744438976,\n        -0.02079230733215809,\n        -0.021234402433037758,\n        -0.007722856942564249,\n        0.009422161616384983,\n        -0.015169404447078705,\n        -1.9984559003205504e-06,\n        -0.003313989844173193,\n        -0.010637925006449223,\n        -0.027630973607301712,\n        -0.0002275237930007279,\n        0.0010966042755171657,\n        -0.015418083406984806,\n        0.01943838968873024,\n        0.010734633542597294,\n        -0.0049459440633654594,\n        0.009670840576291084,\n        -0.015915440395474434,\n        0.020502181723713875,\n        0.008627771399915218,\n        0.0032449124846607447,\n        -0.0041895462200045586,\n        -0.017904870212078094,\n        -0.016316089779138565,\n        -0.004538387525826693,\n        0.018070656806230545,\n        -0.013891471549868584,\n        -0.002217385685071349,\n        -0.03520185872912407,\n        0.006279138848185539,\n        -0.004845781717449427,\n        0.0032414584420621395,\n        0.011515208519995213,\n        0.019051555544137955,\n        0.0023261825554072857,\n        0.01363588497042656,\n        -0.012675709091126919,\n        0.01784960925579071,\n        -0.00929782260209322,\n        0.004807789344340563,\n        0.018346965312957764,\n        0.015058880671858788,\n        -0.010451415553689003,\n        -0.0350913368165493,\n        0.0012943383771926165,\n        -0.009470515884459019,\n        -0.022215303033590317,\n        0.013988180086016655,\n        -0.03050459362566471,\n        0.007066621445119381,\n        -0.008365277200937271,\n        -0.02159360609948635,\n        -0.015708208084106445,\n        -0.003524675965309143,\n        -0.011964211240410805,\n        -0.0006061544991098344,\n        0.003291539615020156,\n        0.011729348450899124,\n        -0.009539593942463398,\n        0.018471306189894676,\n        0.005605633836239576,\n        0.011570470407605171,\n        -0.01677200011909008,\n        0.017269358038902283,\n        0.027257954701781273,\n        -0.002272647572681308,\n        -0.015266112983226776,\n        -0.0106310173869133,\n        -0.004755981266498566,\n        0.007646871730685234,\n        -0.010789895430207253,\n        0.016689108684659004,\n        -0.0033934288658201694,\n        -0.005160084459930658,\n        -0.030062498524785042,\n        0.0069111972115933895,\n        -0.00459019560366869,\n        -0.010879695415496826,\n        -0.004196453839540482,\n        0.003654196159914136,\n        0.0015102053293958306,\n        -0.016260826960206032,\n        0.0024021677672863007,\n        -0.17860661447048187,\n        0.008240938186645508,\n        0.02865331992506981,\n        -0.0327427014708519,\n        0.017628561705350876,\n        -0.00531550869345665,\n        0.020902831107378006,\n        0.011494484730064869,\n        -0.03263217955827713,\n        0.017573298886418343,\n        0.0037163658998906612,\n        -0.005363862495869398,\n        -0.03279796615242958,\n        -0.024757351726293564,\n        0.00933236163109541,\n        -0.0052257077768445015,\n        -0.012060919776558876,\n        0.013739501126110554,\n        0.012475384399294853,\n        0.01632990501821041,\n        0.01677200011909008,\n        -0.01418850477784872,\n        0.029454616829752922,\n        0.009574132040143013,\n        -0.0020636883564293385,\n        0.0008250435930676758,\n        -0.014575337991118431,\n        0.007619240786880255,\n        -0.0050668297335505486,\n        -0.038959670811891556,\n        0.0006678924546577036,\n        -0.015321374870836735,\n        0.012945110909640789,\n        -0.00797844398766756,\n        -0.017062125727534294,\n        -0.0010577482171356678,\n        -0.026194162666797638,\n        -0.01067937072366476,\n        -0.004158461466431618,\n        0.010852064937353134,\n        0.031499311327934265,\n        0.019452204927802086,\n        0.02605600841343403,\n        0.015597684308886528,\n        -0.0034055174328386784,\n        0.0027786397840827703,\n        0.030117761343717575,\n        -0.043076686561107635,\n        0.00266984268091619,\n        -0.02524089440703392,\n        0.006565810181200504,\n        -0.013815486803650856,\n        -0.0004710217472165823,\n        -0.025834960862994194,\n        0.012751693837344646,\n        -0.004977029282599688,\n        -0.0034469638485461473,\n        0.015155589208006859,\n        -0.013283590786159039,\n        -0.01032707653939724,\n        -0.00987116526812315,\n        -0.008075151592493057,\n        0.031195368617773056,\n        0.011080020107328892,\n        0.0020291495602577925,\n        0.001252028509043157,\n        0.006393116433173418,\n        0.00681794248521328,\n        -0.0239560529589653,\n        0.018650906160473824,\n        -0.009594855830073357,\n        -0.009138944558799267,\n        -0.009643210098147392,\n        -0.017324620857834816,\n        0.00987116526812315,\n        0.02528234012424946,\n        -0.028059253469109535,\n        -0.004072114825248718,\n        -0.04578452184796333,\n        0.010216552764177322,\n        -0.016081226989626884,\n        0.034068990498781204,\n        -0.007895550690591335,\n        0.004338062833994627,\n        -0.01988048478960991,\n        -0.012275059707462788,\n        0.004358786158263683,\n        0.014685862697660923,\n        -0.013967457227408886,\n        -0.02334817312657833,\n        0.05357645824551582,\n        -0.030587486922740936,\n        -0.011356329545378685,\n        -0.0064034778624773026,\n        0.054819852113723755,\n        0.031333524733781815,\n        0.007239315193146467,\n        -0.013642792589962482,\n        0.02327909506857395,\n        -0.003992675803601742,\n        -0.01758711412549019,\n        -0.0006514865672215819,\n        -0.01883050799369812,\n        -0.0022225663997232914,\n        0.02274029143154621,\n        -0.0004071251314599067,\n        0.0086899409070611,\n        -0.007702133618295193,\n        0.014001995325088501,\n        -0.011010942980647087,\n        -0.007135699037462473,\n        -0.00887645035982132,\n        0.00456256465986371,\n        -0.009926427155733109,\n        -0.017531853169202805,\n        0.02327909506857395,\n        0.0034884102642536163,\n        0.0008807372651062906,\n        -0.0023210018407553434,\n        0.025738252326846123,\n        0.05584219843149185,\n        -0.015597684308886528,\n        -0.018678538501262665,\n        -0.00960176344960928,\n        -0.018637090921401978,\n        -0.024218548089265823,\n        -0.10753975063562393,\n        -0.035422906279563904,\n        0.015749655663967133,\n        0.01554242242127657,\n        0.0005716157611459494,\n        0.018885770812630653,\n        0.006517455913126469,\n        0.0006303315749391913,\n        -0.015500975772738457,\n        0.02879147417843342,\n        -0.005578002892434597,\n        -0.011687901802361012,\n        -0.0016250465996563435,\n        -0.015473345294594765,\n        0.0021673045121133327,\n        -0.024577749893069267,\n        -0.0019393488764762878,\n        -0.002364175161346793,\n        0.012302691116929054,\n        0.015915440395474434,\n        0.009967873804271221,\n        -0.016813447698950768,\n        0.024660643190145493,\n        -0.0046972655691206455,\n        -0.026746781542897224,\n        0.0015110688982531428,\n        -0.01128725241869688,\n        0.007142606656998396,\n        0.010126751847565174,\n        0.01162573229521513,\n        0.0008539697737433016,\n        -0.018871955573558807,\n        0.007405100855976343,\n        -0.011080020107328892,\n        0.0010689733317121863,\n        0.013850024901330471,\n        -0.014202320016920567,\n        -0.03359926491975784,\n        0.0038096203934401274,\n        -0.017158834263682365,\n        -0.0019894300494343042,\n        0.008047521114349365,\n        0.015362821519374847,\n        0.004887228365987539,\n        0.015500975772738457,\n        -0.008261661045253277,\n        -0.029758557677268982,\n        0.025890221819281578,\n        0.0073498389683663845,\n        -0.012212890200316906,\n        -0.0249783992767334,\n        -0.0045073023065924644,\n        -0.026194162666797638,\n        -0.012406307272613049,\n        0.014506260864436626,\n        -0.027105985209345818,\n        0.005567640997469425,\n        -0.008738295175135136,\n        0.013449376448988914,\n        0.0034210598096251488,\n        0.00815113727003336,\n        0.00398231390863657,\n        -0.015210851095616817,\n        0.0428556390106678,\n        0.041999079287052155,\n        -0.004465856123715639,\n        -0.008137322030961514,\n        -0.015666762366890907,\n        0.008821188472211361,\n        -0.02416328527033329,\n        -0.02318238653242588,\n        0.02072322927415371,\n        -0.008441261947154999,\n        0.02848753333091736,\n        -0.02713361568748951,\n        -0.0015939617296680808,\n        -0.008765926584601402,\n        -0.010948773473501205,\n        -0.0033709786366671324,\n        0.01677200011909008,\n        -0.03600315749645233,\n        -0.012178351171314716,\n        -0.008931712247431278,\n        0.006493278779089451,\n        -0.0023261825554072857,\n        0.012123089283704758,\n        -0.01819499582052231,\n        -0.023776452988386154,\n        0.010492862202227116,\n        -0.031001951545476913,\n        -0.01943838968873024,\n        0.03627946972846985,\n        0.009691563434898853,\n        -0.008123505860567093,\n        -0.01923115737736225,\n        0.01728317327797413,\n        -0.014015811495482922,\n        -0.004731804132461548,\n        0.016661476343870163,\n        0.02423236332833767,\n        -0.0003350255428813398,\n        -0.007619240786880255,\n        -0.04067279398441315,\n        0.017131203785538673,\n        0.02018442563712597,\n        -0.0024505220353603363,\n        0.010472138412296772,\n        0.0069802748039364815,\n        0.032853227108716965,\n        -0.002194935455918312,\n        -0.015639130026102066,\n        -0.014478630386292934,\n        -0.02318238653242588,\n        0.001557696145027876,\n        -0.022906076163053513,\n        -0.008109690621495247,\n        -0.004970121197402477,\n        -0.011694809421896935,\n        0.034704502671957016,\n        -0.011121466755867004,\n        0.012371768243610859,\n        0.01693778671324253,\n        0.009256375953555107,\n        0.01192967314273119,\n        -0.004479671362787485,\n        0.01285530999302864,\n        0.029482249170541763,\n        -0.016647661104798317,\n        -0.0008716708398424089,\n        -0.0017519764369353652,\n        -0.023210017010569572,\n        -0.016288459300994873,\n        0.0030100492294877768,\n        -0.012613539583981037,\n        0.005868128035217524,\n        0.021607421338558197,\n        0.0037509046960622072,\n        -0.0004882910870946944,\n        -0.006690149195492268,\n        0.018498936668038368,\n        0.019079187884926796,\n        -0.004034121986478567,\n        0.009933334775269032,\n        -0.00422753905877471,\n        -0.007101160008460283,\n        -0.03627946972846985,\n        0.01348391454666853,\n        -0.008517247624695301,\n        -0.01376713253557682,\n        0.008945527486503124,\n        0.01649569161236286,\n        -7.420427573379129e-05,\n        0.0249783992767334,\n        0.011805333197116852,\n        0.008821188472211361,\n        -0.00564362620934844,\n        0.002845990238711238,\n        -0.02240872010588646,\n        -0.014299028553068638,\n        -0.014685862697660923,\n        0.003933959640562534,\n        -0.04448586702346802,\n        0.02088901586830616,\n        0.013304313644766808,\n        0.007418916095048189,\n        -0.019286420196294785,\n        0.014699677936732769,\n        0.001908264122903347,\n        -0.003588572610169649,\n        0.016012148931622505,\n        -0.0005072873900644481,\n        -0.020129164680838585,\n        -0.007971535436809063,\n        0.02170412987470627,\n        0.01586017943918705,\n        0.010907326824963093,\n        0.022726476192474365,\n        0.011632639914751053,\n        0.001780470833182335,\n        0.010548124089837074,\n        -0.005353501066565514,\n        0.021621236577630043,\n        0.024881690740585327,\n        -0.011266529560089111,\n        -0.019797591492533684,\n        0.016053594648838043,\n        0.035616323351860046,\n        -0.01116982102394104,\n        -0.013725685887038708,\n        0.018581829965114594,\n        -0.00985044240951538,\n        -0.00011246669600950554,\n        -0.0028736211825162172,\n        -0.0039719524793326855,\n        -0.001913444953970611,\n        -0.01988048478960991,\n        0.008869542740285397,\n        0.03492555022239685,\n        -0.029924344271421432,\n        0.01866472326219082,\n        0.03277033567428589,\n        0.010534308850765228,\n        -0.0067903115414083,\n        0.006406932137906551,\n        -0.011238898150622845,\n        -0.01409179624170065,\n        -0.017020680010318756,\n        0.01934168115258217,\n        -0.011142189614474773,\n        -0.022602135315537453,\n        0.0005897486116737127,\n        0.022187670692801476,\n        0.02119295671582222,\n        0.020999539643526077,\n        0.02102717012166977,\n        0.0017070760950446129,\n        -0.013338852673768997,\n        0.02432907186448574,\n        -0.02327909506857395,\n        -0.010195828974246979,\n        -0.017573298886418343,\n        0.007909365929663181,\n        0.008544878102838993,\n        -0.001028390252031386,\n        0.013449376448988914,\n        -0.009456700645387173,\n        0.02898489125072956,\n        0.0012813863577321172,\n        0.00696991290897131,\n        -0.04570163041353226,\n        0.01670292392373085,\n        0.006165160797536373,\n        -0.0028960714116692543,\n        -0.0239560529589653,\n        -0.01116982102394104,\n        -0.007197868544608355,\n        -0.008489616215229034,\n        -0.03312953561544418,\n        0.0008276340086013079,\n        0.011798425577580929,\n        -0.01936931163072586,\n        0.07836143672466278,\n        0.012136905454099178,\n        0.005968290381133556,\n        -0.02362448163330555,\n        0.009007696993649006,\n        0.0003779830876737833,\n        0.011211267672479153,\n        -0.0032984474673867226,\n        -0.02372119016945362,\n        -0.030449332669377327,\n        -0.011149098165333271,\n        0.0016751277726143599,\n        0.02028113417327404,\n        -0.027893466874957085,\n        0.004503848496824503,\n        0.011639547534286976,\n        -0.004735257942229509,\n        0.0261527169495821,\n        0.004255170002579689,\n        -0.014975987374782562,\n        0.01572202332317829,\n        -0.013856933452188969,\n        0.022008070722222328,\n        0.009525777772068977,\n        -0.007156421896070242,\n        -0.012876033782958984,\n        0.018982479348778725,\n        -0.0005068556638434529,\n        -0.01896866224706173,\n        -0.0367768257856369,\n        0.007080436684191227,\n        0.009670840576291084,\n        -0.0055123791098594666,\n        -0.01744895987212658,\n        -0.01983903907239437,\n        -0.020405473187565804,\n        -0.010403061285614967,\n        0.0023607213515788317,\n        0.019079187884926796,\n        0.047027915716171265,\n        -0.0021690314169973135,\n        0.014975987374782562,\n        -0.018471306189894676,\n        -0.021469267085194588,\n        -0.026719151064753532,\n        -0.0006329219904728234,\n        0.0009714014013297856,\n        -0.0035505800042301416,\n        -0.03522948920726776\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"ada\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 12,\n    \"total_tokens\": 12\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"xrequestid":"ef69651e-3f9b-4e6e-8086-6702c29b9537","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-49432/49433","correlationid":"ef69651e-3f9b-4e6e-8086-6702c29b9537"}
{"specversion":"1.0","id":"62a7e10e-2bd0-4ad6-869d-d31bd0a94468","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"Prompt_variants","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0xe56ecf03d4fff786","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x4d0bbb09563f095d","start_time":"2024-02-05T15:56:05.767721Z","end_time":"2024-02-05T15:56:05.770483Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"render_template_jinja2","node_name":"Prompt_variants","flow_id":"","root_run_id":"","line_run_id":"d13da436-d3ad-4905-9aae-2dfc0cb2b538","inputs":"{\n  \"template\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n {{contexts}} \\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\nuser: {{question}} \\nassistant:\",\n  \"chat_history\": [\n    {\n      \"inputs\": {\n        \"question\": \"how to calculate Attention?\"\n      },\n      \"outputs\": {\n        \"output\": \"To calculate attention, we use a compatibility function of the query with the corresponding key, and then compute the dot products of the query with all keys, divide each by the square root of the dimension of the keys, and apply a softmax function to obtain the weights on the values. This is known as Scaled Dot-Product Attention. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf)\"\n      }\n    }\n  ],\n  \"contexts\": \"Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n  \"question\": \"how to improve Attension calculation efficiency?\"\n}","output":"\"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\n user: \\nhow to calculate Attention? \\nassistant: \\nTo calculate attention, we use a compatibility function of the query with the corresponding key, and then compute the dot products of the query with all keys, divide each by the square root of the dimension of the keys, and apply a softmax function to obtain the weights on the values. This is known as Scaled Dot-Product Attention. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf) \\nuser: how to improve Attension calculation efficiency? \\nassistant:\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"collectdatatype":"pandas.core.frame.DataFrame","contentrange":"bytes 0-17529/17530","correlationid":"26dd5c16-63c3-4420-a6ca-bd6eaa60f102","xrequestid":"26dd5c16-63c3-4420-a6ca-bd6eaa60f102","agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default"}
{"specversion":"1.0","id":"c4bfb9e2-c938-4dd9-b033-7708e15129cf","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"prepare_question_array","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0x110d0b165f5cff88","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x4d0bbb09563f095d","start_time":"2024-02-05T15:56:05.075939Z","end_time":"2024-02-05T15:56:05.076515Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"my_python_tool","node_name":"prepare_question_array","flow_id":"","root_run_id":"","line_run_id":"d13da436-d3ad-4905-9aae-2dfc0cb2b538","inputs":"{\n  \"modified_question\": \"What are some ways to improve the efficiency of Attention calculation?\",\n  \"original_question\": \"how to improve Attension calculation efficiency?\"\n}","output":"[\n  \"What are some ways to improve the efficiency of Attention calculation?\",\n  \"how to improve Attension calculation efficiency?\"\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"xrequestid":"b58a96c5-c4d7-4a87-bfa7-e4e69de87caf","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-968/969","correlationid":"b58a96c5-c4d7-4a87-bfa7-e4e69de87caf"}
{"specversion":"1.0","id":"e83e5c88-e869-4397-a47a-d717bea4b6c9","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"search","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0x79d5c32fe460520a","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x2b2a902ea98f7623","start_time":"2024-02-05T15:56:05.081193Z","end_time":"2024-02-05T15:56:05.657633Z","status":{"status_code":"UNSET"},"attributes":{"span_type":"Retrieval","retrieval.query":"What are some ways to improve the efficiency of Attention calculation?","retrieval.documents":"[\n    {\n        \"document.content\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n        \"document.score\": 0.3891408443450928,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf3\",\n            \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n            \"mtime\": null,\n            \"page_number\": 3,\n            \"stats\": {\n                \"tiktokens\": 551,\n                \"chars\": 2502,\n                \"lines\": 35\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"document.score\": 0.39398106932640076,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf12\",\n            \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n            \"mtime\": null,\n            \"page_number\": 12,\n            \"stats\": {\n                \"tiktokens\": 254,\n                \"chars\": 833,\n                \"lines\": 73\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n        \"document.score\": 0.42212480306625366,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf0\",\n            \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n            \"mtime\": null,\n            \"page_number\": 0,\n            \"stats\": {\n                \"tiktokens\": 668,\n                \"chars\": 2874,\n                \"lines\": 50\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    }\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-9702/9703","correlationid":"e5a4e296-2cbe-4014-a931-53d9aad2c19a","xrequestid":"e5a4e296-2cbe-4014-a931-53d9aad2c19a","modelversion":"default"}
{"specversion":"1.0","id":"5ad6f298-6aa7-41fd-9dd2-7665c1594c8b","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"openai.resources.chat.completions.Completions.create","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0x89e11a2d7769c114","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xbb234096db5a96c4","start_time":"2024-02-05T15:56:05.777037Z","end_time":"2024-02-05T15:56:08.081218Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.chat.completions.Completions.create","node_name":"answer_the_question_with_context","flow_id":"","root_run_id":"","line_run_id":"d13da436-d3ad-4905-9aae-2dfc0cb2b538","inputs":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history:\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"how to calculate Attention?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"To calculate attention, we use a compatibility function of the query with the corresponding key, and then compute the dot products of the query with all keys, divide each by the square root of the dimension of the keys, and apply a softmax function to obtain the weights on the values. This is known as Scaled Dot-Product Attention. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf) \\nuser: how to improve Attension calculation efficiency? \\nassistant:\"\n    }\n  ],\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8ovcE3f3rGGaVqddJNHN2ZzY2b4ny\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"One way to improve the efficiency of attention calculation is to use dot-product attention, which is faster and more space-efficient than additive attention. Additionally, multi-head attention can be used to linearly project the queries, keys, and values multiple times with different, learned linear projections to different dimensions, and then perform the attention function in parallel on each of these projected versions. This can improve the model's performance while also being more parallelizable and requiring less time to train. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf)\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707148566,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 145,\n    \"prompt_tokens\": 1872,\n    \"total_tokens\": 2017\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-10798/10799","correlationid":"7ea5ab1b-75b9-424a-aa16-786194c00133","xrequestid":"7ea5ab1b-75b9-424a-aa16-786194c00133","agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame"}
{"specversion":"1.0","id":"218b701c-622b-4dc9-9f7a-33590773c048","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"openai.resources.embeddings.Embeddings.create","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0xb9ecba887c183cc1","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xcc74e3c1dde67fce","start_time":"2024-02-05T15:56:05.086224Z","end_time":"2024-02-05T15:56:05.610954Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.embeddings.Embeddings.create","node_name":"vector_lookup","flow_id":"","root_run_id":"","line_run_id":"d13da436-d3ad-4905-9aae-2dfc0cb2b538","inputs":"{\n  \"input\": [\n    [\n      5269,\n      311,\n      7417,\n      7867,\n      2711,\n      22702,\n      15374,\n      30\n    ]\n  ],\n  \"model\": \"text-embedding-ada-002\"\n}","output":"{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.023424657061696053,\n        0.03388490527868271,\n        0.03262566775083542,\n        -0.03462899848818779,\n        0.004618392325937748,\n        0.016799356788396835,\n        -0.024168752133846283,\n        -0.018659593537449837,\n        -0.028690556064248085,\n        -0.027273913845419884,\n        0.031051622703671455,\n        0.02695910446345806,\n        -0.0008746684179641306,\n        0.040209706872701645,\n        0.0036990067455917597,\n        0.004317892715334892,\n        0.01874545030295849,\n        0.015983715653419495,\n        0.006267562508583069,\n        -0.014896193519234657,\n        -0.033341143280267715,\n        0.014638622291386127,\n        0.0027188058011233807,\n        -0.004189107101410627,\n        -0.004396595060825348,\n        -0.010016652755439281,\n        0.022637635469436646,\n        -0.017257262021303177,\n        -0.005938444286584854,\n        -0.029062602669000626,\n        0.023667918518185616,\n        -0.0001414181460859254,\n        0.00321963825263083,\n        -0.028189722448587418,\n        -0.012892862781882286,\n        0.002756368136033416,\n        0.009422807954251766,\n        -0.0071404422633349895,\n        0.04023832455277443,\n        -0.011197187006473541,\n        0.03191019222140312,\n        0.005598593503236771,\n        -0.003139147302135825,\n        0.012993029318749905,\n        0.007655584719032049,\n        0.02074878104031086,\n        -0.0016294947126880288,\n        -0.023954110220074654,\n        -0.03027890995144844,\n        0.01565459743142128,\n        0.005337445065379143,\n        0.030622338876128197,\n        -0.01347955223172903,\n        -0.001774378470145166,\n        0.006088694091886282,\n        0.004396595060825348,\n        0.02504163235425949,\n        0.01595509611070156,\n        -0.014445444568991661,\n        -0.007186948321759701,\n        0.007097513880580664,\n        -0.005541355349123478,\n        -0.005691605154424906,\n        0.012299018912017345,\n        -0.017185714095830917,\n        0.0033108615316450596,\n        -0.005441188812255859,\n        -0.007308579050004482,\n        0.016956761479377747,\n        -0.007634120527654886,\n        0.018874235451221466,\n        0.03196743130683899,\n        -0.005308825988322496,\n        -0.016040952876210213,\n        0.037548139691352844,\n        -0.024326156824827194,\n        -0.019589710980653763,\n        -0.008070560172200203,\n        0.024826988577842712,\n        0.020619995892047882,\n        0.001709091360680759,\n        0.0010016652522608638,\n        -0.002266267780214548,\n        0.027059271931648254,\n        0.0010445937514305115,\n        0.0007025072118267417,\n        0.021593041718006134,\n        0.010717818513512611,\n        -0.020290875807404518,\n        0.0032035401090979576,\n        0.01772947609424591,\n        -0.0193464495241642,\n        0.038807373493909836,\n        0.011848269030451775,\n        0.014710170216858387,\n        0.013193362392485142,\n        -0.021521493792533875,\n        0.024454941973090172,\n        -0.019231973215937614,\n        -0.017915498465299606,\n        4.545084266283084e-06,\n        -0.0019604021217674017,\n        -0.040495894849300385,\n        -0.011547769419848919,\n        -0.006811324041336775,\n        -0.0114476028829813,\n        0.006242521107196808,\n        -0.006668228656053543,\n        -0.02280934900045395,\n        0.008778880350291729,\n        -0.006045765243470669,\n        0.026844630017876625,\n        0.003604206256568432,\n        -0.0027617341838777065,\n        -0.0020033305045217276,\n        -0.0025417255237698555,\n        0.005634367000311613,\n        -0.0065644849091768265,\n        -0.033856287598609924,\n        -0.029205696657299995,\n        0.011333126574754715,\n        0.018573736771941185,\n        0.03182433545589447,\n        -0.012599518522620201,\n        -0.007841608487069607,\n        0.02532782219350338,\n        0.018072903156280518,\n        -0.005802503786981106,\n        -0.022866586223244667,\n        -0.02459803782403469,\n        0.003475420642644167,\n        0.029935482889413834,\n        0.012742613442242146,\n        0.012978719547390938,\n        -0.022952444851398468,\n        0.0025882315821945667,\n        0.013665576465427876,\n        0.009286868385970592,\n        -0.037261947989463806,\n        -0.0021750445012003183,\n        0.007054585497826338,\n        0.023438967764377594,\n        0.006525133736431599,\n        -0.02569986879825592,\n        -0.013887373730540276,\n        0.038292232900857925,\n        0.02703065238893032,\n        0.03328390419483185,\n        -0.002790353260934353,\n        0.0005218497244641185,\n        0.0005191667005419731,\n        -0.014710170216858387,\n        0.004042434971779585,\n        0.013021648861467838,\n        -0.004199839197099209,\n        0.00031816287082619965,\n        -0.022537468001246452,\n        0.004300005733966827,\n        -0.008735951967537403,\n        -0.0285760797560215,\n        0.018874235451221466,\n        0.005913402419537306,\n        0.008363905362784863,\n        -0.010474557057023048,\n        0.020548447966575623,\n        0.02303830161690712,\n        -0.005462653003633022,\n        -0.007104668766260147,\n        0.0009135723812505603,\n        0.001971134217455983,\n        0.0073336209170520306,\n        0.02866193652153015,\n        -0.006464318372309208,\n        0.013164743781089783,\n        -0.004772219341248274,\n        -0.002786775818094611,\n        0.037176091223955154,\n        0.0009497933206148446,\n        -0.008485536091029644,\n        -0.02591451071202755,\n        -0.0015624189982190728,\n        0.008771725930273533,\n        0.010367235168814659,\n        0.013021648861467838,\n        -0.022709183394908905,\n        -0.013501016423106194,\n        0.007641274947673082,\n        -0.00038568585296161473,\n        -0.008034786209464073,\n        -0.025900201871991158,\n        0.019747115671634674,\n        -0.004428791347891092,\n        -0.036575090140104294,\n        -0.023811014369130135,\n        -0.6250391006469727,\n        -0.020290875807404518,\n        0.010145437903702259,\n        -0.02497008442878723,\n        0.00012777939264196903,\n        0.00421057129278779,\n        0.0012011040234938264,\n        0.004976130090653896,\n        -0.02784629538655281,\n        0.01638438180088997,\n        0.007433787453919649,\n        0.0027205944061279297,\n        -0.0017877935897558928,\n        -0.00937987957149744,\n        0.009995188564062119,\n        -0.0276030320674181,\n        0.0033680994529277086,\n        -0.028604697436094284,\n        -0.0008308455580845475,\n        0.004271387122571468,\n        -0.02097773179411888,\n        0.021092208102345467,\n        -0.013880218379199505,\n        0.019460925832390785,\n        0.006450009066611528,\n        -0.024841299280524254,\n        0.0023324491921812296,\n        -0.016599023714661598,\n        -0.012706839479506016,\n        0.0202336385846138,\n        0.016885215416550636,\n        -0.002307407557964325,\n        0.014388206414878368,\n        0.026028987020254135,\n        0.05720939487218857,\n        -0.020548447966575623,\n        -0.019174735993146896,\n        0.017443284392356873,\n        -0.02585727348923683,\n        0.04976845532655716,\n        -0.01823030784726143,\n        -0.03402800112962723,\n        0.022008016705513,\n        -0.018187379464507103,\n        -0.024340465664863586,\n        -0.011347436346113682,\n        0.006428544409573078,\n        -0.010331462137401104,\n        -0.004181952681392431,\n        -0.03377043083310127,\n        0.005670140963047743,\n        -0.01129735354334116,\n        -0.014982050284743309,\n        0.005448343697935343,\n        0.0026168504264205694,\n        -0.002895885845646262,\n        0.005827545654028654,\n        0.016112500801682472,\n        0.011833959259092808,\n        -0.009365569800138474,\n        -0.014330968260765076,\n        0.025141797959804535,\n        -0.006936531979590654,\n        -0.01107555627822876,\n        -0.02392549067735672,\n        -0.010188366286456585,\n        0.0026329488027840853,\n        0.02369653806090355,\n        0.018917163833975792,\n        -0.03359871357679367,\n        -0.010767901316285133,\n        0.02140701748430729,\n        -0.016999689862132072,\n        0.03505828604102135,\n        0.03751951828598976,\n        -0.004779374226927757,\n        0.016999689862132072,\n        -0.003541602287441492,\n        -0.013651266694068909,\n        0.014602848328649998,\n        -0.0014568864135071635,\n        -0.0033341143280267715,\n        -0.02900536358356476,\n        -0.015010669827461243,\n        -0.0023181396536529064,\n        0.023539133369922638,\n        -0.0006407974869944155,\n        -0.018416332080960274,\n        -0.010245604440569878,\n        0.006528710946440697,\n        0.010295688174664974,\n        0.00106158631388098,\n        0.008685869164764881,\n        -0.0369185209274292,\n        0.01698538102209568,\n        -0.004049589391797781,\n        -0.016370072960853577,\n        0.011211495846509933,\n        0.040581751614809036,\n        -0.010767901316285133,\n        -0.012313327752053738,\n        -0.01521100290119648,\n        -0.004872385878115892,\n        -0.009072225540876389,\n        0.027946460992097855,\n        0.011397520080208778,\n        -0.007555418182164431,\n        0.0031284152064472437,\n        0.041984084993600845,\n        -0.0016277061076834798,\n        -0.005230123642832041,\n        0.006442854180932045,\n        -0.0046040830202400684,\n        0.015926478430628777,\n        0.006163818761706352,\n        -0.03966594487428665,\n        0.02541367895901203,\n        0.012864244170486927,\n        -0.021507184952497482,\n        0.013300684280693531,\n        -0.005541355349123478,\n        0.0011376055190339684,\n        0.03205328807234764,\n        -0.041239991784095764,\n        -0.008185036480426788,\n        0.014202182181179523,\n        0.01565459743142128,\n        -0.0056415218859910965,\n        -0.015125145204365253,\n        -0.010717818513512611,\n        -0.002200086135417223,\n        0.005258742719888687,\n        -0.0039064944721758366,\n        -0.001746653812006116,\n        0.022637635469436646,\n        0.0012896440457552671,\n        -0.015010669827461243,\n        -0.018516497686505318,\n        -0.004174797795712948,\n        -0.02850453183054924,\n        -0.017171405255794525,\n        0.010603342205286026,\n        0.01764361746609211,\n        -0.05088459700345993,\n        -0.021664589643478394,\n        -0.0060815392062067986,\n        -0.0008558871923014522,\n        0.029048291966319084,\n        -0.005448343697935343,\n        -0.0014801393263041973,\n        0.0007897057221271098,\n        0.0025470915716141462,\n        -0.015926478430628777,\n        -0.007326466031372547,\n        -0.009952260181307793,\n        -0.024340465664863586,\n        -0.009480046108365059,\n        -0.03225362300872803,\n        -0.0215644221752882,\n        -0.03377043083310127,\n        0.013536790385842323,\n        0.022036636248230934,\n        -0.027002032846212387,\n        -0.0025739220436662436,\n        -0.015497192740440369,\n        -0.016641952097415924,\n        -0.014638622291386127,\n        0.032969098538160324,\n        -0.04393017664551735,\n        -0.02517041750252247,\n        0.001309319632127881,\n        -0.03027890995144844,\n        0.005408992525190115,\n        0.01913180574774742,\n        -0.013050267472863197,\n        0.014939121901988983,\n        -0.029291553422808647,\n        -0.0237823948264122,\n        0.001782427541911602,\n        -0.015368406660854816,\n        0.021449945867061615,\n        0.025971749797463417,\n        -0.020047614350914955,\n        -0.010145437903702259,\n        0.03102300502359867,\n        0.0065895263105630875,\n        0.002287731971591711,\n        -0.011984209530055523,\n        -0.0021696784533560276,\n        0.02864762581884861,\n        0.00632837787270546,\n        0.01332930289208889,\n        -0.015497192740440369,\n        -0.01735742762684822,\n        0.00012565533688757569,\n        0.017242951318621635,\n        0.019017331302165985,\n        -0.00520508224144578,\n        -0.004668475594371557,\n        0.016570406034588814,\n        0.03943699225783348,\n        0.025871582329273224,\n        0.029592053964734077,\n        -0.024311846122145653,\n        0.013966076076030731,\n        0.010603342205286026,\n        0.013393695466220379,\n        -0.02547091618180275,\n        0.007555418182164431,\n        0.02268056385219097,\n        -0.004968975204974413,\n        -0.023753777146339417,\n        -0.01472447905689478,\n        -0.005938444286584854,\n        -0.003734780475497246,\n        0.015511502511799335,\n        0.007158329244703054,\n        0.020104853436350822,\n        0.008471226319670677,\n        0.01288570836186409,\n        0.023939799517393112,\n        -0.015597359277307987,\n        0.02694479562342167,\n        -0.007186948321759701,\n        0.0058633191511034966,\n        0.003960155416280031,\n        0.023896871134638786,\n        0.03605994954705238,\n        0.02325294353067875,\n        -0.01173379272222519,\n        -0.027660271152853966,\n        -0.013050267472863197,\n        0.017386047169566154,\n        -0.0008769042906351388,\n        0.02790353260934353,\n        0.010918151587247849,\n        0.012077221646904945,\n        -0.00022682799317408353,\n        0.01721433363854885,\n        -0.00018647071556188166,\n        -0.004103250335901976,\n        -0.007498180028051138,\n        0.006042188033461571,\n        -0.0030586563516408205,\n        -0.0005987632903270423,\n        0.00524443294852972,\n        0.05783901363611221,\n        0.025971749797463417,\n        0.01594078727066517,\n        0.022866586223244667,\n        -0.01854511722922325,\n        0.01059618778526783,\n        -0.01602664403617382,\n        0.013171898201107979,\n        -0.008850428275763988,\n        -0.004160488024353981,\n        0.012019983492791653,\n        0.009558748453855515,\n        0.011783876456320286,\n        0.03731918707489967,\n        0.007076049689203501,\n        0.014123479835689068,\n        0.015668906271457672,\n        -0.019618330523371696,\n        -0.011683709919452667,\n        -0.007301424164324999,\n        -0.00010368800576543435,\n        -0.006038610823452473,\n        0.0074910251423716545,\n        -0.011089865118265152,\n        -0.0041461787186563015,\n        -0.029262935742735863,\n        0.00251131784170866,\n        -0.026229320093989372,\n        0.019790044054389,\n        0.011533459648489952,\n        -0.031280577182769775,\n        -0.005874051246792078,\n        0.019375069066882133,\n        0.011068400926887989,\n        -0.036746807396411896,\n        -0.029363101348280907,\n        0.021135136485099792,\n        -0.005237278528511524,\n        -0.009594522416591644,\n        -0.026315176859498024,\n        0.003543390892446041,\n        0.023295871913433075,\n        0.0005799820646643639,\n        -0.0003329195606056601,\n        -0.002271633828058839,\n        0.007083204574882984,\n        0.0007324677426367998,\n        -0.012749767862260342,\n        -0.0026436808984726667,\n        -0.00897205900400877,\n        0.02554246410727501,\n        0.013121815398335457,\n        0.015125145204365253,\n        -0.010732128284871578,\n        0.0085642384365201,\n        0.011347436346113682,\n        -0.03116609901189804,\n        -0.00867871381342411,\n        0.0012923270696774125,\n        0.014001849107444286,\n        -0.011111329309642315,\n        -0.029735149815678596,\n        0.005924134515225887,\n        -0.00845691654831171,\n        -0.004042434971779585,\n        -0.024240300059318542,\n        0.003171343822032213,\n        0.017114166170358658,\n        0.013980384916067123,\n        -0.00010514131281524897,\n        -0.005788194481283426,\n        0.0027814097702503204,\n        0.02436908520758152,\n        0.05082735791802406,\n        -0.003473632037639618,\n        -0.019990377128124237,\n        0.0037777090910822153,\n        0.019661258906126022,\n        0.04289989173412323,\n        0.007068894803524017,\n        -0.01307173166424036,\n        0.012764077633619308,\n        -0.006117312703281641,\n        0.007308579050004482,\n        -0.013243446126580238,\n        -0.032225001603364944,\n        0.03520137816667557,\n        -0.017743784934282303,\n        -0.017171405255794525,\n        -0.013930302113294601,\n        0.011969899758696556,\n        -0.005659408867359161,\n        0.022050945088267326,\n        0.018502188846468925,\n        0.01141182892024517,\n        -0.026043297722935677,\n        0.019904520362615585,\n        0.0005495743826031685,\n        0.010581878013908863,\n        0.017185714095830917,\n        0.015611669048666954,\n        0.0022609014995396137,\n        0.02401134744286537,\n        0.018444949761033058,\n        0.012706839479506016,\n        0.017557760700583458,\n        0.020662924274802208,\n        -0.01513945497572422,\n        0.011719483882188797,\n        0.013937456533312798,\n        -0.0038313695695251226,\n        0.008893356658518314,\n        -0.015024978667497635,\n        0.035029664635658264,\n        0.009014987386763096,\n        0.005741688422858715,\n        0.024183060973882675,\n        -0.008328131400048733,\n        0.00963745079934597,\n        0.02126392349600792,\n        0.0006363257416523993,\n        0.004249922465533018,\n        0.011826804839074612,\n        -0.009336951188743114,\n        0.008614321239292622,\n        0.0014711958356201649,\n        -0.019761424511671066,\n        0.0005222968757152557,\n        -0.003067599842324853,\n        -0.007033121306449175,\n        -0.008993523195385933,\n        0.01941799744963646,\n        0.014223646372556686,\n        0.009844938293099403,\n        -0.03992351517081261,\n        -0.03050786256790161,\n        0.011139948852360249,\n        -0.020205019041895866,\n        -0.05420440062880516,\n        0.0038707207422703505,\n        0.03188157454133034,\n        -0.0058633191511034966,\n        -0.0007539319922216237,\n        -0.0012967988150194287,\n        -0.012578054331243038,\n        -0.020934803411364555,\n        -0.02039104327559471,\n        0.017071237787604332,\n        -0.002604329725727439,\n        -0.019546782597899437,\n        -0.05139973759651184,\n        -0.008471226319670677,\n        0.005305248778313398,\n        0.015268241055309772,\n        0.011612161993980408,\n        0.007455251645296812,\n        -0.018573736771941185,\n        0.0008894251077435911,\n        -0.024397704750299454,\n        -0.027946460992097855,\n        -0.0009417442488484085,\n        -0.027188057079911232,\n        -0.007129710167646408,\n        0.003033614717423916,\n        0.0029048293363302946,\n        -0.001126873423345387,\n        0.0023610680364072323,\n        0.009072225540876389,\n        -0.000999876530840993,\n        -0.0027116509154438972,\n        0.04830888658761978,\n        -0.027874913066625595,\n        0.03256843239068985,\n        0.008027631789445877,\n        -0.00030810150201432407,\n        0.008571392856538296,\n        -0.004507493693381548,\n        -0.008056250400841236,\n        -0.00738370418548584,\n        -0.017085548490285873,\n        -0.011368900537490845,\n        -0.0021821993868798018,\n        0.011390364728868008,\n        0.002493431093171239,\n        -0.006178128067404032,\n        0.014466908760368824,\n        0.019890209659934044,\n        -0.03070819564163685,\n        0.002452291315421462,\n        -0.014445444568991661,\n        -0.004421636462211609,\n        -0.015253931283950806,\n        0.0255567729473114,\n        0.01682797633111477,\n        0.03462899848818779,\n        0.006954418960958719,\n        0.0010589032899588346,\n        0.02612915448844433,\n        -0.003995928913354874,\n        -0.020934803411364555,\n        0.020992042496800423,\n        0.021249612793326378,\n        0.0006023406749591231,\n        -0.01126157958060503,\n        0.009630296379327774,\n        -0.02569986879825592,\n        0.012685375288128853,\n        0.018588045611977577,\n        -0.013687040656805038,\n        0.015253931283950806,\n        0.003396718529984355,\n        -0.06256115436553955,\n        -0.020333804190158844,\n        -0.02531351149082184,\n        -0.02082032896578312,\n        0.01052463985979557,\n        -0.007505334913730621,\n        0.013608338311314583,\n        0.002838647924363613,\n        -0.012506506405770779,\n        -0.007340775337070227,\n        -0.008041941560804844,\n        0.02999272011220455,\n        -0.017700856551527977,\n        -0.012728303670883179,\n        0.018416332080960274,\n        0.00797754805535078,\n        0.008063405752182007,\n        0.002606118330731988,\n        -0.014330968260765076,\n        -0.04839474335312843,\n        -0.018130142241716385,\n        -0.02451217919588089,\n        -0.028590388596057892,\n        -0.008371059782803059,\n        0.012012828141450882,\n        0.024268917739391327,\n        0.024340465664863586,\n        0.027230985462665558,\n        0.007301424164324999,\n        -0.013787207193672657,\n        -0.0034468017984181643,\n        -0.012034292332828045,\n        0.0011769566917791963,\n        -0.0009220686624757946,\n        -0.022537468001246452,\n        -0.007741441484540701,\n        -0.0045861960388720036,\n        0.017443284392356873,\n        -0.02703065238893032,\n        0.0072477636858820915,\n        -0.005319558084011078,\n        0.022923825308680534,\n        0.018430640920996666,\n        -0.005380373448133469,\n        -0.014094861224293709,\n        -0.014853265136480331,\n        -0.03308357298374176,\n        -0.0012726514833047986,\n        0.004782951436936855,\n        -0.009286868385970592,\n        -0.01495343167334795,\n        -0.04247060790657997,\n        0.0023557019885629416,\n        0.004879540763795376,\n        0.04424498602747917,\n        0.024225989356637,\n        0.0096732247620821,\n        0.007254918571561575,\n        -0.0116694001480937,\n        0.01307173166424036,\n        0.01358687411993742,\n        0.042127180844545364,\n        -0.0024809101596474648,\n        -0.021535802632570267,\n        -0.026386724784970284,\n        0.01913180574774742,\n        0.02612915448844433,\n        0.02273780107498169,\n        -0.003305495483800769,\n        0.018001355230808258,\n        0.011061246506869793,\n        -0.017400356009602547,\n        0.0015364829450845718,\n        0.0009873557137325406,\n        -0.0012547646183520555,\n        0.01647023856639862,\n        -0.000512906291987747,\n        -0.02421168051660061,\n        -0.009065071120858192,\n        -0.0007901529315859079,\n        0.02481267973780632,\n        0.018044283613562584,\n        0.009358415380120277,\n        -0.019160425290465355,\n        0.015182383358478546,\n        -0.019761424511671066,\n        0.0015677850460633636,\n        0.02790353260934353,\n        -0.008735951967537403,\n        0.013443779200315475,\n        -0.00251131784170866,\n        0.001997964456677437,\n        0.001210047397762537,\n        0.009387034922838211,\n        0.004815148189663887,\n        -0.0011358169140294194,\n        0.02401134744286537,\n        0.005569974426180124,\n        0.015268241055309772,\n        0.024111513048410416,\n        -0.028089556843042374,\n        -0.019790044054389,\n        0.01778671331703663,\n        0.008278047665953636,\n        -0.0353158563375473,\n        -0.02451217919588089,\n        0.009608832187950611,\n        0.01218454260379076,\n        0.020877566188573837,\n        -0.019003020599484444,\n        -0.016184048727154732,\n        -0.031709861010313034,\n        0.03070819564163685,\n        0.0118911974132061,\n        0.011791030876338482,\n        0.0018709676805883646,\n        -0.01325775496661663,\n        -0.02924862504005432,\n        0.02037673257291317,\n        -0.01823030784726143,\n        0.018945783376693726,\n        0.017085548490285873,\n        -0.006968728266656399,\n        -0.0107822110876441,\n        0.01742897555232048,\n        -0.020061925053596497,\n        0.01355110015720129,\n        0.0010553259635344148,\n        0.004872385878115892,\n        -0.038206376135349274,\n        0.01867390237748623,\n        -0.015296859666705132,\n        -0.013121815398335457,\n        0.0015955097042024136,\n        -0.013765743002295494,\n        -0.007855917327105999,\n        0.03631751984357834,\n        -0.02910553105175495,\n        -0.032081905752420425,\n        0.024555109441280365,\n        -0.020763089880347252,\n        0.018144451081752777,\n        -0.007158329244703054,\n        -0.008650095202028751,\n        0.010646270588040352,\n        0.001974711427465081,\n        -0.0034020845778286457,\n        0.014102015644311905,\n        0.019518163055181503,\n        -0.01074643712490797,\n        0.015769073739647865,\n        0.0014559919945895672,\n        0.025241965427994728,\n        -0.028018008917570114,\n        0.01906025968492031,\n        -0.011211495846509933,\n        -0.021249612793326378,\n        -0.013522481545805931,\n        0.013966076076030731,\n        0.013322148472070694,\n        0.006664651446044445,\n        0.011948435567319393,\n        -0.0023557019885629416,\n        -0.009129463694989681,\n        0.0060815392062067986,\n        -0.012871398590505123,\n        0.01831616461277008,\n        0.026558438315987587,\n        0.0008965798770077527,\n        -0.009909331798553467,\n        -0.007405168376863003,\n        -0.029964100569486618,\n        -0.03757675737142563,\n        0.028690556064248085,\n        -0.029592053964734077,\n        -0.018788378685712814,\n        -0.012392030097544193,\n        0.010023807175457478,\n        0.01126157958060503,\n        0.020290875807404518,\n        0.00012699684884864837,\n        -0.02392549067735672,\n        0.007662739139050245,\n        0.013908837921917439,\n        -0.010581878013908863,\n        -0.011798186227679253,\n        -0.002640103455632925,\n        -0.006428544409573078,\n        -0.0035934741608798504,\n        0.01764361746609211,\n        -0.023725157603621483,\n        0.02355344407260418,\n        -0.013443779200315475,\n        0.01107555627822876,\n        0.00852846447378397,\n        -0.018816998228430748,\n        -0.02103497087955475,\n        -0.030250290408730507,\n        0.0075124893337488174,\n        -0.0353158563375473,\n        -0.01586923934519291,\n        0.013543945737183094,\n        -0.004618392325937748,\n        -0.0035111946053802967,\n        0.022108184173703194,\n        0.004264232236891985,\n        -0.02104927971959114,\n        -0.01403046865016222,\n        0.0020730893593281507,\n        -0.0002857428917195648,\n        0.028776412829756737,\n        -0.026858938857913017,\n        -0.0016599023947492242,\n        -0.0063462648540735245,\n        -0.014838955365121365,\n        -0.020190710201859474,\n        -0.01399469468742609,\n        0.00808486994355917,\n        -0.0024093626998364925,\n        0.022251278162002563,\n        -0.014710170216858387,\n        -0.01476740837097168,\n        -0.016570406034588814,\n        -0.04141170531511307,\n        -0.013937456533312798,\n        -0.01727157086133957,\n        -0.001257447642274201,\n        0.003953000530600548,\n        0.014431134797632694,\n        -0.007440941873937845,\n        0.04613383859395981,\n        -0.02282365784049034,\n        -0.01698538102209568,\n        -0.016584714874625206,\n        0.027273913845419884,\n        -0.01810152269899845,\n        -0.005630789790302515,\n        0.006528710946440697,\n        -0.022981062531471252,\n        -0.033570095896720886,\n        -0.006271140184253454,\n        -0.004160488024353981,\n        0.0036668104585260153,\n        -0.007612655870616436,\n        0.01085375901311636,\n        0.02039104327559471,\n        -0.021907851099967957,\n        0.010538949631154537,\n        0.027273913845419884,\n        0.02637241594493389,\n        -0.00819934532046318,\n        -0.0005688027595169842,\n        -0.024469250813126564,\n        0.00365607813000679,\n        0.011533459648489952,\n        -0.0025327822659164667,\n        -0.0036417688243091106,\n        0.000766452809330076,\n        0.01462431252002716,\n        -0.03471485525369644,\n        -0.011333126574754715,\n        -0.028375746682286263,\n        -0.011054092086851597,\n        0.014130635187029839,\n        -0.01262813713401556,\n        0.009766235947608948,\n        0.006485782563686371,\n        -0.018115831539034843,\n        0.023739466443657875,\n        0.013522481545805931,\n        -0.01787257008254528,\n        -0.016999689862132072,\n        -0.00014868468861095607,\n        -0.014996360056102276,\n        -0.030250290408730507,\n        0.019389377906918526,\n        -0.020791709423065186,\n        0.004300005733966827,\n        0.014381051063537598,\n        -0.013064577244222164,\n        -0.0066503421403467655,\n        -0.007319311145693064,\n        -0.0018495033727958798,\n        -0.0006233577732928097,\n        -0.03505828604102135,\n        0.0025918087922036648,\n        -0.0006188860279507935,\n        -0.008628631010651588,\n        0.014481217600405216,\n        -0.0044752974063158035,\n        -0.01151199545711279,\n        -0.0032214270904660225,\n        -0.010839449241757393,\n        0.003232159186154604,\n        -0.02303830161690712,\n        -0.00944427214562893,\n        0.0014711958356201649,\n        -0.015611669048666954,\n        0.004267809446901083,\n        -0.015368406660854816,\n        0.012198852375149727,\n        0.023582061752676964,\n        0.018859926611185074,\n        0.23112711310386658,\n        -0.016112500801682472,\n        -0.020992042496800423,\n        0.0370902344584465,\n        -0.015182383358478546,\n        0.015697525814175606,\n        0.023968419060111046,\n        -0.012399185448884964,\n        -0.017114166170358658,\n        0.025571083649992943,\n        0.006990192458033562,\n        -0.016727810725569725,\n        -0.04576179385185242,\n        -0.0019335716497153044,\n        0.008435452356934547,\n        -0.011876888573169708,\n        -0.05228692665696144,\n        -0.02784629538655281,\n        -0.028604697436094284,\n        0.012127304449677467,\n        0.03434281051158905,\n        0.000821902125608176,\n        -0.013629802502691746,\n        -0.022981062531471252,\n        0.030364766716957092,\n        -0.03136643394827843,\n        0.014080551452934742,\n        0.002598963677883148,\n        0.023653609678149223,\n        0.013529635965824127,\n        -0.03345562145113945,\n        0.009787701070308685,\n        0.007240608800202608,\n        0.003155245678499341,\n        -0.02836143597960472,\n        -0.009286868385970592,\n        -0.007215567398816347,\n        -0.006221056915819645,\n        0.02607191540300846,\n        0.010073890909552574,\n        -0.008514154702425003,\n        -0.01240633986890316,\n        -0.02282365784049034,\n        0.00022235627693589777,\n        -0.015039288438856602,\n        0.012535125017166138,\n        0.01513945497572422,\n        -0.017386047169566154,\n        0.0005084345466457307,\n        0.02044828049838543,\n        -0.030479243025183678,\n        0.004561154171824455,\n        0.02547091618180275,\n        0.012091530486941338,\n        -0.009437117725610733,\n        -0.005412569735199213,\n        -0.0036435574293136597,\n        0.0004368870286270976,\n        0.016971072182059288,\n        0.04144032299518585,\n        -0.015983715653419495,\n        0.009866402484476566,\n        -0.009365569800138474,\n        0.03056509979069233,\n        -0.007991857826709747,\n        -0.006707579828798771,\n        -0.026243630796670914,\n        0.010825139470398426,\n        0.014295194298028946,\n        -0.023396039381623268,\n        0.002786775818094611,\n        -0.027488557621836662,\n        0.011540614999830723,\n        0.001717140432447195,\n        -0.047650646418333054,\n        -0.008943439461290836,\n        0.016284216195344925,\n        0.013408005237579346,\n        -0.0011295564472675323,\n        0.02607191540300846,\n        -0.010925306007266045,\n        -0.025571083649992943,\n        0.0011438659857958555,\n        -0.0042535001412034035,\n        -0.026830319315195084,\n        -0.026830319315195084,\n        -0.008793190121650696,\n        0.011404674500226974,\n        -0.020190710201859474,\n        0.008578547276556492,\n        -0.004507493693381548,\n        -0.007741441484540701,\n        -0.02162165939807892,\n        -0.007634120527654886,\n        -0.0007812094991095364,\n        -0.002865478163585067,\n        0.016742119565606117,\n        0.02341034822165966,\n        0.0012744402047246695,\n        0.004153333604335785,\n        -0.02001899667084217,\n        0.014781717211008072,\n        0.007083204574882984,\n        0.0045003388077020645,\n        -0.004754332825541496,\n        -0.01705692894756794,\n        -0.016641952097415924,\n        -0.0029119839891791344,\n        0.013100351206958294,\n        -0.014237956143915653,\n        -0.008871892467141151,\n        -0.03565928339958191,\n        0.006693270523101091,\n        -0.002679454628378153,\n        0.006636032368987799,\n        -0.0012896440457552671,\n        0.023954110220074654,\n        0.00013918228796683252,\n        0.018416332080960274,\n        -0.0251131784170866,\n        0.03116609901189804,\n        -0.01898871175944805,\n        0.010367235168814659,\n        0.01764361746609211,\n        0.014102015644311905,\n        -0.014638622291386127,\n        -0.04687793552875519,\n        0.005036945454776287,\n        -0.014073397032916546,\n        -0.029935482889413834,\n        0.0022930980194360018,\n        -0.026601368561387062,\n        0.019589710980653763,\n        -0.015597359277307987,\n        -0.02000468596816063,\n        -0.012349101714789867,\n        -0.010896687395870686,\n        -0.017672237008810043,\n        -0.008893356658518314,\n        0.0008308455580845475,\n        0.011504841037094593,\n        -0.020061925053596497,\n        0.01742897555232048,\n        -0.00050306849880144,\n        0.025013012811541557,\n        -0.011676555499434471,\n        0.029592053964734077,\n        0.019532471895217896,\n        0.002396841999143362,\n        -0.024912845343351364,\n        -0.010610497556626797,\n        -0.004103250335901976,\n        0.0032214270904660225,\n        -0.011791030876338482,\n        0.01380867138504982,\n        -0.004296428523957729,\n        -0.012742613442242146,\n        -0.033054955303668976,\n        0.017457595095038414,\n        -0.01625559665262699,\n        -9.547122317599133e-05,\n        -0.0012243569362908602,\n        0.011934125795960426,\n        0.00435724388808012,\n        -0.009165237657725811,\n        -0.00439301785081625,\n        -0.18545116484165192,\n        0.010739282704889774,\n        0.028776412829756737,\n        -0.030908528715372086,\n        0.02296675369143486,\n        0.004575463943183422,\n        0.024941464886069298,\n        0.014123479835689068,\n        -0.026028987020254135,\n        0.008879046887159348,\n        0.013157588429749012,\n        0.0014837166527286172,\n        -0.029119839891791344,\n        -0.02044828049838543,\n        0.008650095202028751,\n        -0.012241780757904053,\n        -0.014352432452142239,\n        0.009086535312235355,\n        0.009551594033837318,\n        0.020791709423065186,\n        0.025356439873576164,\n        -0.009573058225214481,\n        0.01647023856639862,\n        0.011461912654340267,\n        0.0054233018308877945,\n        0.011490531265735626,\n        -0.02465527504682541,\n        0.0028923084028065205,\n        -0.006718311924487352,\n        -0.029706530272960663,\n        0.001210047397762537,\n        -0.01714278571307659,\n        0.011626471765339375,\n        -0.006113735493272543,\n        -0.016427310183644295,\n        0.005519891157746315,\n        -0.023496204987168312,\n        -0.020920494571328163,\n        -0.004518225789070129,\n        0.015683216974139214,\n        0.02650120109319687,\n        0.024855608120560646,\n        0.023095538839697838,\n        0.017228642478585243,\n        0.0033734655007719994,\n        0.00575957540422678,\n        0.014924812130630016,\n        -0.04584765061736107,\n        0.0050691417418420315,\n        -0.02725960500538349,\n        0.025456607341766357,\n        -0.014652932062745094,\n        0.010939615778625011,\n        -0.021321160718798637,\n        0.01734311878681183,\n        -0.0031838645227253437,\n        -0.008170726709067822,\n        0.017829641699790955,\n        -0.0174862127751112,\n        0.002733115339651704,\n        -0.015439954586327076,\n        -0.007151174359023571,\n        0.03640337660908699,\n        0.007419477682560682,\n        -0.007290692068636417,\n        -0.006814901251345873,\n        0.009308332577347755,\n        0.006900758016854525,\n        -0.009945104829967022,\n        0.023653609678149223,\n        -0.010975389741361141,\n        -0.013894528150558472,\n        -0.01103978231549263,\n        -0.018645282834768295,\n        0.015511502511799335,\n        0.029119839891791344,\n        -0.021807683631777763,\n        0.010882377624511719,\n        -0.03794880583882332,\n        0.007948929443955421,\n        -0.011919816955924034,\n        0.03391352295875549,\n        0.0010589032899588346,\n        0.010989698581397533,\n        -0.01631283387541771,\n        -0.0030425582081079483,\n        0.01384444534778595,\n        0.004682784900069237,\n        0.0002709862310439348,\n        -0.012671065516769886,\n        0.04341503605246544,\n        -0.04447393864393234,\n        -0.006489359773695469,\n        -0.005112070124596357,\n        0.050541166216135025,\n        0.030107196420431137,\n        0.004049589391797781,\n        -0.012735458090901375,\n        0.029563434422016144,\n        0.0010910996934399009,\n        -0.00897205900400877,\n        -0.004167642910033464,\n        -0.018416332080960274,\n        -0.0005853481125086546,\n        0.011089865118265152,\n        0.0032428912818431854,\n        -0.001219885190948844,\n        -0.01041016448289156,\n        0.010538949631154537,\n        -0.0101168192923069,\n        -0.022194040939211845,\n        -0.006446431390941143,\n        0.0014649354852735996,\n        -0.006138777360320091,\n        -0.001328995218500495,\n        0.024540798738598824,\n        -0.0022269166074693203,\n        0.001330783823505044,\n        -0.011161413043737411,\n        0.018344784155488014,\n        0.03966594487428665,\n        -0.010789365507662296,\n        -0.024612346664071083,\n        -0.0002432615583529696,\n        -0.009551594033837318,\n        -0.0237823948264122,\n        -0.1160786971449852,\n        -0.03225362300872803,\n        0.02355344407260418,\n        0.027302533388137817,\n        -0.011848269030451775,\n        -0.00020804676751140505,\n        0.005820390768349171,\n        0.004679207690060139,\n        -0.01771516539156437,\n        0.0326542891561985,\n        -0.0025345708709210157,\n        -0.015239621512591839,\n        -0.007405168376863003,\n        -0.013322148472070694,\n        0.004310737829655409,\n        -0.011454758234322071,\n        0.012835624627768993,\n        -0.020949114114046097,\n        0.018516497686505318,\n        0.020491208881139755,\n        0.007165484130382538,\n        -0.024984393268823624,\n        0.03125195577740669,\n        0.004403749946504831,\n        -0.03952284902334213,\n        -0.003763399552553892,\n        -0.009243939071893692,\n        0.012349101714789867,\n        0.01155492477118969,\n        0.02266625314950943,\n        0.011977055110037327,\n        -0.011354591697454453,\n        -0.001376395346596837,\n        -0.0101168192923069,\n        0.000986461411230266,\n        0.009522974491119385,\n        -0.028933817520737648,\n        -0.01780102215707302,\n        -0.0014863996766507626,\n        -0.018845615908503532,\n        -0.004353666678071022,\n        0.005934866610914469,\n        0.008449762128293514,\n        0.006163818761706352,\n        0.017314499244093895,\n        -0.007519644219428301,\n        -0.01677073910832405,\n        0.02444063313305378,\n        0.007505334913730621,\n        -0.016885215416550636,\n        -0.033112190663814545,\n        -0.004346511792391539,\n        -0.025141797959804535,\n        -0.011068400926887989,\n        0.021449945867061615,\n        -0.028161102905869484,\n        0.008356750011444092,\n        -0.002568555995821953,\n        0.01624128594994545,\n        -0.0020230060908943415,\n        0.010066735558211803,\n        0.016069572418928146,\n        -0.010846603661775589,\n        0.03514414280653,\n        0.04905297979712486,\n        8.507446909788996e-05,\n        0.008592857047915459,\n        -0.01269968505948782,\n        0.004697094671428204,\n        -0.005366064142435789,\n        -0.024855608120560646,\n        0.016298525035381317,\n        -0.013121815398335457,\n        0.017886878922581673,\n        -0.02939172089099884,\n        0.00044493612949736416,\n        -0.007948929443955421,\n        -0.018430640920996666,\n        -0.018187379464507103,\n        0.011082710698246956,\n        -0.040724847465753555,\n        -0.018058594316244125,\n        -0.011097020469605923,\n        0.018187379464507103,\n        -0.00020335146109573543,\n        0.0121129946783185,\n        -0.018487878143787384,\n        -0.027803365141153336,\n        0.0170426182448864,\n        -0.03162400424480438,\n        -0.014495527371764183,\n        0.02502732165157795,\n        0.01594078727066517,\n        -0.0180156659334898,\n        0.00055851781507954,\n        0.027445629239082336,\n        -0.013193362392485142,\n        -0.0016867327503859997,\n        0.015082216821610928,\n        0.010352926328778267,\n        0.0023735889699310064,\n        -0.010309997946023941,\n        -0.04868093132972717,\n        0.019303521141409874,\n        0.02103497087955475,\n        0.008549928665161133,\n        0.009136618115007877,\n        0.013508171774446964,\n        0.025356439873576164,\n        0.0027957193087786436,\n        -0.012807006016373634,\n        -0.01081082969903946,\n        -0.022766420617699623,\n        0.004353666678071022,\n        -0.032453954219818115,\n        -0.01532547827810049,\n        -0.004686362575739622,\n        -0.020419662818312645,\n        0.029363101348280907,\n        -0.0007977548521012068,\n        0.018731139600276947,\n        0.0307940524071455,\n        0.012620982713997364,\n        -0.004350089002400637,\n        -0.011268734000623226,\n        0.00926540419459343,\n        0.01303595770150423,\n        -0.010066735558211803,\n        -0.002103497041389346,\n        0.017071237787604332,\n        -0.02090618573129177,\n        -0.023882562294602394,\n        0.0026096957735717297,\n        -0.00808486994355917,\n        0.012513660825788975,\n        0.026114843785762787,\n        0.010767901316285133,\n        -0.008471226319670677,\n        0.010646270588040352,\n        0.012499351985752583,\n        0.016656262800097466,\n        -0.024712512269616127,\n        0.009186701849102974,\n        0.0025220499373972416,\n        -0.004568309057503939,\n        -0.028032317757606506,\n        0.008857582695782185,\n        -0.005602170713245869,\n        -0.006428544409573078,\n        0.008507000282406807,\n        0.013136124238371849,\n        0.001126873423345387,\n        0.03153814747929573,\n        0.009866402484476566,\n        0.026300868019461632,\n        -0.0024826989974826574,\n        -0.0031141056679189205,\n        -0.021020662039518356,\n        -0.01558304950594902,\n        -0.01303595770150423,\n        0.013393695466220379,\n        -0.04830888658761978,\n        0.017099857330322266,\n        0.008406833745539188,\n        0.0025578239001333714,\n        -0.019890209659934044,\n        0.010567568242549896,\n        0.006986615248024464,\n        -0.0067827049642801285,\n        0.02371084876358509,\n        -0.009887867607176304,\n        -0.024168752133846283,\n        -0.010088199749588966,\n        0.023911181837320328,\n        0.00963745079934597,\n        0.008557083085179329,\n        0.02326725237071514,\n        0.0085213091224432,\n        -0.003584530670195818,\n        0.00709035899490118,\n        -0.005688027944415808,\n        0.030536482110619545,\n        0.021106518805027008,\n        -0.022766420617699623,\n        -0.01867390237748623,\n        0.014531301334500313,\n        0.033999379724264145,\n        -0.004056744277477264,\n        -0.011161413043737411,\n        0.02348189614713192,\n        0.0018548694206401706,\n        0.00039619437302462757,\n        -0.0023413924500346184,\n        -0.0063212234526872635,\n        -0.017157094553112984,\n        -0.02140701748430729,\n        0.010882377624511719,\n        0.03926527872681618,\n        -0.016484549269080162,\n        0.003838524455204606,\n        0.03070819564163685,\n        0.016413001343607903,\n        0.004271387122571468,\n        0.007158329244703054,\n        -0.011719483882188797,\n        -0.010581878013908863,\n        -0.016713500022888184,\n        0.00893628504127264,\n        -0.008907666429877281,\n        -0.023539133369922638,\n        6.282766844378784e-05,\n        0.02051982842385769,\n        0.018516497686505318,\n        0.010174057446420193,\n        0.01830185577273369,\n        0.004128291737288237,\n        -0.008735951967537403,\n        0.025456607341766357,\n        -0.014867574907839298,\n        -0.011268734000623226,\n        -0.014144944958388805,\n        0.005451920907944441,\n        0.0064786276780068874,\n        0.005934866610914469,\n        0.02266625314950943,\n        -0.0015812001656740904,\n        0.028075246140360832,\n        0.006940109189599752,\n        0.0070045022293925285,\n        -0.03897908702492714,\n        0.009036451578140259,\n        0.003364522010087967,\n        -0.007748596370220184,\n        -0.005713069345802069,\n        -0.017958426848053932,\n        -0.019003020599484444,\n        -0.007405168376863003,\n        -0.016942452639341354,\n        0.004511070903390646,\n        0.020290875807404518,\n        -0.01986159197986126,\n        0.07978979498147964,\n        0.005813235882669687,\n        0.0030389807652682066,\n        -0.02999272011220455,\n        0.018044283613562584,\n        0.013207672163844109,\n        0.005376796238124371,\n        -0.006489359773695469,\n        -0.02312415838241577,\n        -0.039093565195798874,\n        -0.0009345894795842469,\n        0.004049589391797781,\n        0.01222031656652689,\n        -0.016198357567191124,\n        0.0012243569362908602,\n        0.014330968260765076,\n        0.004997594282031059,\n        0.022194040939211845,\n        0.005019058473408222,\n        -0.014137789607048035,\n        0.022408682852983475,\n        -0.01625559665262699,\n        0.010467401705682278,\n        0.012256089597940445,\n        -0.012062911875545979,\n        -0.005809658672660589,\n        0.02296675369143486,\n        0.008635785430669785,\n        -0.004997594282031059,\n        -0.04387293756008148,\n        -0.004110404755920172,\n        0.0002998288255184889,\n        0.00849269051104784,\n        -0.014395360834896564,\n        -0.011826804839074612,\n        -0.016284216195344925,\n        -0.009873557835817337,\n        0.006070807110518217,\n        0.029033983126282692,\n        0.040867943316698074,\n        -0.005788194481283426,\n        0.023868251591920853,\n        -0.03139505162835121,\n        -0.013214826583862305,\n        -0.020362423732876778,\n        -0.0016053474973887205,\n        0.0029942637775093317,\n        0.001788688008673489,\n        -0.049138836562633514\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"ada\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 8,\n    \"total_tokens\": 8\n  }\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-49348/49349","correlationid":"a1e5ddfa-f94a-461c-9245-f771d00d0c57","xrequestid":"a1e5ddfa-f94a-461c-9245-f771d00d0c57","modelversion":"default"}
{"specversion":"1.0","id":"d91dbf64-d236-4147-b401-4818342ac060","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"vector_lookup","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0x2b2a902ea98f7623","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x4d0bbb09563f095d","start_time":"2024-02-05T15:56:05.080216Z","end_time":"2024-02-05T15:56:05.668144Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"search","node_name":"vector_lookup","flow_id":"","root_run_id":"","line_run_id":"d13da436-d3ad-4905-9aae-2dfc0cb2b538","inputs":"{\n  \"mlindex_content\": \"embeddings:\\n  api_base: https:\/\/promptflow-ci-weu.openai.azure.com\/\\n  api_type: azure\\n  api_version: 2023-03-15-preview\\n  batch_size: '16'\\n  connection:\\n    id: \\n      \/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourceGroups\/promptflow\/providers\/Microsoft.MachineLearningServices\/workspaces\/pf-xp\/connections\/azure_open_ai_connection\\n  connection_type: workspace_connection\\n  deployment: text-embedding-ada-003\\n  dimension: 1536\\n  file_format_version: '2'\\n  kind: open_ai\\n  model: text-embedding-ada-002\\n  schema_version: '2'\\nindex:\\n  engine: langchain.vectorstores.FAISS\\n  kind: faiss\\n  method: FlatL2\\n  path: \\n    azureml:\/\/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourcegroups\/promptflow\/workspaces\/pf-xp\/datastores\/workspaceblobstore\/paths\/azureml\/f7ee0381-06fc-4566-820f-86ba174db987\/index\/\\nself:\\n  path: \\n    azureml:\/\/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourcegroups\/promptflow\/workspaces\/pf-xp\/datastores\/workspaceblobstore\/paths\/azureml\/f7ee0381-06fc-4566-820f-86ba174db987\/index\/\\n  asset_id: \\n    azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/serene-honey-h9lrkyczzk\/versions\/1\\n\",\n  \"queries\": [\n    \"What are some ways to improve the efficiency of Attention calculation?\",\n    \"how to improve Attension calculation efficiency?\"\n  ],\n  \"query_type\": \"Vector\",\n  \"top_k\": 3\n}","output":"[\n  [\n    {\n      \"text\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf3\",\n        \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n        \"mtime\": null,\n        \"page_number\": 3,\n        \"stats\": {\n          \"tiktokens\": 551,\n          \"chars\": 2502,\n          \"lines\": 35\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.3891408443450928\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf12\",\n        \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n        \"mtime\": null,\n        \"page_number\": 12,\n        \"stats\": {\n          \"tiktokens\": 254,\n          \"chars\": 833,\n          \"lines\": 73\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.39398106932640076\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf0\",\n        \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n        \"mtime\": null,\n        \"page_number\": 0,\n        \"stats\": {\n          \"tiktokens\": 668,\n          \"chars\": 2874,\n          \"lines\": 50\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.42212480306625366\n    }\n  ],\n  [\n    {\n      \"text\": \"Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf3\",\n        \"chunk_hash\": \"5e70d3f796a778829940a4e9a74ec45d4b77ab22a6d4528922932a29f8efe7eb\",\n        \"mtime\": null,\n        \"page_number\": 3,\n        \"stats\": {\n          \"tiktokens\": 551,\n          \"chars\": 2502,\n          \"lines\": 35\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.403403103351593\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf12\",\n        \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n        \"mtime\": null,\n        \"page_number\": 12,\n        \"stats\": {\n          \"tiktokens\": 254,\n          \"chars\": 833,\n          \"lines\": 73\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.42596590518951416\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf0\",\n        \"chunk_hash\": \"c75669a2c4ea0f4ddb0e7b2c4e85534297615151bb8e8dfef1d05b62aa134744\",\n        \"mtime\": null,\n        \"page_number\": 0,\n        \"stats\": {\n          \"tiktokens\": 668,\n          \"chars\": 2874,\n          \"lines\": 50\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.4357953667640686\n    }\n  ]\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-19801/19802","correlationid":"ca5deab0-720b-49a1-9d81-124a7a5e9d1d","xrequestid":"ca5deab0-720b-49a1-9d81-124a7a5e9d1d","modelversion":"default"}
{"specversion":"1.0","id":"74f0590a-6569-457f-a786-ee6e50065b47","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-05T15:56:09Z","data":[{"name":"answer_the_question_with_context","context":{"request_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","trace_id":"0xeb680f9487455dd80bbb9ca58dbc11a7","span_id":"0xbb234096db5a96c4","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x4d0bbb09563f095d","start_time":"2024-02-05T15:56:05.774437Z","end_time":"2024-02-05T15:56:08.083729Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"AzureOpenAI.chat","node_name":"answer_the_question_with_context","flow_id":"","root_run_id":"","line_run_id":"d13da436-d3ad-4905-9aae-2dfc0cb2b538","inputs":"{\n  \"prompt\": \"{{prompt_text}}\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"prompt_text\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfScaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \\\"Scaled Dot-Product Attention\\\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by\\u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n\\u221adk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1\\u221adk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1\\u221adk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\u00b7k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\n user: \\nhow to calculate Attention? \\nassistant: \\nTo calculate attention, we use a compatibility function of the query with the corresponding key, and then compute the dot products of the query with all keys, divide each by the square root of the dimension of the keys, and apply a softmax function to obtain the weights on the values. This is known as Scaled Dot-Product Attention. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf) \\nuser: how to improve Attension calculation efficiency? \\nassistant:\",\n  \"stream\": false\n}","output":"\"One way to improve the efficiency of attention calculation is to use dot-product attention, which is faster and more space-efficient than additive attention. Additionally, multi-head attention can be used to linearly project the queries, keys, and values multiple times with different, learned linear projections to different dimensions, and then perform the attention function in parallel on each of these projected versions. This can improve the model's performance while also being more parallelizable and requiring less time to train. (Source: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf)\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-9897/9898","correlationid":"da7ba383-1a04-4ae5-a298-c9b61482efb1","xrequestid":"da7ba383-1a04-4ae5-a298-c9b61482efb1","agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame"}
