{"specversion":"1.0","id":"35c46487-eed8-45d5-b2a8-a242f7806003","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-07T04:22:03Z","data":[{"name":"prepare_question_array","context":{"trace_id":"0x0ef89cf7e7aa594e6c0e2281bb5c44b9","span_id":"0xc20e38ce957ebf22","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xacab7ff109688c71","start_time":"2024-02-07T04:22:01.666584Z","end_time":"2024-02-07T04:22:01.667259Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"my_python_tool","node_name":"prepare_question_array","line_run_id":"f232dcce-ac1f-4b3e-a6a1-ab45ba364aaf","inputs":"{\n  \"modified_question\": \"How are you?\",\n  \"original_question\": \"how are you\"\n}","output":"[\n  \"How are you?\",\n  \"how are you\"\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-748/749","correlationid":"888b6b2c-cc5b-447c-88af-ddb70e9a7030","xrequestid":"888b6b2c-cc5b-447c-88af-ddb70e9a7030","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"5ff0b6a6-1924-41bd-9360-43683c070e5b","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-07T04:22:03Z","data":[{"name":"modify_query_with_history","context":{"trace_id":"0x0ef89cf7e7aa594e6c0e2281bb5c44b9","span_id":"0xb7d25d99f02b064e","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xacab7ff109688c71","start_time":"2024-02-07T04:22:01.051811Z","end_time":"2024-02-07T04:22:01.662827Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"AzureOpenAI.chat","node_name":"modify_query_with_history","line_run_id":"f232dcce-ac1f-4b3e-a6a1-ab45ba364aaf","inputs":"{\n  \"prompt\": \"system: \\nGiven the following conversation history and the users next question,rephrase the question to be a stand alone question.\\nIf the conversation is irrelevant or empty, just restate the original question.\\nDo not add more details than necessary to the question.\\nconversation:\\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\n\\nuser:\\nFollow up Input: {{question}} \\nStandalone Question:\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"chat_history\": [],\n  \"question\": \"how are you\"\n}","output":"\"How are you?\"","__computed__.cumulative_token_count.completion":4,"__computed__.cumulative_token_count.prompt":77,"__computed__.cumulative_token_count.total":81},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"correlationid":"9ccd3eb7-9709-43e9-85af-c78d14a9602e","xrequestid":"9ccd3eb7-9709-43e9-85af-c78d14a9602e","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-1517/1518"}
{"specversion":"1.0","id":"6e92124c-45fc-4c63-9859-6ce4e2aa6c56","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-07T04:22:08Z","data":[{"name":"Embeddings.create","context":{"trace_id":"0x0ef89cf7e7aa594e6c0e2281bb5c44b9","span_id":"0xfcd05a71bc153405","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x1f3902a006497f4b","start_time":"2024-02-07T04:22:04.804887Z","end_time":"2024-02-07T04:22:05.488343Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Embedding","function":"Embeddings.create","node_name":"vector_lookup","line_run_id":"f232dcce-ac1f-4b3e-a6a1-ab45ba364aaf","inputs":"{\n  \"input\": [\n    [\n      5269,\n      527,\n      499\n    ]\n  ],\n  \"model\": \"text-embedding-ada-002\"\n}","embedding.model":"ada","embedding.embeddings":"[\n  {\n    \"embedding.vector\": \"\u003c1536 dimensional vector\u003e\",\n    \"embedding.text\": [\n      5269,\n      527,\n      499\n    ]\n  }\n]","output":"{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.014047916978597641,\n        -0.006731834728270769,\n        -0.011691450141370296,\n        -0.04123492166399956,\n        -0.01821555197238922,\n        0.01824151910841465,\n        -0.028303567320108414,\n        -0.02138347551226616,\n        -0.029601896181702614,\n        -0.014645148068666458,\n        0.026135358959436417,\n        0.01039961352944374,\n        -0.013840184547007084,\n        -0.00022254166833590716,\n        -0.012139374390244484,\n        -0.006030736956745386,\n        0.03432781249284744,\n        -0.012093932367861271,\n        0.014749014750123024,\n        -0.017034074291586876,\n        0.006020999513566494,\n        0.007504340261220932,\n        -0.0002493197098374367,\n        -0.01828046888113022,\n        -0.01795588620007038,\n        -0.00599503330886364,\n        0.00923760887235403,\n        -0.01096438616514206,\n        0.003709974465891719,\n        -0.03567807376384735,\n        -0.0021795693319290876,\n        -0.0048849619925022125,\n        -0.008432645350694656,\n        0.006160569842904806,\n        0.0092765586450696,\n        -0.019448965787887573,\n        -0.005683434195816517,\n        -0.0027540798764675856,\n        0.024071015417575836,\n        0.005316656082868576,\n        0.017865004017949104,\n        -0.00858844444155693,\n        -0.005323147866874933,\n        -0.006218994967639446,\n        -0.0427929162979126,\n        0.005488684866577387,\n        -0.00949727464467287,\n        -0.012165340594947338,\n        -0.020955026149749756,\n        0.04398737847805023,\n        0.0179299209266901,\n        0.013775268569588661,\n        -0.024343663826584816,\n        -0.023837316781282425,\n        -0.0017251543467864394,\n        0.0009145103394985199,\n        -0.0002261932095279917,\n        0.012035507708787918,\n        -0.015086580067873001,\n        -0.021565241739153862,\n        -0.0019474931759759784,\n        0.006507873069494963,\n        -0.012613263912498951,\n        0.020682377740740776,\n        0.015982426702976227,\n        0.0074134571477770805,\n        0.023850299417972565,\n        -0.003752170130610466,\n        -0.0035509292501956224,\n        0.005073219537734985,\n        0.005887920968234539,\n        0.007679614704102278,\n        -0.009283050894737244,\n        0.001880953786894679,\n        0.01643684133887291,\n        -0.023330967873334885,\n        -0.01108772773295641,\n        0.016592642292380333,\n        -0.0024944141041487455,\n        0.009412883780896664,\n        0.02630414068698883,\n        -0.047311101108789444,\n        -0.009717990644276142,\n        0.023019369691610336,\n        0.012671688571572304,\n        -0.00010275054955855012,\n        -0.03458748012781143,\n        0.020916076377034187,\n        -0.01278853788971901,\n        -0.00515761086717248,\n        0.001456562546081841,\n        0.019916363060474396,\n        -0.006816226057708263,\n        0.01772218756377697,\n        -0.017774119973182678,\n        0.0010110734729096293,\n        0.0118407579138875,\n        0.01694319024682045,\n        1.227960365213221e-05,\n        -0.014489348977804184,\n        -0.005073219537734985,\n        0.009322000667452812,\n        -0.012691163457930088,\n        -0.00927006732672453,\n        -0.007147300057113171,\n        0.0031890200916677713,\n        -0.016774408519268036,\n        -0.006089162081480026,\n        0.014554265886545181,\n        -0.021993689239025116,\n        0.003169545205309987,\n        0.01689125783741474,\n        -0.0036904995795339346,\n        -0.02318815141916275,\n        0.002741096541285515,\n        -0.0037716450169682503,\n        -0.01485288143157959,\n        -0.007004483602941036,\n        -0.01511254720389843,\n        -0.006718851160258055,\n        0.013411736115813255,\n        0.0005603100289590657,\n        0.006277419626712799,\n        -0.017644288018345833,\n        0.021461375057697296,\n        0.01447636540979147,\n        -0.021318558603525162,\n        -0.007978230714797974,\n        0.014151783660054207,\n        6.481500895461068e-05,\n        0.021227674558758736,\n        0.009873790666460991,\n        0.0014768489636480808,\n        0.0005668016383424401,\n        -0.0161382257938385,\n        0.01848820224404335,\n        -0.01696915738284588,\n        0.013372786343097687,\n        -0.005173840094357729,\n        -0.01776113733649254,\n        -0.002426251769065857,\n        0.02499282918870449,\n        0.01772218756377697,\n        -0.008276846259832382,\n        -0.012106915935873985,\n        0.01343770232051611,\n        0.030043328180909157,\n        -0.004151406232267618,\n        -0.0074134571477770805,\n        -0.015956461429595947,\n        0.01171092502772808,\n        -0.0200981292873621,\n        0.020266912877559662,\n        0.021266624331474304,\n        0.007277132943272591,\n        0.01925421506166458,\n        -0.004463005345314741,\n        0.03832666575908661,\n        0.004635033663362265,\n        -0.003937182016670704,\n        -0.030147194862365723,\n        -0.012093932367861271,\n        0.028251633048057556,\n        -0.0040962272323668,\n        -0.00952973309904337,\n        0.0338863804936409,\n        0.02786213532090187,\n        0.007828922010958195,\n        0.003557420801371336,\n        0.006835700944066048,\n        0.0105748875066638,\n        0.0042974683456122875,\n        -0.046739835292100906,\n        0.011126677505671978,\n        -0.012892404571175575,\n        -0.012593789026141167,\n        0.012437989935278893,\n        0.004044293891638517,\n        -0.02682347223162651,\n        -0.01741058938205242,\n        -0.016826340928673744,\n        0.01511254720389843,\n        0.032276451587677,\n        0.0385863296687603,\n        0.0009250592556782067,\n        0.001389211742207408,\n        0.008653361350297928,\n        0.005569830536842346,\n        0.03204275295138359,\n        -0.0031565618701279163,\n        0.02713507041335106,\n        0.013853168115019798,\n        0.006644197273999453,\n        -0.019228249788284302,\n        -0.6738845705986023,\n        -0.025447243824601173,\n        0.023499751463532448,\n        -0.0024116456042975187,\n        -0.009471308439970016,\n        0.015618895180523396,\n        0.01825450174510479,\n        0.005225773435086012,\n        -0.02004619687795639,\n        0.009088301099836826,\n        -0.005160856992006302,\n        0.019695647060871124,\n        0.004303959663957357,\n        -0.0042487806640565395,\n        -0.002075703116133809,\n        -0.01799483597278595,\n        0.0027118842117488384,\n        -0.0017219085711985826,\n        -0.027706336230039597,\n        0.001561240293085575,\n        -0.01800782047212124,\n        -6.811154162278399e-05,\n        -0.02292848564684391,\n        -0.0063683027401566505,\n        0.013983001001179218,\n        -0.004966107662767172,\n        0.015463095158338547,\n        0.011360377073287964,\n        -0.03458748012781143,\n        0.025200560688972473,\n        -0.015281329862773418,\n        0.015528012067079544,\n        0.017826054245233536,\n        -0.024447530508041382,\n        0.049881789833307266,\n        -0.014255649410188198,\n        -0.008796177804470062,\n        0.024862995371222496,\n        0.000686897081322968,\n        0.04923262819647789,\n        1.35918789965217e-05,\n        -0.010029589757323265,\n        -0.0022120275534689426,\n        -0.01525536272674799,\n        0.012976796366274357,\n        -0.010802095755934715,\n        0.008958468213677406,\n        -0.01368438545614481,\n        0.021500324830412865,\n        -0.005040761549025774,\n        0.02111082524061203,\n        0.023512734100222588,\n        0.000984295504167676,\n        -0.005832741968333721,\n        0.0025869200471788645,\n        0.0030835308134555817,\n        0.008815652690827847,\n        -0.003239330369979143,\n        0.0075303069315850735,\n        0.014255649410188198,\n        0.0030543184839189053,\n        0.012340614572167397,\n        -0.007484865374863148,\n        3.578011455829255e-05,\n        -0.013489635661244392,\n        0.003924198914319277,\n        -0.0011271116090938449,\n        -0.004794078879058361,\n        0.02211053855717182,\n        -0.0077250562608242035,\n        0.011542142368853092,\n        0.014138800092041492,\n        -0.011944624595344067,\n        0.014619181863963604,\n        0.019942330196499825,\n        0.04354594647884369,\n        0.007536798715591431,\n        -0.011529159732162952,\n        0.015450112521648407,\n        -0.0029764187056571245,\n        0.01642385870218277,\n        0.01538519561290741,\n        -0.019176315516233444,\n        -0.005160856992006302,\n        0.016800373792648315,\n        -0.027005238458514214,\n        -0.014424433000385761,\n        0.026979271322488785,\n        0.005229019094258547,\n        0.004005344118922949,\n        0.027732303366065025,\n        0.030432825908064842,\n        0.0038560363464057446,\n        -0.02944609709084034,\n        0.013106629252433777,\n        -0.019682664424180984,\n        -0.009315509349107742,\n        0.007621190045028925,\n        0.013502619229257107,\n        -0.024408580735325813,\n        0.001573412213474512,\n        -0.009205151349306107,\n        0.0164498258382082,\n        0.029290296137332916,\n        0.013216987252235413,\n        -0.010094505734741688,\n        -0.02736877091228962,\n        0.01770920492708683,\n        0.01564486138522625,\n        -0.003075416199862957,\n        0.0012747965520247817,\n        -0.0033464424777776003,\n        -0.016073310747742653,\n        -0.006920092273503542,\n        0.021695073693990707,\n        -0.028615165501832962,\n        -0.0003706323041114956,\n        -0.0030965141486376524,\n        0.03676867112517357,\n        -0.018176602199673653,\n        0.019085433334112167,\n        -0.011048777960240841,\n        0.021513307467103004,\n        -0.011146152392029762,\n        -0.00940639153122902,\n        0.0317830890417099,\n        -0.008504053577780724,\n        0.007075891830027103,\n        -0.024278748780488968,\n        0.003460046136751771,\n        0.022292304784059525,\n        -0.00910777598619461,\n        0.009471308439970016,\n        -0.01824151910841465,\n        0.032458219677209854,\n        -0.004975845105946064,\n        0.0059171332977712154,\n        0.0016472545685246587,\n        0.005323147866874933,\n        -0.018111687153577805,\n        -0.0032847716938704252,\n        -0.007303099147975445,\n        -0.0038755114655941725,\n        0.000278532097581774,\n        0.008153504692018032,\n        -0.004456513561308384,\n        -0.015021664090454578,\n        -0.00611512828618288,\n        -0.005560093093663454,\n        -0.0040962272323668,\n        -0.010892977938055992,\n        0.006673410069197416,\n        -0.013398752547800541,\n        0.02347378432750702,\n        0.03414604812860489,\n        -0.007627681363373995,\n        0.004375367891043425,\n        -0.0213315412402153,\n        0.0010005246149376035,\n        -0.00939990021288395,\n        0.002560953376814723,\n        0.012366581708192825,\n        -0.03593774139881134,\n        -0.012736605480313301,\n        0.0019280181732028723,\n        -0.034042179584503174,\n        -0.004180618561804295,\n        0.03378251567482948,\n        -0.003492504358291626,\n        -0.04583100602030754,\n        0.003312361193820834,\n        -0.009841332212090492,\n        -0.005832741968333721,\n        0.006972025614231825,\n        0.009938706643879414,\n        0.009334984235465527,\n        -0.005715892184525728,\n        0.012795030139386654,\n        0.002807636046782136,\n        -0.017605338245630264,\n        -0.011931641027331352,\n        -0.0037911199033260345,\n        -0.023668533191084862,\n        0.006738326512277126,\n        0.011925149708986282,\n        0.008374220691621304,\n        0.0203448124229908,\n        0.029913494363427162,\n        -0.015060613863170147,\n        0.019163332879543304,\n        0.0029569438192993402,\n        0.004858995322138071,\n        -0.025174595415592194,\n        -0.004849257878959179,\n        0.005511405412107706,\n        -0.0061248657293617725,\n        -0.011555125936865807,\n        0.0108215706422925,\n        0.01210042368620634,\n        0.010360663756728172,\n        0.031653255224227905,\n        -0.009549207985401154,\n        0.027264904230833054,\n        -0.008737752214074135,\n        -0.006822717841714621,\n        -0.01902051642537117,\n        -0.006559805944561958,\n        -0.019098415970802307,\n        0.008510544896125793,\n        0.02843339927494526,\n        0.0037034829147160053,\n        0.0014906437136232853,\n        0.0020270159002393484,\n        -0.00988677330315113,\n        0.0020529823377728462,\n        0.018838750198483467,\n        -0.002588542876765132,\n        0.0027946527116000652,\n        -0.026953306049108505,\n        0.0045603797771036625,\n        0.015203430317342281,\n        -0.012308157049119473,\n        0.004417563788592815,\n        -0.028277600184082985,\n        -0.01537221297621727,\n        0.026641706004738808,\n        0.02188982255756855,\n        0.025213545188307762,\n        -0.013229969888925552,\n        -0.019708631560206413,\n        -0.03258805349469185,\n        0.021227674558758736,\n        0.012236748822033405,\n        -0.0008804291719570756,\n        -0.0010849159443750978,\n        -0.002588542876765132,\n        0.025382326915860176,\n        -0.01957879774272442,\n        0.021708056330680847,\n        -0.009257083758711815,\n        0.0021000467240810394,\n        0.011775841936469078,\n        0.04222165048122406,\n        -0.039988525211811066,\n        0.026745572686195374,\n        0.009763432666659355,\n        0.030121227726340294,\n        -0.008945485576987267,\n        0.003330213250592351,\n        0.010464529506862164,\n        -0.02369450032711029,\n        0.011133168824017048,\n        -0.00897145178169012,\n        0.004995319992303848,\n        0.0063066319562494755,\n        -0.024369630962610245,\n        -0.0018468726193532348,\n        0.0046252962201833725,\n        0.009503766894340515,\n        0.020968008786439896,\n        0.015333262272179127,\n        0.022538987919688225,\n        0.014281616546213627,\n        -0.0250447615981102,\n        0.006478660739958286,\n        0.0015133644919842482,\n        0.010815078392624855,\n        -0.013450685888528824,\n        -0.004904436878859997,\n        0.000786706106737256,\n        0.0015799037646502256,\n        -0.002825487870723009,\n        0.023525716736912727,\n        -0.006848684046417475,\n        0.03225048631429672,\n        0.011360377073287964,\n        0.007056416943669319,\n        -0.02843339927494526,\n        -0.004252026788890362,\n        0.011646009050309658,\n        -0.013606485910713673,\n        -0.03232838585972786,\n        0.015969444066286087,\n        0.031393591314554214,\n        -0.006757801398634911,\n        -0.01826748624444008,\n        -0.029238363727927208,\n        0.007108349818736315,\n        -0.007484865374863148,\n        0.009049351327121258,\n        -0.00470319576561451,\n        0.01356753520667553,\n        0.007355032488703728,\n        0.003395129693672061,\n        0.006985008716583252,\n        0.01200304925441742,\n        0.049596160650253296,\n        0.011366868391633034,\n        -0.004265009891241789,\n        0.00038604994188062847,\n        -0.0065695433877408504,\n        0.00848457869142294,\n        -0.007880855351686478,\n        -0.02970576286315918,\n        -0.007536798715591431,\n        -0.0033107383642345667,\n        -0.015437128953635693,\n        -0.008945485576987267,\n        -0.018371352925896645,\n        0.003044581040740013,\n        -0.012282189913094044,\n        0.0012496414128690958,\n        -0.02583674155175686,\n        -0.012671688571572304,\n        0.006069687195122242,\n        -0.011204577051103115,\n        -0.012236748822033405,\n        -0.011250019073486328,\n        0.03567807376384735,\n        -0.03985869139432907,\n        -0.008166488260030746,\n        -0.015553978271782398,\n        -0.033237215131521225,\n        -0.007095366716384888,\n        0.058788325637578964,\n        0.01004906464368105,\n        0.019981279969215393,\n        0.01174338348209858,\n        0.02395416609942913,\n        0.005680188536643982,\n        -0.012106915935873985,\n        -0.01848820224404335,\n        -0.008802669122815132,\n        -0.002909879432991147,\n        -0.002947206376120448,\n        -0.03077039122581482,\n        -0.014813931658864021,\n        0.0035931249149143696,\n        0.0038852489087730646,\n        -0.0011879708617925644,\n        0.005810021422803402,\n        -0.017618320882320404,\n        -0.0050862031057477,\n        0.00940639153122902,\n        -0.002741096541285515,\n        -0.0030543184839189053,\n        0.0200332123786211,\n        0.022577937692403793,\n        0.01421669963747263,\n        0.0333930142223835,\n        0.014060900546610355,\n        -0.005780808627605438,\n        0.010113981552422047,\n        -0.00329288630746305,\n        0.0017235314007848501,\n        0.007517323363572359,\n        0.016826340928673744,\n        -0.002375941723585129,\n        0.002854700433090329,\n        0.027966002002358437,\n        0.00580028397962451,\n        0.0013875889126211405,\n        0.00580677529796958,\n        -0.006602001842111349,\n        0.016358941793441772,\n        0.03199082240462303,\n        -0.0018793308408930898,\n        -0.022993402555584908,\n        0.01616419292986393,\n        -0.008900043554604053,\n        -0.02212352305650711,\n        0.0013332213275134563,\n        -0.00849107000976801,\n        -0.009068826213479042,\n        0.040767524391412735,\n        0.0081924544647336,\n        -0.029498029500246048,\n        -0.015060613863170147,\n        -0.003953411243855953,\n        0.01039961352944374,\n        -0.030380893498659134,\n        -0.021513307467103004,\n        -0.004469496663659811,\n        -0.016748441383242607,\n        -0.010341188870370388,\n        0.004326680675148964,\n        -0.009438849985599518,\n        -0.0019815743435174227,\n        -0.021565241739153862,\n        -0.03440571203827858,\n        -0.0011798562481999397,\n        0.01720285601913929,\n        -0.021811923012137413,\n        -0.010100997984409332,\n        -0.010152931325137615,\n        -0.022487053647637367,\n        -0.029030630365014076,\n        -0.001508495770394802,\n        0.006121620070189238,\n        0.007952263578772545,\n        -0.0025220036040991545,\n        -0.014839897863566875,\n        0.022811636328697205,\n        0.022720754146575928,\n        -0.01225622370839119,\n        -0.036820605397224426,\n        0.01693020761013031,\n        0.006868159398436546,\n        -0.009088301099836826,\n        0.016618607565760612,\n        -0.015073597431182861,\n        0.01852715201675892,\n        0.009555699303746223,\n        0.019422998651862144,\n        0.016592642292380333,\n        0.0066312141716480255,\n        0.01796887069940567,\n        -0.017605338245630264,\n        0.0001052863517543301,\n        0.017034074291586876,\n        0.020396744832396507,\n        -0.0072187078185379505,\n        -0.014684098772704601,\n        0.01039961352944374,\n        0.010204863734543324,\n        -0.017086006700992584,\n        -0.0009404768934473395,\n        -0.026213258504867554,\n        -0.015995411202311516,\n        0.02553812600672245,\n        -0.01870891824364662,\n        0.028511298820376396,\n        -0.02005917951464653,\n        -0.012418515048921108,\n        0.02687540464103222,\n        -0.04011835902929306,\n        0.022733736783266068,\n        -0.006160569842904806,\n        0.03284771740436554,\n        0.022201422601938248,\n        0.0016991877928376198,\n        0.026979271322488785,\n        -0.0020253928378224373,\n        -0.026226241141557693,\n        8.28191768960096e-05,\n        -0.06803242862224579,\n        0.017137939110398293,\n        -0.003872265573590994,\n        -0.007251166272908449,\n        -0.005378326866775751,\n        -1.1068759704357944e-05,\n        -0.009698515757918358,\n        -0.014878847636282444,\n        -0.0118407579138875,\n        -0.0017998082330450416,\n        0.026226241141557693,\n        -0.004144914448261261,\n        -0.023824332281947136,\n        -0.02270776964724064,\n        -0.032951585948467255,\n        0.0038105950225144625,\n        0.025654977187514305,\n        0.0025690679904073477,\n        -0.012080948799848557,\n        -0.03362671658396721,\n        -0.014021950773894787,\n        -0.002530118217691779,\n        -0.024914929643273354,\n        -0.021461375057697296,\n        -0.035080842673778534,\n        -0.012353598140180111,\n        0.0009096416179090738,\n        -0.01828046888113022,\n        0.008439136669039726,\n        -0.021474357694387436,\n        -0.0044273012317717075,\n        -0.027706336230039597,\n        0.0006787825259380043,\n        -0.01567082852125168,\n        -0.03199082240462303,\n        0.0020627197809517384,\n        0.0026745572686195374,\n        0.015281329862773418,\n        0.012463956139981747,\n        0.02864113263785839,\n        0.0016480660997331142,\n        0.013184528797864914,\n        0.010769637301564217,\n        -0.015164479613304138,\n        -0.010341188870370388,\n        -0.0024424809962511063,\n        0.003145201364532113,\n        -0.017306722700595856,\n        0.018864717334508896,\n        0.021448390558362007,\n        0.027498602867126465,\n        0.00988677330315113,\n        0.013204003684222698,\n        -0.004699950106441975,\n        0.00515761086717248,\n        -0.0048200455494225025,\n        -0.008724769577383995,\n        -0.025122661143541336,\n        -0.015034646727144718,\n        0.014632165431976318,\n        0.0029715499840676785,\n        -0.019682664424180984,\n        0.005576321855187416,\n        -0.003034843597561121,\n        -0.01929316483438015,\n        0.010107489302754402,\n        0.0114707350730896,\n        0.026641706004738808,\n        -0.0042325519025325775,\n        0.03539244085550308,\n        -0.012093932367861271,\n        -0.0009120759787037969,\n        -0.01158109214156866,\n        0.028303567320108414,\n        0.005313410423696041,\n        -0.02217545546591282,\n        -0.00606644107028842,\n        0.010211355984210968,\n        0.023305000737309456,\n        0.007439423818141222,\n        0.03071845881640911,\n        -0.012580805458128452,\n        0.009432358667254448,\n        0.009542716667056084,\n        0.014749014750123024,\n        -0.0048070624470710754,\n        -0.011905674822628498,\n        -0.005693171638995409,\n        -0.012074457481503487,\n        -0.0033756548073142767,\n        -0.021954739466309547,\n        -0.03235435113310814,\n        -0.028225667774677277,\n        0.009633599780499935,\n        0.0024424809962511063,\n        0.001506061409600079,\n        0.03406814858317375,\n        -0.007322574034333229,\n        -0.022811636328697205,\n        -0.022798653692007065,\n        0.009309017099440098,\n        0.009347966872155666,\n        -0.02661573886871338,\n        0.01160056795924902,\n        -0.005164102651178837,\n        -0.004089735448360443,\n        -0.009562191553413868,\n        -0.003567158244550228,\n        0.004346155561506748,\n        -0.010594362393021584,\n        0.012288682162761688,\n        0.014229683205485344,\n        0.0040962272323668,\n        -0.013054695911705494,\n        0.013320853002369404,\n        -0.002922862535342574,\n        0.002356466604396701,\n        0.0042974683456122875,\n        0.018085720017552376,\n        0.015320279635488987,\n        -0.002919616876170039,\n        -0.008276846259832382,\n        -0.016060326248407364,\n        -0.02687540464103222,\n        0.025629010051488876,\n        0.008776702918112278,\n        -0.0011668730294331908,\n        -0.014528298750519753,\n        -0.017332689836621284,\n        -0.01108772773295641,\n        0.02159120701253414,\n        -0.02710910513997078,\n        0.0019231494516134262,\n        0.022240372374653816,\n        -0.007303099147975445,\n        0.01798185333609581,\n        -0.009769923985004425,\n        0.0014224814949557185,\n        0.006283911410719156,\n        8.378277561860159e-05,\n        0.01981249637901783,\n        -0.023642566055059433,\n        0.004401334561407566,\n        -0.007958754897117615,\n        -0.009328491985797882,\n        -0.024071015417575836,\n        -0.01027627196162939,\n        -0.006887634284794331,\n        0.03261401876807213,\n        -0.027758268639445305,\n        -0.007686106488108635,\n        -0.0177481546998024,\n        0.021045910194516182,\n        0.012425006367266178,\n        0.012288682162761688,\n        -0.03471731022000313,\n        0.00021280419605318457,\n        0.0032815260346978903,\n        -0.010522955097258091,\n        0.014359516091644764,\n        -0.005826250184327364,\n        -0.013126104138791561,\n        0.003307492472231388,\n        -0.014359516091644764,\n        -0.023136219009757042,\n        -0.018397318199276924,\n        -0.0008479709504172206,\n        -0.00036860365071333945,\n        -0.01031522173434496,\n        -0.0007964435499161482,\n        -0.01903349906206131,\n        0.011003335937857628,\n        0.01329488679766655,\n        0.01108123641461134,\n        -0.01421669963747263,\n        -0.003182528307661414,\n        -0.023915216326713562,\n        -0.002145488280802965,\n        0.02425278164446354,\n        0.002982910256832838,\n        0.01082806196063757,\n        -0.023395884782075882,\n        7.034304871922359e-05,\n        1.9703153157024644e-05,\n        -0.0448962077498436,\n        0.012450972571969032,\n        0.00217794650234282,\n        -0.016229109838604927,\n        -0.015060613863170147,\n        0.011133168824017048,\n        0.02552514337003231,\n        0.005891166627407074,\n        -0.02713507041335106,\n        0.013853168115019798,\n        0.01665755733847618,\n        0.004128685686737299,\n        -0.012833979912102222,\n        -0.021487340331077576,\n        0.008627394214272499,\n        -0.021798940375447273,\n        0.018644001334905624,\n        -0.004923911765217781,\n        -0.003466537920758128,\n        0.013372786343097687,\n        0.008926010690629482,\n        0.025732876732945442,\n        0.005449735093861818,\n        -0.01953984797000885,\n        -0.0034730294719338417,\n        -0.00509269442409277,\n        0.010808587074279785,\n        0.0053263939917087555,\n        -0.00534586887806654,\n        -0.013398752547800541,\n        -0.007309590931981802,\n        -0.023538701236248016,\n        0.011418801732361317,\n        0.005904150195419788,\n        -0.00901040155440569,\n        0.03077039122581482,\n        0.01696915738284588,\n        -0.0200332123786211,\n        0.013983001001179218,\n        -0.003794365795329213,\n        -0.020448677241802216,\n        -0.01877383328974247,\n        -0.026472924277186394,\n        -0.011250019073486328,\n        -0.013262428343296051,\n        -0.022253355011343956,\n        0.025732876732945442,\n        0.008140521124005318,\n        -0.005310164764523506,\n        -0.04502604156732559,\n        -0.0026940321549773216,\n        -0.028485333546996117,\n        0.000682028301525861,\n        -0.010347680188715458,\n        0.02395416609942913,\n        0.00017141997523140162,\n        -0.005823004525154829,\n        -0.014891831204295158,\n        0.023681515827775,\n        0.01212639082223177,\n        0.0024570871610194445,\n        -0.0059171332977712154,\n        -0.004732408560812473,\n        0.0005505725275725126,\n        0.02346080169081688,\n        0.012872929684817791,\n        -0.0014476365176960826,\n        -0.04084542393684387,\n        0.002190929837524891,\n        0.03409411385655403,\n        -0.0400664247572422,\n        -0.016813358291983604,\n        0.01851416751742363,\n        -0.012029016390442848,\n        -0.01925421506166458,\n        -0.010899470187723637,\n        -0.003177659586071968,\n        0.0019312640652060509,\n        0.021500324830412865,\n        0.028719032183289528,\n        -0.009068826213479042,\n        -0.006076178513467312,\n        0.0075432900339365005,\n        -0.01223025657236576,\n        0.004777849651873112,\n        0.008724769577383995,\n        0.027238937094807625,\n        -0.012139374390244484,\n        0.015696795657277107,\n        -0.006731834728270769,\n        0.0007359901210293174,\n        0.02264285460114479,\n        -0.020526578649878502,\n        0.014774980954825878,\n        -0.009023385122418404,\n        -0.00936744175851345,\n        0.013333836570382118,\n        0.0019653451163321733,\n        -0.0059171332977712154,\n        -0.005637992639094591,\n        0.012950829230248928,\n        -0.021980706602334976,\n        0.010451546870172024,\n        0.022564953193068504,\n        -0.010477513074874878,\n        -0.013054695911705494,\n        0.0006662049563601613,\n        0.013093645684421062,\n        -0.02320113591849804,\n        -0.018851732835173607,\n        -0.01421669963747263,\n        -0.017280755564570427,\n        -0.014891831204295158,\n        0.005044007208198309,\n        0.023746432736516,\n        -0.0076082064770162106,\n        0.005452980753034353,\n        -0.004284484777599573,\n        -0.011801808141171932,\n        0.0010881618363782763,\n        -0.008439136669039726,\n        0.024343663826584816,\n        -0.006537085399031639,\n        -0.012795030139386654,\n        0.024356648325920105,\n        -0.007828922010958195,\n        -0.029056597501039505,\n        0.009328491985797882,\n        0.007342049386352301,\n        -0.0022444857750087976,\n        0.024577364325523376,\n        0.23598423600196838,\n        0.0038787571247667074,\n        -0.005874937865883112,\n        0.0333930142223835,\n        0.00923760887235403,\n        0.009471308439970016,\n        0.026200274005532265,\n        0.009016892872750759,\n        -0.001411121105775237,\n        0.023850299417972565,\n        0.006378040183335543,\n        -0.008004196919500828,\n        -0.030380893498659134,\n        0.013242953456938267,\n        -0.004417563788592815,\n        0.0019896887242794037,\n        -0.030640559270977974,\n        -0.005891166627407074,\n        -0.009088301099836826,\n        -0.007361524272710085,\n        -0.008439136669039726,\n        -0.017124956473708153,\n        -0.015294312499463558,\n        -0.000736801594030112,\n        0.023915216326713562,\n        0.011847250163555145,\n        0.004735654219985008,\n        -0.005293935537338257,\n        0.018851732835173607,\n        -0.00014971353812143207,\n        -0.008666344918310642,\n        0.0015547487419098616,\n        0.018631018698215485,\n        0.02343483455479145,\n        -0.015008680522441864,\n        0.00806911289691925,\n        -0.0037132203578948975,\n        -0.002108161337673664,\n        0.02218843810260296,\n        -0.0003862527955789119,\n        0.030121227726340294,\n        0.021552257239818573,\n        0.0105748875066638,\n        -0.026265190914273262,\n        -0.012639230117201805,\n        0.005170594435185194,\n        0.02030586265027523,\n        -0.03513277694582939,\n        0.01976056396961212,\n        0.021279608830809593,\n        -0.016540708020329475,\n        0.01724180579185486,\n        0.03471731022000313,\n        0.04712933301925659,\n        -0.008530019782483578,\n        0.022759703919291496,\n        0.011892691254615784,\n        -0.0004079592472407967,\n        -0.015138513408601284,\n        -0.002473316388204694,\n        -0.003029974875971675,\n        0.032276451587677,\n        -0.011944624595344067,\n        0.028069868683815002,\n        -0.01642385870218277,\n        -0.0021114072296768427,\n        -0.01976056396961212,\n        0.0005607157363556325,\n        0.0010832931147888303,\n        -0.012041999027132988,\n        -0.018319418653845787,\n        -0.018618034198880196,\n        0.011392834596335888,\n        -0.007569256704300642,\n        -0.024940894916653633,\n        -0.0030965141486376524,\n        0.03513277694582939,\n        0.017865004017949104,\n        0.00871178600937128,\n        0.023395884782075882,\n        -0.011749875731766224,\n        -0.004469496663659811,\n        -0.010841045528650284,\n        -0.006744817830622196,\n        0.0011409063590690494,\n        -0.030354926362633705,\n        0.013028728775680065,\n        -0.015982426702976227,\n        0.002692409325391054,\n        0.012210781686007977,\n        -0.0029894020408391953,\n        -0.025654977187514305,\n        -0.006647443398833275,\n        0.0046934583224356174,\n        0.017527438700199127,\n        0.023551683872938156,\n        0.0105748875066638,\n        0.013411736115813255,\n        -0.011321427300572395,\n        0.020461661741137505,\n        -0.023616600781679153,\n        -0.014398465864360332,\n        0.001811168622225523,\n        0.010113981552422047,\n        0.00020854406466241926,\n        0.004252026788890362,\n        -0.009068826213479042,\n        0.009607632644474506,\n        0.015359229408204556,\n        -0.010425579734146595,\n        -0.01368438545614481,\n        -0.01716390624642372,\n        0.007368015591055155,\n        0.0027118842117488384,\n        0.012152357026934624,\n        -0.0033091155346482992,\n        0.014839897863566875,\n        0.004284484777599573,\n        -0.0025771826039999723,\n        -0.006517610512673855,\n        0.02264285460114479,\n        0.0018679705681279302,\n        -0.016605624929070473,\n        0.0003974103310611099,\n        0.0074004740454256535,\n        -0.00502777798101306,\n        -0.018332401290535927,\n        -0.007991213351488113,\n        -0.009594650007784367,\n        -0.044143177568912506,\n        0.018644001334905624,\n        -0.026148341596126556,\n        0.0020692115649580956,\n        -0.010516462847590446,\n        -0.023902231827378273,\n        0.006972025614231825,\n        0.00580028397962451,\n        -0.01589154452085495,\n        -0.040819454938173294,\n        -0.016995122656226158,\n        -0.004488971550017595,\n        -0.008555986918509007,\n        -0.001936132786795497,\n        0.0017543667927384377,\n        0.016566675156354904,\n        -0.028874831274151802,\n        0.007114841602742672,\n        -0.005842479411512613,\n        -0.026239223778247833,\n        -0.013995984569191933,\n        -0.02475912868976593,\n        -0.004771358333528042,\n        -0.01641087606549263,\n        0.005034269765019417,\n        0.02655082382261753,\n        -0.02190280705690384,\n        -0.033704616129398346,\n        -0.012769063003361225,\n        -0.0024765620473772287,\n        0.0039014779031276703,\n        -0.022474071010947227,\n        0.0068292091600596905,\n        0.022344237193465233,\n        -0.0014581854920834303,\n        0.0032523134723305702,\n        -0.007796464022248983,\n        -0.16712087392807007,\n        0.0385863296687603,\n        0.005706154741346836,\n        -0.03601564094424248,\n        0.015424145385622978,\n        0.0059788040816783905,\n        0.039754826575517654,\n        0.0114707350730896,\n        -0.012554839253425598,\n        0.025213545188307762,\n        -0.0067642927169799805,\n        -0.01001011487096548,\n        -0.033990245312452316,\n        -0.0023499750532209873,\n        0.011412309482693672,\n        -0.02060447819530964,\n        0.01018538884818554,\n        -0.003872265573590994,\n        0.010224338620901108,\n        0.03282175213098526,\n        0.020163046196103096,\n        6.050414958735928e-05,\n        0.01716390624642372,\n        -0.006494889501482248,\n        0.012872929684817791,\n        -0.007251166272908449,\n        -0.0009307394502684474,\n        0.01590452715754509,\n        0.0014947010204195976,\n        -0.014242666773498058,\n        -0.03331511467695236,\n        -0.014930780977010727,\n        0.00021990443929098547,\n        0.02030586265027523,\n        -0.005891166627407074,\n        -0.00914672575891018,\n        -0.004320188891142607,\n        -0.01213288214057684,\n        -0.020993975922465324,\n        0.020189011469483376,\n        0.03199082240462303,\n        0.013801234774291515,\n        0.01238605659455061,\n        -0.0038300699088722467,\n        -0.008237896487116814,\n        -0.00519980676472187,\n        0.020383762195706367,\n        0.011256510391831398,\n        -0.003761907573789358,\n        -0.006137849297374487,\n        -0.00476811220869422,\n        -0.016865290701389313,\n        0.00494014099240303,\n        0.010282764211297035,\n        0.020253928378224373,\n        0.0379631333053112,\n        0.001713793957605958,\n        0.027576502412557602,\n        -0.016073310747742653,\n        -0.025590060278773308,\n        0.005738613195717335,\n        -0.023512734100222588,\n        0.016008393839001656,\n        -0.016332976520061493,\n        -0.013671401888132095,\n        -0.010393122211098671,\n        0.0035411918070167303,\n        0.010931928642094135,\n        -0.031367622315883636,\n        0.015709778293967247,\n        -0.004342909902334213,\n        -0.027031205594539642,\n        -0.02112380973994732,\n        -0.03811893239617348,\n        0.03019912727177143,\n        0.005105677992105484,\n        -0.039962559938430786,\n        0.0032620509155094624,\n        0.00264372187666595,\n        -0.02140944078564644,\n        -0.00783541426062584,\n        0.038975831121206284,\n        -0.0022996647749096155,\n        0.017319705337285995,\n        -0.0021974213887006044,\n        -0.004303959663957357,\n        0.005498422309756279,\n        -0.011327918618917465,\n        -0.0189815666526556,\n        -0.012833979912102222,\n        0.025914642959833145,\n        -0.010380138643085957,\n        0.004699950106441975,\n        -0.008549494668841362,\n        0.008380712009966373,\n        0.014281616546213627,\n        0.006199520081281662,\n        0.0013575650518760085,\n        -0.0014208585489541292,\n        0.00309813697822392,\n        0.008685819804668427,\n        -0.0013421473558992147,\n        -0.029108531773090363,\n        0.009302525781095028,\n        0.012016032822430134,\n        -0.0033561799209564924,\n        0.006861667614430189,\n        0.01355455256998539,\n        0.027784235775470734,\n        -0.015748728066682816,\n        -0.01538519561290741,\n        0.015722760930657387,\n        0.029368197545409203,\n        0.02657678909599781,\n        0.00896496046334505,\n        0.037911199033260345,\n        -0.014658131636679173,\n        -0.009724482893943787,\n        0.022331254556775093,\n        -0.024850012734532356,\n        0.025966575369238853,\n        -0.009029876440763474,\n        -0.01874786801636219,\n        -0.016618607565760612,\n        0.014671115204691887,\n        0.0020367533434182405,\n        -0.0855858325958252,\n        -0.0226947870105505,\n        0.014190733432769775,\n        0.02372046746313572,\n        0.003299378091469407,\n        0.02159120701253414,\n        0.012470447458326817,\n        -0.003599616466090083,\n        -0.006335844285786152,\n        0.004440284334123135,\n        -0.01589154452085495,\n        -0.036872535943984985,\n        -0.02895273081958294,\n        0.002028638729825616,\n        0.022590920329093933,\n        -0.014359516091644764,\n        -0.03313335031270981,\n        -0.014554265886545181,\n        -0.003214986529201269,\n        0.021967722102999687,\n        -0.011717417277395725,\n        -0.0074783735908567905,\n        0.0176832377910614,\n        -0.0026307387743145227,\n        -0.013983001001179218,\n        -0.008211929351091385,\n        -0.017073024064302444,\n        0.020773259922862053,\n        0.014060900546610355,\n        0.002750833984464407,\n        -0.023668533191084862,\n        -0.026524856686592102,\n        0.009614124894142151,\n        -0.012885913252830505,\n        -0.027472637593746185,\n        -0.003953411243855953,\n        -0.0353664755821228,\n        0.0017576125683262944,\n        -0.0088416188955307,\n        -0.037651535123586655,\n        -0.01131493505090475,\n        0.027991969138383865,\n        -0.008990926668047905,\n        -0.02526547759771347,\n        0.0016042474890127778,\n        0.005095940548926592,\n        -0.017397604882717133,\n        0.0026842947117984295,\n        0.010522955097258091,\n        -0.0203448124229908,\n        -0.017306722700595856,\n        -0.013749301433563232,\n        -0.022526003420352936,\n        -0.0017527438467368484,\n        0.031056024134159088,\n        -0.0071537913754582405,\n        -0.007406965829432011,\n        0.011035794392228127,\n        -0.010496987961232662,\n        0.005478947423398495,\n        0.01329488679766655,\n        -0.005008303094655275,\n        -0.028277600184082985,\n        0.04689563438296318,\n        0.004414317663758993,\n        -0.007523815147578716,\n        -0.009172692894935608,\n        0.0058489711955189705,\n        0.024019083008170128,\n        0.005105677992105484,\n        0.0037326952442526817,\n        0.020929059013724327,\n        -0.013450685888528824,\n        0.0015766579890623689,\n        -0.00883512757718563,\n        0.0038527906872332096,\n        -0.02033182792365551,\n        -0.017073024064302444,\n        0.004131931345909834,\n        -0.03695043548941612,\n        -0.00470319576561451,\n        -0.01715092360973358,\n        -0.021227674558758736,\n        -0.0076601398177444935,\n        0.044662509113550186,\n        -0.0015977558214217424,\n        -1.0067119546874892e-05,\n        -0.0013526963302865624,\n        -0.0023012878373265266,\n        0.0005367777775973082,\n        0.0017365147359669209,\n        0.0358598418533802,\n        0.024161897599697113,\n        -0.017384622246026993,\n        -0.024032065644860268,\n        -0.007712072692811489,\n        -0.00991274043917656,\n        0.0014322189381346107,\n        0.025732876732945442,\n        -0.0035606666933745146,\n        -0.015463095158338547,\n        -0.005758088082075119,\n        -0.03274385258555412,\n        0.020747292786836624,\n        -0.019955312833189964,\n        -0.01066577062010765,\n        -0.003761907573789358,\n        -0.01848820224404335,\n        0.0030835308134555817,\n        -0.03315931558609009,\n        0.006056703627109528,\n        -0.01847521774470806,\n        -0.015748728066682816,\n        0.022993402555584908,\n        0.016086293384432793,\n        -0.002877421211451292,\n        -0.023551683872938156,\n        -0.02864113263785839,\n        0.029887529090046883,\n        0.016345959156751633,\n        0.020383762195706367,\n        0.004076752346009016,\n        0.0049531240947544575,\n        -0.004849257878959179,\n        -0.003352934028953314,\n        -0.01109421905130148,\n        0.0037002370227128267,\n        0.007621190045028925,\n        -0.023681515827775,\n        0.04113105684518814,\n        -0.012158849276602268,\n        -0.009373934008181095,\n        0.01525536272674799,\n        -0.030822325497865677,\n        0.010990353301167488,\n        0.01881278306245804,\n        0.006394268944859505,\n        -0.0034795210231095552,\n        0.012236748822033405,\n        0.04689563438296318,\n        0.027290871366858482,\n        -0.002588542876765132,\n        -0.018890684470534325,\n        -0.024135932326316833,\n        0.015618895180523396,\n        -0.005777562968432903,\n        0.005024532321840525,\n        0.004865487106144428,\n        -0.026693640276789665,\n        0.011172118596732616,\n        0.0226947870105505,\n        -0.006494889501482248,\n        0.014034934341907501,\n        -0.006400760728865862,\n        -0.017657270655035973,\n        -0.024135932326316833,\n        -0.015618895180523396,\n        -0.027784235775470734,\n        -0.017047056928277016,\n        0.0026469677686691284,\n        0.006264436524361372,\n        -0.019994262605905533,\n        0.039469193667173386,\n        0.027784235775470734,\n        0.027810202911496162,\n        -0.014839897863566875,\n        0.017384622246026993,\n        0.006868159398436546,\n        0.0017608583439141512,\n        0.0021162759512662888,\n        0.015800660476088524,\n        -0.01951388083398342,\n        -0.014502332545816898,\n        -0.004923911765217781,\n        -0.001364056603051722,\n        0.0066182310692965984,\n        -0.0003984246286563575,\n        0.007199232932180166,\n        0.0026388531550765038,\n        -0.015229396522045135,\n        -0.014008967205882072,\n        0.03149745613336563,\n        0.03051072545349598,\n        -0.012885913252830505,\n        -0.018436267971992493,\n        -0.004521430004388094,\n        0.014268632978200912,\n        0.029316263273358345,\n        -0.014229683205485344,\n        0.00632610684260726,\n        -0.011321427300572395,\n        0.023642566055059433,\n        -0.03954709321260452,\n        -0.0056542218662798405,\n        -0.0035704041365534067,\n        0.039988525211811066,\n        0.008250879123806953,\n        0.004907682538032532,\n        0.016034360975027084,\n        0.02320113591849804,\n        0.025473210960626602,\n        0.008854602463543415,\n        -0.0015222905203700066,\n        0.00953622441738844,\n        -0.027784235775470734,\n        -0.023123234510421753,\n        -0.004404580220580101,\n        0.00901040155440569,\n        -0.013710351660847664,\n        -0.026498889550566673,\n        0.0016220995457842946,\n        0.02760246954858303,\n        0.009322000667452812,\n        0.01551502849906683,\n        0.016566675156354904,\n        0.01770920492708683,\n        0.0056672049686312675,\n        -0.023902231827378273,\n        0.004196847788989544,\n        -0.024135932326316833,\n        -0.029627863317728043,\n        0.035600174218416214,\n        -0.014515315182507038,\n        -0.0031809054780751467,\n        0.011412309482693672,\n        -0.016527725383639336,\n        0.01669650711119175,\n        -0.013671401888132095,\n        -0.015683811157941818,\n        -0.012963812798261642,\n        0.011263001710176468,\n        -0.001111694029532373,\n        0.023538701236248016,\n        -0.00013105006655678153,\n        0.01069173775613308,\n        -0.030095260590314865,\n        -0.014761998318135738,\n        -0.018955599516630173,\n        -0.0044922176748514175,\n        0.01844925247132778,\n        -0.014891831204295158,\n        0.03253611922264099,\n        -0.0038333155680447817,\n        0.009977656416594982,\n        0.008030163124203682,\n        0.017047056928277016,\n        0.0350029431283474,\n        0.03147149085998535,\n        -0.015060613863170147,\n        -0.022305287420749664,\n        0.011360377073287964,\n        -0.00273785088211298,\n        -0.00580677529796958,\n        -0.0012050113873556256,\n        -0.048375729471445084,\n        -0.009743957780301571,\n        -0.010503479279577732,\n        -0.02710910513997078,\n        0.002806012984365225,\n        -0.01079560350626707,\n        0.015528012067079544,\n        0.012691163457930088,\n        -0.0002740690833888948,\n        0.006020999513566494,\n        0.01748848892748356,\n        -0.0174365546554327,\n        -0.007004483602941036,\n        0.03856036439538002,\n        0.002872552489861846,\n        -0.006881142500787973,\n        -0.03814489766955376,\n        -0.001618853653781116,\n        0.00041282796883024275,\n        -0.008562478236854076,\n        -0.028017934411764145,\n        0.03160132095217705,\n        -0.00966605730354786,\n        -0.005715892184525728,\n        -0.016099276021122932,\n        0.006264436524361372,\n        -0.0016667295712977648,\n        0.01134090218693018,\n        0.020786244422197342,\n        -0.0035768956877291203,\n        -0.014242666773498058,\n        0.013502619229257107,\n        -1.3794742699246854e-05,\n        0.003275034250691533,\n        0.013184528797864914,\n        -0.005488684866577387\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"ada\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 3,\n    \"total_tokens\": 3\n  }\n}","__computed__.cumulative_token_count.prompt":3,"__computed__.cumulative_token_count.total":3},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-49511/49512","correlationid":"85912514-2746-4c1e-828a-4030fb3fcb9e","xrequestid":"85912514-2746-4c1e-828a-4030fb3fcb9e"}
{"specversion":"1.0","id":"79f4a036-b569-481e-9e16-0944035b5c52","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-07T04:22:08Z","data":[{"name":"vector_lookup","context":{"trace_id":"0x0ef89cf7e7aa594e6c0e2281bb5c44b9","span_id":"0xc58e0c1975f63a4f","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xacab7ff109688c71","start_time":"2024-02-07T04:22:01.669189Z","end_time":"2024-02-07T04:22:05.522260Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"search","node_name":"vector_lookup","line_run_id":"f232dcce-ac1f-4b3e-a6a1-ab45ba364aaf","inputs":"{\n  \"mlindex_content\": \"embeddings:\\n  api_base: https:\/\/promptflow-ci-weu.openai.azure.com\/\\n  api_type: azure\\n  api_version: 2023-03-15-preview\\n  batch_size: '16'\\n  connection:\\n    id: \\n      \/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourceGroups\/promptflow\/providers\/Microsoft.MachineLearningServices\/workspaces\/pf-xp\/connections\/azure_open_ai_connection\\n  connection_type: workspace_connection\\n  deployment: text-embedding-ada-003\\n  dimension: 1536\\n  file_format_version: '2'\\n  kind: open_ai\\n  model: text-embedding-ada-002\\n  schema_version: '2'\\nindex:\\n  engine: langchain.vectorstores.FAISS\\n  kind: faiss\\n  method: FlatL2\\n  path: \\n    azureml:\/\/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourcegroups\/promptflow\/workspaces\/pf-xp\/datastores\/workspaceblobstore\/paths\/azureml\/f7ee0381-06fc-4566-820f-86ba174db987\/index\/\\nself:\\n  path: \\n    azureml:\/\/subscriptions\/96aede12-2f73-41cb-b983-6d11a904839b\/resourcegroups\/promptflow\/workspaces\/pf-xp\/datastores\/workspaceblobstore\/paths\/azureml\/f7ee0381-06fc-4566-820f-86ba174db987\/index\/\\n  asset_id: \\n    azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/serene-honey-h9lrkyczzk\/versions\/1\\n\",\n  \"queries\": [\n    \"How are you?\",\n    \"how are you\"\n  ],\n  \"query_type\": \"Vector\",\n  \"top_k\": 3\n}","output":"[\n  [\n    {\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf13\",\n        \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n        \"mtime\": null,\n        \"page_number\": 13,\n        \"stats\": {\n          \"tiktokens\": 304,\n          \"chars\": 835,\n          \"lines\": 113\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.5357727408409119\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf14\",\n        \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n        \"mtime\": null,\n        \"page_number\": 14,\n        \"stats\": {\n          \"tiktokens\": 291,\n          \"chars\": 838,\n          \"lines\": 113\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.5490214824676514\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf12\",\n        \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n        \"mtime\": null,\n        \"page_number\": 12,\n        \"stats\": {\n          \"tiktokens\": 254,\n          \"chars\": 833,\n          \"lines\": 73\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.5668976306915283\n    }\n  ],\n  [\n    {\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf13\",\n        \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n        \"mtime\": null,\n        \"page_number\": 13,\n        \"stats\": {\n          \"tiktokens\": 304,\n          \"chars\": 835,\n          \"lines\": 113\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.5345910787582397\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf14\",\n        \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n        \"mtime\": null,\n        \"page_number\": 14,\n        \"stats\": {\n          \"tiktokens\": 291,\n          \"chars\": 838,\n          \"lines\": 113\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.5457529425621033\n    },\n    {\n      \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n      \"metadata\": {\n        \"source_doc_id\": \"1706.03762.pdf12\",\n        \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n        \"mtime\": null,\n        \"page_number\": 12,\n        \"stats\": {\n          \"tiktokens\": 254,\n          \"chars\": 833,\n          \"lines\": 73\n        },\n        \"source\": {\n          \"filename\": \"1706.03762.pdf\",\n          \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n          \"mtime\": 1706775620.0\n        }\n      },\n      \"score\": 0.5477610230445862\n    }\n  ]\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-12624/12625","correlationid":"ed51d286-679c-4c30-9008-e9f5a783c64a","xrequestid":"ed51d286-679c-4c30-9008-e9f5a783c64a"}
{"specversion":"1.0","id":"f52f673d-2451-4e2d-8d06-667915467bb9","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-07T04:22:08Z","data":[{"name":"answer_the_question_with_context","context":{"trace_id":"0x0ef89cf7e7aa594e6c0e2281bb5c44b9","span_id":"0x86ba86e50e7dc354","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xacab7ff109688c71","start_time":"2024-02-07T04:22:05.622018Z","end_time":"2024-02-07T04:22:06.682149Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"AzureOpenAI.chat","node_name":"answer_the_question_with_context","line_run_id":"f232dcce-ac1f-4b3e-a6a1-ab45ba364aaf","inputs":"{\n  \"prompt\": \"{{prompt_text}}\",\n  \"deployment_name\": \"gpt-35-turbo\",\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"prompt_text\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\nuser: how are you \\nassistant:\",\n  \"stream\": false\n}","output":"\"I'm an AI language model, so I don't have feelings, but I'm here to help you with any questions you have. Do you have a specific question in mind?\"","__computed__.cumulative_token_count.completion":36,"__computed__.cumulative_token_count.prompt":1109,"__computed__.cumulative_token_count.total":1145},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"xrequestid":"6d38a67a-eb1b-44ad-a9d8-c1d7b458af5d","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-5345/5346","correlationid":"6d38a67a-eb1b-44ad-a9d8-c1d7b458af5d"}
{"specversion":"1.0","id":"bb1a1bd4-80e3-4044-99eb-568b7c57209b","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-07T04:22:08Z","data":[{"name":"search","context":{"trace_id":"0x0ef89cf7e7aa594e6c0e2281bb5c44b9","span_id":"0x1f3902a006497f4b","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xc58e0c1975f63a4f","start_time":"2024-02-07T04:22:04.579947Z","end_time":"2024-02-07T04:22:05.505074Z","status":{"status_code":"UNSET"},"attributes":{"span_type":"Retrieval","retrieval.query":"how are you","retrieval.documents":"[\n    {\n        \"document.content\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n        \"document.score\": 0.5345910787582397,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf13\",\n            \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n            \"mtime\": null,\n            \"page_number\": 13,\n            \"stats\": {\n                \"tiktokens\": 304,\n                \"chars\": 835,\n                \"lines\": 113\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n        \"document.score\": 0.5457529425621033,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf14\",\n            \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n            \"mtime\": null,\n            \"page_number\": 14,\n            \"stats\": {\n                \"tiktokens\": 291,\n                \"chars\": 838,\n                \"lines\": 113\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"document.score\": 0.5477610230445862,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf12\",\n            \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n            \"mtime\": null,\n            \"page_number\": 12,\n            \"stats\": {\n                \"tiktokens\": 254,\n                \"chars\": 833,\n                \"lines\": 73\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    }\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-6116/6117","correlationid":"3467ade1-c1ce-4758-b39f-ae8d4c79830a","xrequestid":"3467ade1-c1ce-4758-b39f-ae8d4c79830a","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"7f0c90db-9ba7-4553-aaa9-efab7980b6e2","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-07T04:22:08Z","data":[{"name":"generate_prompt_context","context":{"trace_id":"0x0ef89cf7e7aa594e6c0e2281bb5c44b9","span_id":"0x50a18e61efda1a10","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xacab7ff109688c71","start_time":"2024-02-07T04:22:05.606207Z","end_time":"2024-02-07T04:22:05.613146Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"generate_prompt_context","node_name":"generate_prompt_context","line_run_id":"f232dcce-ac1f-4b3e-a6a1-ab45ba364aaf","inputs":"{\n  \"search_result\": [\n    [\n      {\n        \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf13\",\n          \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n          \"mtime\": null,\n          \"page_number\": 13,\n          \"stats\": {\n            \"tiktokens\": 304,\n            \"chars\": 835,\n            \"lines\": 113\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.5357727408409119\n      },\n      {\n        \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf14\",\n          \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n          \"mtime\": null,\n          \"page_number\": 14,\n          \"stats\": {\n            \"tiktokens\": 291,\n            \"chars\": 838,\n            \"lines\": 113\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.5490214824676514\n      },\n      {\n        \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf12\",\n          \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n          \"mtime\": null,\n          \"page_number\": 12,\n          \"stats\": {\n            \"tiktokens\": 254,\n            \"chars\": 833,\n            \"lines\": 73\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.5668976306915283\n      }\n    ],\n    [\n      {\n        \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf13\",\n          \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n          \"mtime\": null,\n          \"page_number\": 13,\n          \"stats\": {\n            \"tiktokens\": 304,\n            \"chars\": 835,\n            \"lines\": 113\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.5345910787582397\n      },\n      {\n        \"text\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf14\",\n          \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n          \"mtime\": null,\n          \"page_number\": 14,\n          \"stats\": {\n            \"tiktokens\": 291,\n            \"chars\": 838,\n            \"lines\": 113\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.5457529425621033\n      },\n      {\n        \"text\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"metadata\": {\n          \"source_doc_id\": \"1706.03762.pdf12\",\n          \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n          \"mtime\": null,\n          \"page_number\": 12,\n          \"stats\": {\n            \"tiktokens\": 254,\n            \"chars\": 833,\n            \"lines\": 73\n          },\n          \"source\": {\n            \"filename\": \"1706.03762.pdf\",\n            \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n            \"mtime\": 1706775620.0\n          }\n        },\n        \"score\": 0.5477610230445862\n      }\n    ]\n  ]\n}","output":"\"Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-15198/15199","correlationid":"11d3263b-9ba0-441e-b0f7-e46c5d604ce2","xrequestid":"11d3263b-9ba0-441e-b0f7-e46c5d604ce2","agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame"}
{"specversion":"1.0","id":"54a6ec9f-2d8a-4914-b286-683145c60f1d","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-07T04:22:08Z","data":[{"name":"model","context":{"trace_id":"0x0ef89cf7e7aa594e6c0e2281bb5c44b9","span_id":"0xacab7ff109688c71","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":null,"start_time":"2024-02-07T04:22:01.048942Z","end_time":"2024-02-07T04:22:06.685589Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Flow","line_run_id":"f232dcce-ac1f-4b3e-a6a1-ab45ba364aaf","inputs":"{\n  \"question\": \"how are you\",\n  \"chat_history\": []\n}","output":"{\n  \"output\": \"I'm an AI language model, so I don't have feelings, but I'm here to help you with any questions you have. Do you have a specific question in mind?\"\n}"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"correlationid":"f86d7eba-b89d-4482-8be8-38ea40484ff1","xrequestid":"f86d7eba-b89d-4482-8be8-38ea40484ff1","agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","contentrange":"bytes 0-748/749"}
{"specversion":"1.0","id":"1ad4b1e4-e406-4336-9381-06e97c121efb","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-07T04:22:08Z","data":[{"name":"Embeddings.create","context":{"trace_id":"0x0ef89cf7e7aa594e6c0e2281bb5c44b9","span_id":"0x98177bddae4a4160","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x003127715ac5dda4","start_time":"2024-02-07T04:22:04.805635Z","end_time":"2024-02-07T04:22:05.513866Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Embedding","function":"Embeddings.create","node_name":"vector_lookup","line_run_id":"f232dcce-ac1f-4b3e-a6a1-ab45ba364aaf","inputs":"{\n  \"input\": [\n    [\n      4438,\n      527,\n      499,\n      30\n    ]\n  ],\n  \"model\": \"text-embedding-ada-002\"\n}","embedding.model":"ada","embedding.embeddings":"[\n  {\n    \"embedding.vector\": \"\u003c1536 dimensional vector\u003e\",\n    \"embedding.text\": [\n      4438,\n      527,\n      499,\n      30\n    ]\n  }\n]","output":"{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.01686037704348564,\n        -0.012121477164328098,\n        0.006747848354279995,\n        -0.02617795392870903,\n        -0.01613321155309677,\n        0.01757521741092205,\n        -0.011006079614162445,\n        -0.010198802687227726,\n        -0.01810518465936184,\n        -0.010611684992909431,\n        0.027656933292746544,\n        0.0016160942614078522,\n        -0.007222354412078857,\n        -0.011468260549008846,\n        0.007216191850602627,\n        -0.01535674650222063,\n        0.028495023027062416,\n        -0.011770219542086124,\n        0.013791492208838463,\n        -0.02073037624359131,\n        0.0025389164220541716,\n        0.006239448674023151,\n        0.0010499220807105303,\n        -0.007968006655573845,\n        -0.016059262678027153,\n        -0.007721510250121355,\n        0.02499477006494999,\n        -0.012509709224104881,\n        0.02208610810339451,\n        -0.024945469573140144,\n        0.005453740246593952,\n        0.007394901942461729,\n        -0.013199900276958942,\n        0.0040887645445764065,\n        0.008830745704472065,\n        -0.022234005853533745,\n        0.003817618126049638,\n        -0.010303563438355923,\n        0.020151108503341675,\n        -0.006288748234510422,\n        0.02691744454205036,\n        0.0012694582110270858,\n        -0.005364385433495045,\n        -0.01425983663648367,\n        -0.04138680174946785,\n        0.014346109703183174,\n        -0.010235777124762535,\n        -0.015221173875033855,\n        -0.017883338034152985,\n        0.035914573818445206,\n        0.020496204495429993,\n        0.015159549191594124,\n        -0.030023302882909775,\n        -0.021888909861445427,\n        0.0018425631569698453,\n        -0.004874472972005606,\n        -0.005056264344602823,\n        0.019066521897912025,\n        -0.009668834507465363,\n        -0.02563566155731678,\n        -0.0015390639891847968,\n        0.006183987017720938,\n        -0.001216307282447815,\n        0.015147224068641663,\n        0.010852019302546978,\n        -0.010229614563286304,\n        0.02553706243634224,\n        0.008892369456589222,\n        -0.009995442815124989,\n        0.006803310010582209,\n        -0.012645282782614231,\n        0.007770809810608625,\n        -0.008947831578552723,\n        -0.0034047362860292196,\n        0.01250354666262865,\n        -0.013890091329813004,\n        -0.02236957848072052,\n        0.011357337236404419,\n        -0.004775874316692352,\n        -0.0053397356532514095,\n        0.020952222868800163,\n        -0.040893808007240295,\n        -0.010346700437366962,\n        0.011437448672950268,\n        0.018166810274124146,\n        -0.0012193885631859303,\n        -0.012337162159383297,\n        0.01268225722014904,\n        -0.010081716813147068,\n        -0.02273932471871376,\n        -0.005798835773020983,\n        0.00672936113551259,\n        0.00016445954679511487,\n        0.028495023027062416,\n        -0.014481683261692524,\n        0.0027807913720607758,\n        0.01259598322212696,\n        0.019337669014930725,\n        -0.005059345625340939,\n        -0.022406553849577904,\n        -0.020446904003620148,\n        -0.000660919351503253,\n        -0.01064865943044424,\n        -0.005358222872018814,\n        0.0014473979827016592,\n        0.0024711298756301403,\n        -0.011727082543075085,\n        -0.004877554252743721,\n        0.012263212352991104,\n        -0.016071587800979614,\n        -0.013101301155984402,\n        0.010956780053675175,\n        0.00597446458414197,\n        -0.01857353001832962,\n        0.0015097925206646323,\n        -0.005604719743132591,\n        -0.003762156469747424,\n        -0.009681159630417824,\n        -0.010981429368257523,\n        -0.005111726000905037,\n        0.010617846623063087,\n        0.013532671146094799,\n        -0.0012717690551653504,\n        -0.025241265073418617,\n        0.008990968577563763,\n        0.006673899013549089,\n        -0.028741519898176193,\n        -0.0025173479225486517,\n        0.0015691057778894901,\n        -0.003731344360858202,\n        0.027114640921354294,\n        0.009570235386490822,\n        -0.007456526160240173,\n        -0.0027468979824334383,\n        -0.005869703367352486,\n        0.018955599516630173,\n        -0.023207668215036392,\n        0.02292419597506523,\n        -0.02010180801153183,\n        -0.01549232006072998,\n        0.0009682700037956238,\n        0.021050821989774704,\n        0.02844572439789772,\n        -0.01674945279955864,\n        -0.010254263877868652,\n        0.0018317789072170854,\n        0.023306267336010933,\n        -0.012232400476932526,\n        -0.005885109771043062,\n        -0.016601555049419403,\n        0.022850247099995613,\n        -0.007961845025420189,\n        0.028051327913999557,\n        0.036900561302900314,\n        0.0030673437286168337,\n        0.017723115161061287,\n        -0.007659886032342911,\n        0.026942092925310135,\n        0.001193968579173088,\n        -0.008658197708427906,\n        -0.030097251757979393,\n        -0.00923130288720131,\n        0.021482190117239952,\n        0.0028085222002118826,\n        -0.016687829047441483,\n        0.027681583538651466,\n        0.04471450671553612,\n        -0.010753420181572437,\n        -0.012756206095218658,\n        -0.00303345057182014,\n        0.0009490124648436904,\n        -0.010285076685249805,\n        -0.04530610144138336,\n        0.007752322591841221,\n        -0.015184199437499046,\n        -0.005875865928828716,\n        0.008054280653595924,\n        -0.007641398813575506,\n        -0.03179807960987091,\n        -0.016626205295324326,\n        -0.03423839807510376,\n        0.01748894341289997,\n        0.027188589796423912,\n        0.025142667815089226,\n        0.005533851683139801,\n        0.003472522832453251,\n        0.01000776793807745,\n        0.008725984953343868,\n        0.03990782052278519,\n        -0.020052509382367134,\n        0.019953910261392593,\n        0.028371773660182953,\n        0.01466655544936657,\n        -0.016811078414320946,\n        -0.6811197996139526,\n        -0.027928080409765244,\n        0.01764916628599167,\n        -0.0073271156288683414,\n        0.015701841562986374,\n        0.02482222206890583,\n        0.014592606574296951,\n        0.02943171001970768,\n        -0.01846260577440262,\n        0.025339864194393158,\n        -0.0058419727720320225,\n        0.016416681930422783,\n        0.0010915183229371905,\n        -0.008109742775559425,\n        0.0012240103678777814,\n        -0.031477633863687515,\n        -0.008966319262981415,\n        -0.003863836405798793,\n        -0.019066521897912025,\n        -0.00034817663254216313,\n        -0.016552256420254707,\n        0.005589313339442015,\n        -0.02409505657851696,\n        -0.003087371587753296,\n        0.01639203354716301,\n        0.0026082436088472605,\n        0.03152693435549736,\n        -0.015048625878989697,\n        -0.02598075568675995,\n        0.026473749428987503,\n        -0.022714674472808838,\n        0.01064865943044424,\n        0.006914233323186636,\n        -0.020816650241613388,\n        0.05344049260020256,\n        -0.0007298614364117384,\n        -0.007351765409111977,\n        0.02299814485013485,\n        -0.0034047362860292196,\n        0.03359750658273697,\n        -0.006692386232316494,\n        -0.008239153772592545,\n        0.011788706295192242,\n        -0.03150228410959244,\n        0.009976955130696297,\n        -0.0015290500596165657,\n        0.009705808945000172,\n        0.000689805718138814,\n        0.013668243773281574,\n        -0.000998311792500317,\n        0.003503334941342473,\n        0.011659295298159122,\n        -0.00996463093906641,\n        0.01131420023739338,\n        0.0011985903838649392,\n        0.0030842905398458242,\n        0.02879081852734089,\n        0.005638612899929285,\n        0.009607210755348206,\n        0.0033246248494833708,\n        -0.006190149113535881,\n        0.01920209638774395,\n        -0.013717543333768845,\n        -0.0005357452319003642,\n        -0.014148912392556667,\n        0.005780348554253578,\n        -0.00420893169939518,\n        0.007813946343958378,\n        0.005133294500410557,\n        -0.00495766568928957,\n        0.007641398813575506,\n        0.0009189706761389971,\n        -0.01000776793807745,\n        -0.005019289907068014,\n        0.014987001195549965,\n        0.022431202232837677,\n        0.02573425881564617,\n        -0.003817618126049638,\n        0.011024566367268562,\n        0.0032753252889961004,\n        0.023934833705425262,\n        -0.0008165204199030995,\n        -0.026769544929265976,\n        0.0027931162621825933,\n        0.00987835694104433,\n        -0.028544321656227112,\n        -0.024772921577095985,\n        0.017008274793624878,\n        0.02362671308219433,\n        0.010044742375612259,\n        0.028544321656227112,\n        0.026227252557873726,\n        0.008294614963233471,\n        -0.03349890559911728,\n        0.006433564703911543,\n        -0.01127722579985857,\n        -0.014925377443432808,\n        0.0166508536785841,\n        0.006408915389329195,\n        -0.00336159928701818,\n        -0.002737654373049736,\n        -0.016515281051397324,\n        0.02691744454205036,\n        0.017439644783735275,\n        0.0070374817587435246,\n        -0.003229107242077589,\n        -0.016330407932400703,\n        0.021235693246126175,\n        0.018967924639582634,\n        0.0028824713081121445,\n        0.012842480093240738,\n        0.001813291572034359,\n        -0.0003707079740706831,\n        -0.004714250098913908,\n        0.02455107495188713,\n        -0.025487761944532394,\n        0.012916428968310356,\n        0.01910349726676941,\n        0.032118525356054306,\n        -0.019596491008996964,\n        0.013569645583629608,\n        -0.012201588600873947,\n        0.01490072812885046,\n        0.0063349660485982895,\n        -0.009373038075864315,\n        0.01758754253387451,\n        -0.02371298521757126,\n        0.014321460388600826,\n        -0.03423839807510376,\n        0.017168497666716576,\n        0.014469358138740063,\n        -0.0032414321321994066,\n        0.0051178885623812675,\n        -0.020533178001642227,\n        0.028914067894220352,\n        -0.0004772022657562047,\n        0.0105500603094697,\n        -0.002240039175376296,\n        0.019953910261392593,\n        -0.004643382504582405,\n        -0.0009590263944119215,\n        -0.013890091329813004,\n        0.0070005073212087154,\n        0.014913052320480347,\n        0.007388739846646786,\n        -0.0016515281749889255,\n        -0.0012294024927541614,\n        -0.012139963917434216,\n        -0.00978592038154602,\n        -0.0028208470903337,\n        0.0007094484171830118,\n        -0.0056201256811618805,\n        -0.018499579280614853,\n        0.01847493089735508,\n        0.024242954328656197,\n        -0.015788115561008453,\n        -0.0076906983740627766,\n        -0.022714674472808838,\n        0.006267179735004902,\n        -0.011579183861613274,\n        0.012454248033463955,\n        0.018080536276102066,\n        -0.012207751162350178,\n        -0.003377005225047469,\n        0.0039131357334554195,\n        -0.03303056210279465,\n        -0.0011053838534280658,\n        0.02898801676928997,\n        -0.006821797229349613,\n        -0.02627655304968357,\n        0.005213405936956406,\n        -0.018055886030197144,\n        -0.01557859405875206,\n        -0.004027140326797962,\n        0.004785117693245411,\n        -0.006599950138479471,\n        -0.012435760349035263,\n        -0.011301875114440918,\n        0.020163433626294136,\n        -0.013668243773281574,\n        0.0074318768456578255,\n        -0.0041935257613658905,\n        -0.018906299024820328,\n        0.021765662357211113,\n        0.028396423906087875,\n        -0.008399376645684242,\n        0.030368398874998093,\n        0.044887054711580276,\n        -0.014752829447388649,\n        0.01612088643014431,\n        0.024242954328656197,\n        0.011585346423089504,\n        -0.025216616690158844,\n        -0.0050809141248464584,\n        -0.005136375781148672,\n        -0.01847493089735508,\n        -0.017821714282035828,\n        -0.0021999834571033716,\n        0.021876584738492966,\n        0.022579101845622063,\n        0.02219703048467636,\n        0.005154862999916077,\n        0.03253756836056709,\n        -0.014161237515509129,\n        0.002437236486002803,\n        -0.024403177201747894,\n        0.0024819141253829002,\n        -0.01281783077865839,\n        0.014235186390578747,\n        0.036456868052482605,\n        0.00158528215251863,\n        0.007271653972566128,\n        0.001062246854417026,\n        -0.0014412356540560722,\n        0.004624894820153713,\n        0.00783243402838707,\n        -0.010402162559330463,\n        0.0021429809276014566,\n        -0.019892286509275436,\n        0.013298499397933483,\n        0.01918977126479149,\n        0.004809767473489046,\n        -0.006969695445150137,\n        -0.01930069364607334,\n        -0.0037929685786366463,\n        0.020335981622338295,\n        0.013125951401889324,\n        0.031921327114105225,\n        0.002600540639832616,\n        -0.013286174274981022,\n        -0.03369610384106636,\n        0.012386460788547993,\n        0.015455345623195171,\n        0.0013711381470784545,\n        -0.0041349828243255615,\n        0.00523189315572381,\n        0.0170329250395298,\n        0.0009929196676239371,\n        0.03172412887215614,\n        -0.003651232924312353,\n        -0.01271923165768385,\n        0.01637970842421055,\n        0.031477633863687515,\n        -0.02219703048467636,\n        0.024575725197792053,\n        0.0028470372781157494,\n        0.02011413313448429,\n        0.0010753420647233725,\n        0.013199900276958942,\n        0.01965811476111412,\n        -0.013249199837446213,\n        -0.0057279677130281925,\n        -0.01339709758758545,\n        -0.008873882703483105,\n        0.008454837836325169,\n        -0.01747661828994751,\n        -0.0033277058973908424,\n        -0.0004444644437171519,\n        0.005062426906079054,\n        0.020446904003620148,\n        0.008393214084208012,\n        0.02328161709010601,\n        0.0046372199431061745,\n        -0.0044061290100216866,\n        0.009970792569220066,\n        0.00450472766533494,\n        0.0004991558962501585,\n        -0.025586361065506935,\n        -0.011499072425067425,\n        -0.006957370322197676,\n        -0.026301201432943344,\n        -0.008812258020043373,\n        0.026104005053639412,\n        -0.00018535398703534156,\n        0.02824852615594864,\n        0.0050716702826321125,\n        -0.001026042620651424,\n        -0.030565595254302025,\n        0.0007957222987897694,\n        0.00788173358887434,\n        -0.017686141654849052,\n        -0.026153303682804108,\n        0.0082453154027462,\n        0.015899039804935455,\n        0.007487338501960039,\n        -0.011258738115429878,\n        -0.03362215682864189,\n        0.011566859669983387,\n        -0.00833775196224451,\n        0.014567957259714603,\n        -0.0158374160528183,\n        0.01673712767660618,\n        -0.011012242175638676,\n        -0.013483371585607529,\n        0.01493770256638527,\n        0.01327384915202856,\n        0.040992408990859985,\n        0.004969990346580744,\n        -0.0002595918776933104,\n        0.01466655544936657,\n        -0.008479488082230091,\n        -0.006073063239455223,\n        -0.00141581567004323,\n        -0.0335235558450222,\n        -0.011468260549008846,\n        -0.006211717613041401,\n        0.0033831677865236998,\n        -0.018807701766490936,\n        -0.015282797627151012,\n        -0.01412426307797432,\n        0.011123165488243103,\n        0.008097417652606964,\n        -0.019584165886044502,\n        -0.016971301287412643,\n        -0.0010722607839852571,\n        0.00032160122646018863,\n        -0.010426811873912811,\n        0.008035793900489807,\n        0.03825629502534866,\n        -0.02745973691344261,\n        -0.010334375314414501,\n        -0.007961845025420189,\n        -0.02299814485013485,\n        -0.010192640125751495,\n        0.06714571267366409,\n        0.00176553288474679,\n        0.01000776793807745,\n        0.014099613763391972,\n        0.004572514444589615,\n        0.004877554252743721,\n        -0.0017331801354885101,\n        -0.024021107703447342,\n        -0.013914740644395351,\n        0.003198295133188367,\n        -0.0047450619749724865,\n        -0.020779674872756004,\n        -0.02627655304968357,\n        0.006156255956739187,\n        -0.0003564573999028653,\n        -0.0015198064502328634,\n        -0.00046372198266908526,\n        -0.025487761944532394,\n        -0.006526001263409853,\n        0.011529885232448578,\n        -0.009668834507465363,\n        0.009336063638329506,\n        0.015319772064685822,\n        0.02391018345952034,\n        0.006797147449105978,\n        0.013606620021164417,\n        0.003968597389757633,\n        0.004458509851247072,\n        -0.0007017453899607062,\n        -0.022492827847599983,\n        -0.01344639714807272,\n        -0.005157944280654192,\n        0.01637970842421055,\n        0.002600540639832616,\n        0.0010167990112677217,\n        0.022344930097460747,\n        0.010587034747004509,\n        0.011221763677895069,\n        0.008497974835336208,\n        -0.007080618757754564,\n        0.02426760457456112,\n        0.030466996133327484,\n        0.0015082518802955747,\n        -0.012380298227071762,\n        0.014728180132806301,\n        -0.008880045264959335,\n        -0.024316903203725815,\n        -0.009989280253648758,\n        -0.009003293700516224,\n        0.0008534949738532305,\n        0.027656933292746544,\n        0.01711919903755188,\n        -0.02418133057653904,\n        -0.008504137396812439,\n        -0.006048413924872875,\n        0.01127106323838234,\n        -0.01918977126479149,\n        -0.015171874314546585,\n        -0.0054568215273320675,\n        -0.009040268138051033,\n        -0.008041956461966038,\n        -0.008257640525698662,\n        -0.013606620021164417,\n        -0.008719822391867638,\n        -0.012891779653728008,\n        -0.050778329372406006,\n        -0.0020166514441370964,\n        -0.005462984088808298,\n        0.0022338766139000654,\n        -0.00706213153898716,\n        -0.009175840765237808,\n        -0.03182272985577583,\n        -0.018339356407523155,\n        -0.0031412928365170956,\n        0.019165121018886566,\n        0.014161237515509129,\n        0.012965728528797626,\n        -0.02281327359378338,\n        0.023688336834311485,\n        0.027730882167816162,\n        0.002055166522040963,\n        -0.026030056178569794,\n        0.025339864194393158,\n        -0.0013380150776356459,\n        -0.004723493941128254,\n        0.012435760349035263,\n        -0.005003883969038725,\n        0.02047155424952507,\n        0.0008242234471254051,\n        0.016823401674628258,\n        0.025118017569184303,\n        0.008078930899500847,\n        0.02371298521757126,\n        -0.012614470906555653,\n        0.01259598322212696,\n        0.010747257620096207,\n        0.007203867193311453,\n        0.0029687450733035803,\n        -0.029481010511517525,\n        0.013791492208838463,\n        -0.0036758827045559883,\n        -0.03132973611354828,\n        0.0046372199431061745,\n        -0.03216782584786415,\n        -0.008904694579541683,\n        0.023589737713336945,\n        -0.01177638117223978,\n        0.02573425881564617,\n        -0.010808882303535938,\n        -0.01177638117223978,\n        0.018080536276102066,\n        -0.04330947622656822,\n        0.02373763546347618,\n        -0.008066605776548386,\n        0.018622828647494316,\n        0.009798245504498482,\n        0.00634112861007452,\n        0.03534763306379318,\n        -0.016182510182261467,\n        -0.02010180801153183,\n        0.007672211155295372,\n        -0.05920851603150368,\n        0.02163008786737919,\n        -0.010784232057631016,\n        -0.01064249686896801,\n        0.008504137396812439,\n        -0.008990968577563763,\n        -0.01575114205479622,\n        -0.006914233323186636,\n        -0.016170185059309006,\n        -0.008769121021032333,\n        0.014740505255758762,\n        -0.002552781719714403,\n        -0.02165473811328411,\n        -0.01471585500985384,\n        -0.018536554649472237,\n        0.0027931162621825933,\n        0.028568971902132034,\n        0.003064262680709362,\n        -0.009249789640307426,\n        -0.028223875910043716,\n        -0.008054280653595924,\n        -0.008004981093108654,\n        -0.03825629502534866,\n        -0.03475603833794594,\n        -0.038626037538051605,\n        -0.009853706695139408,\n        0.009625697508454323,\n        -0.020175758749246597,\n        0.005598557181656361,\n        -0.006085388362407684,\n        0.0070374817587435246,\n        -0.03288266435265541,\n        -0.0014974676305428147,\n        -0.004452347289770842,\n        -0.028100628405809402,\n        -0.004005571827292442,\n        -0.0007853232091292739,\n        0.015036300756037235,\n        0.011295712552964687,\n        0.038946483284235,\n        0.017612192779779434,\n        0.027755532413721085,\n        0.00300109782256186,\n        -0.02100152149796486,\n        -0.009299089200794697,\n        -0.008251477964222431,\n        -0.013125951401889324,\n        -0.018622828647494316,\n        0.023663686588406563,\n        0.009699646383523941,\n        0.02427992783486843,\n        0.013138276524841785,\n        0.020138783380389214,\n        0.0024018026888370514,\n        -0.011400474235415459,\n        0.00852262508124113,\n        -0.0058327289298176765,\n        -0.023022795096039772,\n        -0.020151108503341675,\n        0.01425983663648367,\n        -0.005496877245604992,\n        -0.024119704961776733,\n        0.015233498066663742,\n        0.007931032218039036,\n        -0.0349285863339901,\n        -0.01042064931243658,\n        0.007055968977510929,\n        0.02064410224556923,\n        -0.0077153476886451244,\n        0.020348304882645607,\n        -0.016774103045463562,\n        -0.018770726397633553,\n        -0.009169678203761578,\n        0.008313102647662163,\n        -0.000665541214402765,\n        -0.013643594458699226,\n        -0.0004525526019278914,\n        0.02037295512855053,\n        0.02536451444029808,\n        -0.00838088896125555,\n        0.029111264273524284,\n        0.0020104891154915094,\n        0.006901908665895462,\n        0.01403798907995224,\n        0.01547999493777752,\n        -0.019054198637604713,\n        -0.014050314202904701,\n        0.0008665900677442551,\n        0.007456526160240173,\n        -0.0018209946574643254,\n        -0.004933015909045935,\n        -0.02990005351603031,\n        -0.022431202232837677,\n        0.018043560907244682,\n        0.006211717613041401,\n        -0.011153977364301682,\n        0.02455107495188713,\n        -0.0010599360102787614,\n        -0.030861390754580498,\n        -0.02201215922832489,\n        0.006328803952783346,\n        0.023441839963197708,\n        -0.009761271066963673,\n        0.019719738513231277,\n        -0.015060951001942158,\n        -0.015097925439476967,\n        0.00193499936722219,\n        -0.0013233793433755636,\n        0.015406046062707901,\n        -0.006033007521182299,\n        0.013187575154006481,\n        0.0170329250395298,\n        0.0015352125046774745,\n        -0.0037960498593747616,\n        -0.001717774081043899,\n        0.005509201902896166,\n        -0.008485650643706322,\n        -0.010266589000821114,\n        0.006082307081669569,\n        0.010845856741070747,\n        -0.009890681132674217,\n        -0.009939980693161488,\n        0.008787608705461025,\n        -0.012398785911500454,\n        0.021716361865401268,\n        0.015640217810869217,\n        -0.006901908665895462,\n        -0.015467669814825058,\n        -0.008584248833358288,\n        -0.010432974435389042,\n        0.023947158828377724,\n        -0.016071587800979614,\n        0.00778929702937603,\n        0.024514101445674896,\n        0.008725984953343868,\n        0.010131015442311764,\n        -0.0027191671542823315,\n        0.017242446541786194,\n        0.017365695908665657,\n        -0.0009081864263862371,\n        0.01840098202228546,\n        -0.00851029995828867,\n        0.000255162623943761,\n        -0.013791492208838463,\n        0.00041942959069274366,\n        -0.03497788682579994,\n        -0.004905284848064184,\n        -0.0005480700638145208,\n        0.022948846220970154,\n        -0.02064410224556923,\n        -0.007129918318241835,\n        -0.010137178003787994,\n        0.019719738513231277,\n        0.017525918781757355,\n        0.020693400874733925,\n        -0.018364006653428078,\n        0.0010660983389243484,\n        -0.007025157101452351,\n        0.0010460704797878861,\n        0.024477126076817513,\n        -0.017883338034152985,\n        -0.0011646971106529236,\n        -0.005308923311531544,\n        -0.009644185192883015,\n        -0.014543307945132256,\n        -0.04335877671837807,\n        -0.004347586072981358,\n        -0.005114807281643152,\n        -0.016330407932400703,\n        -0.017907988280057907,\n        -0.018881650641560555,\n        0.01602228730916977,\n        0.020077159628272057,\n        0.0072100297547876835,\n        -0.007610586937516928,\n        -0.004680356942117214,\n        -0.019497891888022423,\n        0.005740292835980654,\n        0.016059262678027153,\n        0.012941078282892704,\n        0.0060638198629021645,\n        -0.021137095987796783,\n        0.007820108905434608,\n        -0.013655919581651688,\n        -0.025931457057595253,\n        0.006821797229349613,\n        0.0033492743968963623,\n        -0.002526591531932354,\n        -0.013779168017208576,\n        0.0033061373978853226,\n        0.021334292367100716,\n        -0.015110249631106853,\n        -0.017698464915156364,\n        0.0029040395747870207,\n        0.01408728864043951,\n        0.006110037676990032,\n        0.008276128210127354,\n        -0.017809389159083366,\n        0.010476111434400082,\n        -0.027607634663581848,\n        0.019251395016908646,\n        0.011560697108507156,\n        -0.02689279429614544,\n        0.027410436421632767,\n        0.01821610890328884,\n        0.014950026758015156,\n        0.011030728928744793,\n        -0.01462958101183176,\n        -0.004175038542598486,\n        -0.004923772532492876,\n        0.004289043135941029,\n        0.009983117692172527,\n        -0.011338849551975727,\n        -0.016157861799001694,\n        -0.009422337636351585,\n        -0.01686037704348564,\n        0.012102989479899406,\n        -0.004581758286803961,\n        -0.020607126876711845,\n        0.017414994537830353,\n        0.02635050192475319,\n        -0.01068563386797905,\n        -0.001600688206963241,\n        0.007370252627879381,\n        -0.01258982066065073,\n        -0.013939390890300274,\n        -0.02763228490948677,\n        -0.021802635863423347,\n        -0.015467669814825058,\n        -0.019510217010974884,\n        0.027681583538651466,\n        0.015603243373334408,\n        -0.005367466248571873,\n        -0.043703872710466385,\n        0.0015475372783839703,\n        -0.03150228410959244,\n        -0.013606620021164417,\n        -0.00852262508124113,\n        0.010494598187506199,\n        -0.009970792569220066,\n        -0.007776971906423569,\n        -0.013668243773281574,\n        0.012891779653728008,\n        0.016552256420254707,\n        0.0017162335570901632,\n        6.509054219350219e-05,\n        -0.00982905738055706,\n        -0.007857083342969418,\n        -0.006085388362407684,\n        0.004168875981122255,\n        -0.018068211153149605,\n        -0.026054704561829567,\n        -0.0007876341114751995,\n        0.0035865274257957935,\n        -0.0427425354719162,\n        -0.015258148312568665,\n        0.012780855409801006,\n        -0.016182510182261467,\n        -0.014185886830091476,\n        -0.004304449073970318,\n        -0.008085092529654503,\n        0.00791870802640915,\n        0.020964547991752625,\n        0.010562385432422161,\n        -0.013581969775259495,\n        -0.017082223668694496,\n        0.015110249631106853,\n        -0.02455107495188713,\n        -0.0045786770060658455,\n        0.003956272732466459,\n        0.03517508506774902,\n        -0.014641906134784222,\n        0.021445216611027718,\n        -0.00910805445164442,\n        0.00020143405708950013,\n        0.01408728864043951,\n        -0.014074963517487049,\n        0.010124853812158108,\n        0.001854887930676341,\n        -0.00973662082105875,\n        0.008547274395823479,\n        0.0013703678268939257,\n        0.004116495605558157,\n        -0.023072093725204468,\n        0.032192472368478775,\n        -0.02600540593266487,\n        0.012485059909522533,\n        0.01557859405875206,\n        -0.01339709758758545,\n        -0.008479488082230091,\n        -0.013335473835468292,\n        -0.0017378019401803613,\n        -0.022431202232837677,\n        -0.010174152441322803,\n        -0.019682765007019043,\n        -0.024698972702026367,\n        -0.009896843694150448,\n        0.0021337373182177544,\n        0.00394086679443717,\n        -0.01713152416050434,\n        0.008220666088163853,\n        -0.004763549659401178,\n        -0.007573612034320831,\n        0.010821206495165825,\n        -0.01566486805677414,\n        0.017883338034152985,\n        -0.00459716422483325,\n        -0.0024926981423050165,\n        0.03125578537583351,\n        -0.013742192648351192,\n        -0.006233286112546921,\n        0.016885027289390564,\n        0.005182594060897827,\n        0.011887305416166782,\n        0.015060951001942158,\n        0.23565088212490082,\n        -0.0035526342689990997,\n        -0.0044646719470620155,\n        0.03140368312597275,\n        0.011782543733716011,\n        0.012158451601862907,\n        0.02908661589026451,\n        0.006390427704900503,\n        0.007222354412078857,\n        0.02318301796913147,\n        -0.0033492743968963623,\n        -0.027213240042328835,\n        -0.0340658500790596,\n        0.014752829447388649,\n        -0.009588723070919514,\n        -0.004852904472500086,\n        -0.03307986259460449,\n        -0.002161468379199505,\n        -0.010864343494176865,\n        -0.02427992783486843,\n        -0.01119095180183649,\n        -0.016355058178305626,\n        -0.017316395416855812,\n        -0.005139457061886787,\n        0.024970119819045067,\n        0.033005911856889725,\n        -0.014407734386622906,\n        -0.005287354812026024,\n        0.020779674872756004,\n        -0.011807193979620934,\n        -0.007536637596786022,\n        -0.01208450272679329,\n        0.005148700438439846,\n        0.03322776034474373,\n        0.010494598187506199,\n        0.003392411395907402,\n        0.0042058504186570644,\n        0.0091943284496665,\n        -0.0007244693115353584,\n        -0.00034451769897714257,\n        0.013594294898211956,\n        0.02708999067544937,\n        -0.009718134067952633,\n        -0.021938210353255272,\n        -0.009379200637340546,\n        -0.01050692331045866,\n        0.011720919981598854,\n        -0.0422002412378788,\n        0.014432383701205254,\n        0.01774776540696621,\n        -0.0007772350218147039,\n        0.014789803884923458,\n        0.03929157927632332,\n        0.05373628810048103,\n        -0.014888403005897999,\n        0.014013339765369892,\n        0.01629343442618847,\n        -0.0026606242172420025,\n        -0.016170185059309006,\n        -0.010654821060597897,\n        0.005872784648090601,\n        0.027878781780600548,\n        -0.015726491808891296,\n        0.036999158561229706,\n        -0.011474423110485077,\n        0.013508020900189877,\n        -0.02960425801575184,\n        -0.0066615743562579155,\n        -0.013064326718449593,\n        0.009520936757326126,\n        -0.01891862414777279,\n        -0.006396590266376734,\n        0.012867129407823086,\n        -0.011468260549008846,\n        -0.02827317640185356,\n        -0.00701899453997612,\n        0.028741519898176193,\n        0.006593787577003241,\n        0.014001014642417431,\n        0.012029040604829788,\n        -0.01739034429192543,\n        0.007203867193311453,\n        -0.021864261478185654,\n        -0.016712479293346405,\n        -0.015208848752081394,\n        -0.022147731855511665,\n        0.015553943812847137,\n        -0.015319772064685822,\n        -0.006569138262420893,\n        0.004036384169012308,\n        -0.011018403805792332,\n        -0.009360713884234428,\n        0.0018471849616616964,\n        0.008756796829402447,\n        0.015812765806913376,\n        0.010833531618118286,\n        0.009607210755348206,\n        0.005690993275493383,\n        0.0007032860303297639,\n        0.016416681930422783,\n        -0.013729868456721306,\n        -0.010907480493187904,\n        -0.008695172145962715,\n        0.010679471306502819,\n        -0.012694582343101501,\n        0.0023925588466227055,\n        -0.010925968177616596,\n        0.007129918318241835,\n        0.012441922910511494,\n        -0.012706906534731388,\n        -0.010747257620096207,\n        -0.03295661509037018,\n        0.014592606574296951,\n        -0.001231713336892426,\n        0.0042305001989007,\n        -0.007012831978499889,\n        0.020434578880667686,\n        0.013199900276958942,\n        -0.015369071625173092,\n        -0.00977359525859356,\n        0.0052349744364619255,\n        0.006575300358235836,\n        -0.025561710819602013,\n        0.009668834507465363,\n        0.01490072812885046,\n        -0.01946091651916504,\n        -0.017168497666716576,\n        -0.009311414323747158,\n        -0.006790984887629747,\n        -0.033745404332876205,\n        0.015184199437499046,\n        -0.01894327439367771,\n        0.01050692331045866,\n        -0.012121477164328098,\n        -0.014925377443432808,\n        -0.0006632303120568395,\n        0.006581462919712067,\n        -0.008091255091130733,\n        -0.02962890826165676,\n        -0.013914740644395351,\n        0.009582560509443283,\n        0.0032999750692397356,\n        -0.007419551722705364,\n        -0.008738309144973755,\n        0.012941078282892704,\n        -0.03884788602590561,\n        0.021395916119217873,\n        -0.02372531034052372,\n        -0.009631860069930553,\n        -0.021938210353255272,\n        -0.049767691642045975,\n        0.0007753092795610428,\n        -0.011443611234426498,\n        -0.0015498482389375567,\n        0.028667571023106575,\n        -0.014728180132806301,\n        -0.03396724909543991,\n        -0.0029887729324400425,\n        0.0015583215281367302,\n        -0.0008912397897802293,\n        0.0021414405200630426,\n        0.012306349352002144,\n        0.03399189934134483,\n        -0.021494515240192413,\n        -0.0038022121880203485,\n        -0.010118691250681877,\n        -0.15519434213638306,\n        0.04303833097219467,\n        0.009576397947967052,\n        -0.04863380640745163,\n        0.015726491808891296,\n        0.016885027289390564,\n        0.03450954332947731,\n        -0.0005819633952341974,\n        -0.008750634267926216,\n        0.028371773660182953,\n        0.00672319857403636,\n        -0.021765662357211113,\n        -0.040450114756822586,\n        0.012879454530775547,\n        -0.005943652708083391,\n        -0.00552768912166357,\n        -0.016626205295324326,\n        0.001549077918753028,\n        0.015221173875033855,\n        0.02834712527692318,\n        0.024255279451608658,\n        -0.0017054493073374033,\n        0.009034105576574802,\n        0.004572514444589615,\n        0.0052472990937530994,\n        -0.0022369578946381807,\n        -0.0006054576369933784,\n        0.024427827447652817,\n        0.00521032465621829,\n        -0.020607126876711845,\n        -0.020939897745847702,\n        -0.00836856383830309,\n        -0.015886714681982994,\n        0.010821206495165825,\n        -0.004483159631490707,\n        0.0007279356941580772,\n        0.0015868227928876877,\n        -0.007179217413067818,\n        -0.022246330976486206,\n        0.01630575954914093,\n        0.018955599516630173,\n        0.01965811476111412,\n        0.01271923165768385,\n        0.0019319182028993964,\n        -0.026966743171215057,\n        -0.006827959790825844,\n        0.018240759149193764,\n        0.022073782980442047,\n        -0.005740292835980654,\n        -0.015048625878989697,\n        -0.008534949272871017,\n        -0.006042251363396645,\n        0.02573425881564617,\n        0.0032722442410886288,\n        0.01544302050024271,\n        0.03312916308641434,\n        -0.007801621686667204,\n        0.037985146045684814,\n        -0.008633548393845558,\n        -0.02871686965227127,\n        0.00783243402838707,\n        -0.019682765007019043,\n        0.013064326718449593,\n        -0.004698844160884619,\n        0.00012084117042832077,\n        -0.008880045264959335,\n        -0.0061716618947684765,\n        0.010087879374623299,\n        -0.01840098202228546,\n        0.017168497666716576,\n        0.004326017573475838,\n        -0.020360630005598068,\n        -0.003959354013204575,\n        -0.023971807211637497,\n        0.007536637596786022,\n        0.010660983622074127,\n        -0.022751647979021072,\n        -0.004181201104074717,\n        -0.014013339765369892,\n        -0.008806096389889717,\n        -0.005604719743132591,\n        0.03741820529103279,\n        -0.009496286511421204,\n        0.018253084272146225,\n        -0.017686141654849052,\n        0.011794868856668472,\n        0.012109152041375637,\n        0.00040094234282150865,\n        -0.009970792569220066,\n        -0.023663686588406563,\n        0.013064326718449593,\n        -0.013298499397933483,\n        -0.005419847089797258,\n        -0.0011431286111474037,\n        0.01686037704348564,\n        0.029185213148593903,\n        0.0044615911319851875,\n        0.006871096324175596,\n        -0.0010722607839852571,\n        0.0030627220403403044,\n        0.0007988034631125629,\n        -0.0018379413522779942,\n        -0.010950617492198944,\n        0.013434072025120258,\n        0.03142833337187767,\n        0.008960156701505184,\n        -0.0026513806078583,\n        0.01894327439367771,\n        0.007561287377029657,\n        -0.017895663157105446,\n        -0.004267474636435509,\n        0.02092757262289524,\n        0.01408728864043951,\n        0.025611011311411858,\n        0.00706213153898716,\n        0.017981937155127525,\n        -0.008208340965211391,\n        -0.012257050722837448,\n        0.01783403940498829,\n        -0.03125578537583351,\n        0.04195374622941017,\n        0.005872784648090601,\n        -0.019867636263370514,\n        -0.013471046462655067,\n        0.005533851683139801,\n        -0.00035992375342175364,\n        -0.10550060123205185,\n        -0.021137095987796783,\n        0.00836856383830309,\n        0.011764056980609894,\n        0.01740266941487789,\n        0.024575725197792053,\n        0.01864747889339924,\n        -0.011844168417155743,\n        -0.006328803952783346,\n        0.011696269735693932,\n        -0.014826778322458267,\n        -0.03362215682864189,\n        -0.018635153770446777,\n        -0.004572514444589615,\n        0.01208450272679329,\n        -0.02770623378455639,\n        -0.013828466646373272,\n        -0.024760596454143524,\n        -0.012207751162350178,\n        0.016256459057331085,\n        -0.011770219542086124,\n        -0.0028578215278685093,\n        0.00370053225196898,\n        -0.014383085072040558,\n        -0.023158367723226547,\n        0.0016037693712860346,\n        -0.017772415652871132,\n        0.011985903605818748,\n        0.010346700437366962,\n        -0.0003847659972961992,\n        -0.022763973101973534,\n        -0.03796049579977989,\n        0.019177446141839027,\n        -0.015911364927887917,\n        -0.012109152041375637,\n        -0.00021972873946651816,\n        -0.04663718119263649,\n        0.0010121772065758705,\n        0.00302266632206738,\n        -0.019695090129971504,\n        0.017636841163039207,\n        0.019855313003063202,\n        0.0003967056982219219,\n        -0.04136215150356293,\n        -0.003947028890252113,\n        0.022480502724647522,\n        -0.022073782980442047,\n        0.01803123578429222,\n        0.005860459990799427,\n        -0.020718051120638847,\n        -0.03616107255220413,\n        -0.01390241552144289,\n        -0.02236957848072052,\n        0.014839103445410728,\n        0.026153303682804108,\n        0.0014011799357831478,\n        -0.0017732358537614346,\n        0.02262840047478676,\n        -0.014876077882945538,\n        0.007006669882684946,\n        0.015208848752081394,\n        0.020138783380389214,\n        -0.023072093725204468,\n        0.036777313798666,\n        0.01674945279955864,\n        -0.00450472766533494,\n        -0.0018518067663535476,\n        0.014962351880967617,\n        0.005857378710061312,\n        0.00020201178267598152,\n        -0.003145914524793625,\n        0.011086191050708294,\n        0.004615651443600655,\n        -0.0014812913723289967,\n        -0.004997721407562494,\n        0.012059852480888367,\n        -0.0029102021362632513,\n        -0.00969348382204771,\n        -0.007727672811597586,\n        -0.02689279429614544,\n        -0.009638022631406784,\n        -0.013631269335746765,\n        -0.008904694579541683,\n        0.0033369495067745447,\n        0.010075554251670837,\n        -0.0032968937885016203,\n        -0.0075859371572732925,\n        0.0004926083493046463,\n        -0.005108644720166922,\n        -0.0052935173735022545,\n        -0.01132652536034584,\n        0.029850754886865616,\n        0.02018808200955391,\n        -0.0119057921692729,\n        -0.013976365327835083,\n        0.000644743035081774,\n        -0.01637970842421055,\n        0.010186477564275265,\n        0.020755024626851082,\n        0.014185886830091476,\n        -0.01140663679689169,\n        -0.0028978772461414337,\n        -0.04123890399932861,\n        0.029012665152549744,\n        -0.018277732655405998,\n        -0.01891862414777279,\n        -0.0007818568265065551,\n        -0.016157861799001694,\n        0.0037344256415963173,\n        -0.02869221940636635,\n        -0.0060545760206878185,\n        -0.01136349979788065,\n        -0.007955682463943958,\n        0.012281700037419796,\n        0.027016041800379753,\n        0.006390427704900503,\n        -0.014641906134784222,\n        0.0035156598314642906,\n        0.03034374862909317,\n        0.008799933828413486,\n        0.010944454930722713,\n        0.013015028089284897,\n        -0.013939390890300274,\n        -0.0254384633153677,\n        -0.008571923710405827,\n        -0.00334003078751266,\n        0.0032414321321994066,\n        0.01493770256638527,\n        -0.031748779118061066,\n        0.030910691246390343,\n        -0.017267096787691116,\n        -0.00706213153898716,\n        0.024760596454143524,\n        -0.04422151297330856,\n        0.008122067898511887,\n        0.026030056178569794,\n        0.007943357340991497,\n        -0.011061540804803371,\n        0.013520346023142338,\n        0.04084450751543045,\n        0.019337669014930725,\n        -0.0030581001192331314,\n        -0.029234513640403748,\n        -0.033745404332876205,\n        0.02526591531932354,\n        -0.007043644320219755,\n        -0.0029933948535472155,\n        -0.0064520519226789474,\n        -0.0008273046696558595,\n        0.010913643054664135,\n        0.0322171226143837,\n        -0.004951503127813339,\n        0.016416681930422783,\n        0.01840098202228546,\n        -0.0024495613761246204,\n        -0.020668750628829002,\n        -0.006624599918723106,\n        -0.02020040713250637,\n        -0.025758909061551094,\n        -0.004997721407562494,\n        -0.0009636481991037726,\n        -0.021851936355233192,\n        0.04594699293375015,\n        0.01992926187813282,\n        0.028544321656227112,\n        0.005875865928828716,\n        0.02446480095386505,\n        0.007191542536020279,\n        0.004227418918162584,\n        0.0017948043532669544,\n        0.003303056349977851,\n        -0.019867636263370514,\n        -0.002124493708834052,\n        -0.005040858406573534,\n        0.003281487850472331,\n        -0.006193230394273996,\n        0.00034740634146146476,\n        0.015615568496286869,\n        0.020939897745847702,\n        0.004655707161873579,\n        -0.021432891488075256,\n        0.04503495246171951,\n        0.02536451444029808,\n        -0.0018271571025252342,\n        -0.02329394221305847,\n        -0.013544995337724686,\n        0.01740266941487789,\n        0.025315215811133385,\n        -0.02183961123228073,\n        0.012220075353980064,\n        -0.014469358138740063,\n        0.021617764607071877,\n        -0.038354892283678055,\n        -0.009169678203761578,\n        -0.01339709758758545,\n        0.02871686965227127,\n        0.0018795375945046544,\n        0.009471637196838856,\n        0.023503463715314865,\n        0.012515871785581112,\n        0.015270472504198551,\n        0.021408241242170334,\n        -0.002822387730702758,\n        0.022677699103951454,\n        -0.008023468777537346,\n        -0.015627892687916756,\n        -0.015603243373334408,\n        0.01666317880153656,\n        -0.003053478430956602,\n        -0.03719635680317879,\n        -0.003959354013204575,\n        0.023022795096039772,\n        0.01132652536034584,\n        0.029209863394498825,\n        0.01140663679689169,\n        0.02084129862487316,\n        0.0020382199436426163,\n        -0.012941078282892704,\n        -0.0009482421446591616,\n        -0.010402162559330463,\n        -0.01485142856836319,\n        0.04742597043514252,\n        -0.015406046062707901,\n        -0.001824075821787119,\n        0.01584973931312561,\n        -0.011924279853701591,\n        0.01059319730848074,\n        -0.0026606242172420025,\n        -0.010112528689205647,\n        -0.0023478814400732517,\n        0.009847545064985752,\n        0.007277816068381071,\n        0.013335473835468292,\n        0.005157944280654192,\n        0.007764647249132395,\n        -0.029407061636447906,\n        -0.009483962319791317,\n        -0.021494515240192413,\n        -0.0053982785902917385,\n        0.026596998795866966,\n        -0.016995949670672417,\n        0.032365020364522934,\n        0.007524312939494848,\n        0.004905284848064184,\n        0.025339864194393158,\n        0.020865948870778084,\n        0.0335235558450222,\n        0.021309642121195793,\n        -0.025832857936620712,\n        -0.02790343016386032,\n        -0.0024911577347666025,\n        0.002942554885521531,\n        0.0035433906596153975,\n        -0.0036974509712308645,\n        -0.0362350195646286,\n        -0.01711919903755188,\n        -0.0004937637713737786,\n        -0.02861827053129673,\n        -0.01640435867011547,\n        -0.01631808467209339,\n        0.010888993740081787,\n        0.011018403805792332,\n        0.0033492743968963623,\n        0.008775283582508564,\n        0.01390241552144289,\n        -0.024920819327235222,\n        0.0022908791434019804,\n        0.033646803349256516,\n        0.00023744568170513958,\n        -0.009952305816113949,\n        -0.03776330128312111,\n        0.008479488082230091,\n        0.012614470906555653,\n        -0.020335981622338295,\n        -0.021038496866822243,\n        0.023959482088685036,\n        -0.010377512313425541,\n        -0.005065507721155882,\n        -0.016194835305213928,\n        -0.0007009750697761774,\n        0.0021861179266124964,\n        0.014013339765369892,\n        0.024785246700048447,\n        -0.020631777122616768,\n        -0.014789803884923458,\n        -0.007345602847635746,\n        -0.008152879774570465,\n        -0.00044639018597081304,\n        -0.0012193885631859303,\n        -0.013471046462655067\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"ada\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 4,\n    \"total_tokens\": 4\n  }\n}","__computed__.cumulative_token_count.prompt":4,"__computed__.cumulative_token_count.total":4},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"agent":"azureml-ai-monitoring/0.1.0b4","modelversion":"default","contentrange":"bytes 0-49572/49573","correlationid":"e99887ac-4672-4781-b3c6-be7b2f6b1ead","xrequestid":"e99887ac-4672-4781-b3c6-be7b2f6b1ead","collectdatatype":"pandas.core.frame.DataFrame"}
{"specversion":"1.0","id":"a04da108-753d-4368-9fc2-78e17f1e917c","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-07T04:22:08Z","data":[{"name":"Prompt_variants","context":{"trace_id":"0x0ef89cf7e7aa594e6c0e2281bb5c44b9","span_id":"0x178e05b251c84796","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xacab7ff109688c71","start_time":"2024-02-07T04:22:05.617198Z","end_time":"2024-02-07T04:22:05.619787Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"Tool","function":"render_template_jinja2","node_name":"Prompt_variants","line_run_id":"f232dcce-ac1f-4b3e-a6a1-ab45ba364aaf","inputs":"{\n  \"template\": \"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n {{contexts}} \\n\\n chat history: \\n{% for item in chat_history %} user: \\n{{ item.inputs.question }} \\nassistant: \\n{{ item.outputs.output }} \\n{% endfor %}\\nuser: {{question}} \\nassistant:\",\n  \"chat_history\": [],\n  \"contexts\": \"Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n  \"question\": \"how are you\"\n}","output":"\"system: \\nYou are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\\n\\n user: \\n Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\nuser: how are you \\nassistant:\""},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"contentrange":"bytes 0-9159/9160","correlationid":"cc3d67b7-c86f-4a4b-9a77-b0017a2ab2ad","xrequestid":"cc3d67b7-c86f-4a4b-9a77-b0017a2ab2ad","modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4"}
{"specversion":"1.0","id":"63aef46c-7e12-460c-b0a9-d742466d32c1","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-07T04:22:08Z","data":[{"name":"search","context":{"trace_id":"0x0ef89cf7e7aa594e6c0e2281bb5c44b9","span_id":"0x003127715ac5dda4","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0xc58e0c1975f63a4f","start_time":"2024-02-07T04:22:04.579491Z","end_time":"2024-02-07T04:22:05.514894Z","status":{"status_code":"UNSET"},"attributes":{"span_type":"Retrieval","retrieval.query":"How are you?","retrieval.documents":"[\n    {\n        \"document.content\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\",\n        \"document.score\": 0.5357727408409119,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf13\",\n            \"chunk_hash\": \"aa4f697e49dd50753e9f96cae1144c6584c392af82c3ffed6e73f5de0079fb49\",\n            \"mtime\": null,\n            \"page_number\": 13,\n            \"stats\": {\n                \"tiktokens\": 304,\n                \"chars\": 835,\n                \"lines\": 113\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\",\n        \"document.score\": 0.5490214824676514,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf14\",\n            \"chunk_hash\": \"3df33b21080f9572d9759494e760175b113ca72e16897dedc7ed1db8c0cbeb73\",\n            \"mtime\": null,\n            \"page_number\": 14,\n            \"stats\": {\n                \"tiktokens\": 291,\n                \"chars\": 838,\n                \"lines\": 113\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    },\n    {\n        \"document.content\": \"Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\",\n        \"document.score\": 0.5668976306915283,\n        \"document.metadata\": {\n            \"source_doc_id\": \"1706.03762.pdf12\",\n            \"chunk_hash\": \"2ab3047247294a730f42d2e9fb421bfa7bcf724d98adf2714c6566d1d3199abd\",\n            \"mtime\": null,\n            \"page_number\": 12,\n            \"stats\": {\n                \"tiktokens\": 254,\n                \"chars\": 833,\n                \"lines\": 73\n            },\n            \"source\": {\n                \"filename\": \"1706.03762.pdf\",\n                \"url\": \"azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\",\n                \"mtime\": 1706775620.0\n            }\n        }\n    }\n]"},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"modelversion":"default","collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-6117/6118","correlationid":"bfd0cb9a-d911-4f21-880a-ac3b9d04ccd0","xrequestid":"bfd0cb9a-d911-4f21-880a-ac3b9d04ccd0"}
{"specversion":"1.0","id":"24f19827-7be9-425e-ad8a-446bb980e70f","source":"/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourceGroups/promptflow/providers/Microsoft.MachineLearningServices/workspaces/pf-xp/onlineEndpoints/pf-tst-0130/deployments/brown","type":"azureml.inference.tracing_spans","datacontenttype":"application/json","time":"2024-02-07T04:22:08Z","data":[{"name":"openai.resources.chat.completions.Completions.create","context":{"trace_id":"0x0ef89cf7e7aa594e6c0e2281bb5c44b9","span_id":"0x6e887ca01c2de4b7","trace_state":"[]"},"kind":"SpanKind.INTERNAL","parent_id":"0x86ba86e50e7dc354","start_time":"2024-02-07T04:22:05.624621Z","end_time":"2024-02-07T04:22:06.681226Z","status":{"status_code":"OK"},"attributes":{"framework":"promptflow","span_type":"LLM","function":"openai.resources.chat.completions.Completions.create","node_name":"answer_the_question_with_context","line_run_id":"f232dcce-ac1f-4b3e-a6a1-ab45ba364aaf","inputs":"{\n  \"model\": \"gpt-35-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are an AI assistant that helps users answer questions given a specific context and conversation history. You will be given a context and chat history, and then asked a question based on that context and history. Your answer should be as precise as possible, and should only come from the context.\\nPlease add citation after each sentence when possible in a form \\\"(Source: citation)\\\".\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Content: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word \\u2018its\\u2019 for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003eFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf\\n\\nContent: Title: 1706.03762.pdfAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n\u003cEOS\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\n\u003cpad\u003e\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\nSource: azureml:\/\/locations\/centralus\/workspaces\/e96a5813-c919-42fc-b816-908261dd06ab\/data\/vector-index-input-1706775617472\/versions\/1\/1706.03762.pdf \\n\\n chat history: \\nuser: how are you \\nassistant:\"\n    }\n  ],\n  \"temperature\": 0.0,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null,\n  \"max_tokens\": 1000,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"logit_bias\": {},\n  \"user\": \"\",\n  \"response_format\": null\n}","output":"{\n  \"id\": \"chatcmpl-8pTjijPmRzndFhUrtEDHhvLjKhX32\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"I'm an AI language model, so I don't have feelings, but I'm here to help you with any questions you have. Do you have a specific question in mind?\",\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707279726,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 36,\n    \"prompt_tokens\": 1109,\n    \"total_tokens\": 1145\n  }\n}","__computed__.cumulative_token_count.completion":36,"__computed__.cumulative_token_count.prompt":1109,"__computed__.cumulative_token_count.total":1145,"llm.token_count.completion":36,"llm.token_count.prompt":1109,"llm.token_count.total":1145},"events":[],"links":[],"resource":{"attributes":{"service.name":"promptflow"},"schema_url":""}}],"collectdatatype":"pandas.core.frame.DataFrame","agent":"azureml-ai-monitoring/0.1.0b4","contentrange":"bytes 0-6229/6230","correlationid":"1303ba79-3a31-45f8-8dbf-9e02e834c29e","xrequestid":"1303ba79-3a31-45f8-8dbf-9e02e834c29e","modelversion":"default"}
