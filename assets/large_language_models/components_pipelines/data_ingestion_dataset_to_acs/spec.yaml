$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
type: pipeline

tags:
    Preview: ""

version: 0.0.50
name: llm_ingest_dataset_to_acs
display_name: LLM - Dataset to ACS Pipeline (With Test Data Generation and Auto Prompt)
is_deterministic: false

description: Single job pipeline to chunk data from AzureML data asset, and create ACS embeddings index

settings:
   default_compute: serverless

inputs:
   # openai settings
   openai_api_type:
     type: string
     default: "azure"
     description: "Type of OpenAI endpoint hosting model. Defaults to azure for AOAI endpoints."
   openai_api_version:
     type: string
     default: "2023-03-15-preview"
     description: "Version of OpenAI API to use for communicating with LLM."
      # llm_model config
   llm_config:
     type: string
     default: '{"type": "azure_open_ai", "model_name": "gpt-35-turbo", "deployment_name": "gpt-35-turbo", "temperature": 0, "max_tokens": 2000}'
     description: "JSON describing the LLM provider and model details to use for prompt generation."
   llm_connection:
     type: string
     optional: true
     description: "Azure OpenAI workspace connection ARM ID"
   acs_config:
     type: string
     description: "JSON describing the acs index to create or update."
   acs_connection:
     type: string
     optional: true
     description: "Azure Cognitive Search workspace connection ARM ID"
   # register settings
   register_outputs:
     type: string
     enum:
      - "True"
      - "False"
     default: "True"
     description: "Boolean to determine whether generated data should be saved as an AzureML Dataset"
   embeddings_dataset_name:
     type: string
     optional: true
     default: "EmbeddingsOutput"
     description: "Name of the vector index"
   # compute settings
   serverless_instance_count:
     type: integer
     default: 1
     optional: true
     description: "Instance count to use for the serverless compute"
   serverless_instance_type:
     type: string
     default: "Standard_E8s_v3"
     optional: true
     description: "The Instance Type to be used for the serverless compute"
   # data to import
   input_data:
     type: uri_folder
     mode: rw_mount
     description: "Input AzureML data asset UriFolder to bring in data from."
   # Data Chunker
   chunk_size:
     type: integer
     default: 1024
     description: "Chunk size (by token) to pass into the text splitter before performing embeddings"
   chunk_overlap:
     type: integer
     default: 0
     description: "Overlap of content (by token) between the chunks"
   input_glob:
     type: string
     optional: true
     description: "Glob pattern to filter files from the input folder. e.g. 'articles/**/*''"
   max_sample_files:
     type: integer
     default: -1
     optional: true
     description: "Number of files read in during QA test data generation"
   data_source_url:
     type: string
     description: "The url which can be appended to file names to form citation links for documents"
   document_path_replacement_regex:
     type: string
     optional: true
     description: "A JSON string with two fields, 'match_pattern' and 'replacement_pattern' to be used with re.sub on the source url. e.g. '{\"match_pattern\": \"(.*)/articles/(.*)(\\\\.[^.]+)$\", \"replacement_pattern\": \"\\\\1/\\\\2\"}' would remove '/articles' from the middle of the url."
   # data generation component
   dataset_size:
     type: integer
     default: 20
     optional: true
     description: "Number of question/answer pairs which data generator should produce for model evaluation"
   # Auto Prompt
   task_type_for_prompt:
    type: string
    enum:
      - arithmetic
      - multiple_choice
      - abstractive
    default: abstractive
    optional: true
    description: "Task type for the prompt. Options: arithmetic, multiple_choice, abstractive"
   text_keys:
    type: string
    description: "Name of key(s) for Input texts in test data. (Pass comma separated key names in case of multiple keys)"
    optional: true
    default: question
   answer_key:
    type: string
    optional: true
    default: answer
    description: "Name of key for labels in the test data"
   context_key:
    type: string
    optional: true
    default: context
    description: "Key for the context in the test data, required only in case of Multiple choice QnA"
   prediction_temperature:
    type: number
    optional: true
    default: 0.0
    description: "Temperature setting passed in to the completion model in AutoPrompt generation"
   prediction_max_tokens:
    type: integer
    optional: true
    default: 500
    description: "The maximum token count that can be used for the response."
   prediction_logprobs:
    type: integer
    optional: true
    default: 1
    description: "Log probability setting passed into the completion model in AutoPrompt generation."
   meta_prompts:
    type: integer
    optional: true
    default: 10
    description: "Number of prompts to generate for each meta prompt"
   meta_prompts_best_of:
    type: integer
    optional: true
    default: 100
    description: "Number of prompts to generate for each meta prompt"
   primary_metric:
    type: string
    optional: False
    default: gpt_similarity
    enum:
      - f1_score
      - bert_f1
      - bert_precision
      - bert_recall
      - exact_match
      - gpt_similarity
    description: "The primary metric used to determine the best prompt during AutoPrompt generation"
   top_k:
    type: integer
    optional: True
    default: 3
    description: "Number of best prompts outputted by autoprompt generation"
   # Embeddings components
   embeddings_container:
    type: uri_folder
    optional: true
    mode: direct
    description: "Folder to contain generated embeddings. Should be parent folder of the 'embeddings' output path used for for this component. Will compare input data to existing embeddings and only embed changed/new data, reusing existing chunks."
   embeddings_model:
    type: string
    default: "azure_open_ai://deployment/text-embedding-ada-002/model/text-embedding-ada-002"
    description: "The model to use to embed data. E.g. 'hugging_face://model/sentence-transformers/all-mpnet-base-v2' or 'azure_open_ai://deployment/{deployment_name}/model/{model_name}'"
   embedding_connection:
    type: string
    optional: true
    description: "Azure OpenAI workspace connection ARM ID for embeddings"

outputs:
   acs_index:
     type: uri_folder
     description: "Folder containing the ACS MLIndex. Deserialized using azureml.rag.mlindex.MLIndex(uri)."
   data_generation_output:
     type: uri_folder
     description: "Folder containing the QA Generation test dataset."
   best_prompt:
     type: uri_file
     description: "File containing the best prompts generated."
#defaults:
#  compute: azureml:cpu-cluster
jobs:
  #############
  validate_deployments_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_validate_deployments:0.0.31'
    identity:
      type: user_identity
    inputs:
      llm_config: ${{parent.inputs.llm_config}}
      embeddings_model: ${{parent.inputs.embeddings_model}}
      check_embeddings: "True"
      check_completion: "True"
      check_acs: "True"
      acs_config: ${{parent.inputs.acs_config}}
    outputs:
      output_data:
        type: uri_file
    environment_variables:
       AZUREML_WORKSPACE_CONNECTION_ID_AOAI_EMBEDDING : ${{parent.inputs.embedding_connection}}
       AZUREML_WORKSPACE_CONNECTION_ID_AOAI_COMPLETION: ${{parent.inputs.llm_connection}}
       AZUREML_WORKSPACE_CONNECTION_ID_ACS : ${{parent.inputs.acs_connection}}
  ############
  data_chunking_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_crack_and_chunk:0.0.30'
    inputs:
      input_data: ${{parent.inputs.input_data}}
      input_glob: ${{parent.inputs.input_glob}}
      chunk_size: ${{parent.inputs.chunk_size}}
      chunk_overlap: ${{parent.inputs.chunk_overlap}}
      data_source_url: ${{parent.inputs.data_source_url}}
      document_path_replacement_regex: ${{parent.inputs.document_path_replacement_regex}}
      max_sample_files: ${{parent.inputs.max_sample_files}}
    outputs:
      output_chunks:
        type: uri_folder
    environment_variables:
       AZUREML_WORKSPACE_CONNECTION_ID_AOAI : ${{parent.inputs.llm_connection}}
  ############
  data_generation_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_qa_data_generation:0.0.28'
    inputs:
      openai_api_version: ${{parent.inputs.openai_api_version}}
      openai_api_type: ${{parent.inputs.openai_api_type}}
      input_data: ${{parent.jobs.data_chunking_job.outputs.output_chunks}}
      llm_config: ${{parent.inputs.llm_config}}
      dataset_size: ${{parent.inputs.dataset_size}}
      deployment_validation: ${{parent.jobs.validate_deployments_job.outputs.output_data}}
    outputs:
      output_data: ${{parent.outputs.data_generation_output}}
    environment_variables:
       AZUREML_WORKSPACE_CONNECTION_ID_AOAI : ${{parent.inputs.llm_connection}}
  ############
  register_qa_data_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_register_qa_data_asset:0.0.21'
    inputs:
      storage_uri: ${{parent.jobs.data_generation_job.outputs.output_data}}
      register_output: ${{parent.inputs.register_outputs}}
      asset_name: ${{parent.inputs.embeddings_dataset_name}}
  ############
  hard_prompt_sweep_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: azureml:llm_autoprompt_qna:0.0.24
    inputs:
      openai_api_version: ${{parent.inputs.openai_api_version}}
      openai_api_type: ${{parent.inputs.openai_api_type}}
      llm_config: ${{parent.inputs.llm_config}}
      task_type: ${{parent.inputs.task_type_for_prompt}}
      text_keys:  ${{parent.inputs.text_keys}}
      answer_key: ${{parent.inputs.answer_key}}
      context_key: ${{parent.inputs.context_key}}
      prediction_temperature: ${{parent.inputs.prediction_temperature}}
      prediction_max_tokens: ${{parent.inputs.prediction_max_tokens}}
      primary_metric: ${{parent.inputs.primary_metric}}
      n_prompts: ${{parent.inputs.meta_prompts}}
      prediction_logprobs: ${{parent.inputs.prediction_logprobs}}
      dev_data: ${{parent.jobs.data_generation_job.outputs.output_data}}
      test_data: ${{parent.jobs.data_generation_job.outputs.output_data}}
      best_of: ${{parent.inputs.meta_prompts_best_of}}
      top_k: ${{parent.inputs.top_k}}
    outputs:
      best_prompt: ${{parent.outputs.best_prompt}}
    environment_variables:
       AZUREML_WORKSPACE_CONNECTION_ID_AOAI : ${{parent.inputs.llm_connection}}
  ############
  register_autoprompt_data_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_register_autoprompt_data_asset:0.0.21'
    inputs:
      storage_uri: ${{parent.jobs.hard_prompt_sweep_job.outputs.best_prompt}}
      register_output: ${{parent.inputs.register_outputs}}
      asset_name: ${{parent.inputs.embeddings_dataset_name}}
  ############
  promptflow_creation_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_create_promptflow:0.0.38'
    inputs:
      best_prompts: ${{parent.jobs.hard_prompt_sweep_job.outputs.best_prompt}}
      mlindex_asset_id: ${{parent.jobs.register_mlindex_asset_job.outputs.asset_id}}
      mlindex_name: ${{parent.inputs.embeddings_dataset_name}}
      llm_connection_name: ${{parent.inputs.llm_connection}}
      llm_config: ${{parent.inputs.llm_config}}
      embedding_connection: ${{parent.inputs.embedding_connection}}
      embeddings_model: ${{parent.inputs.embeddings_model}}
  ############
  embeddings_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_generate_embeddings:0.0.24'
    inputs:
      chunks_source:
        type: uri_folder
        path: ${{parent.jobs.data_chunking_job.outputs.output_chunks}}
      embeddings_container: ${{parent.inputs.embeddings_container}}
      embeddings_model: ${{parent.inputs.embeddings_model}}
      deployment_validation: ${{parent.jobs.validate_deployments_job.outputs.output_data}}
    outputs:
      embeddings:
        type: uri_folder
    environment_variables:
       AZUREML_WORKSPACE_CONNECTION_ID_AOAI : ${{parent.inputs.embedding_connection}}
  ############
  # embeddings_parallel_job:
  #   type: command
  #   resources:
  #     instance_count: ${{parent.inputs.serverless_instance_count}}
  #     instance_type: ${{parent.inputs.serverless_instance_type}}
  #     properties:
  #       compute_specification:
  #         automatic: true
  #   retry_settings:
  #     timeout: 3600
  #     max_retries: 3
  #   mini_batch_size: "3"
  #   mini_batch_error_threshold: 0
  #   logging_level: "INFO"
  #   component: 'azureml:llm_rag_generate_embeddings_parallel:0.0.10'
  #   inputs:
  #     chunks_source:
  #       type: uri_folder
  #       path: ${{parent.jobs.data_chunking_job.outputs.output_chunks}}
  #     embeddings_container: ${{parent.inputs.embeddings_container}}
  #     embeddings_model: ${{parent.inputs.embeddings_model}}
  #     deployment_validation: ${{parent.jobs.validate_deployments_job.outputs.output_data}}
  #   outputs:
  #     embeddings:
  #       type: uri_folder
  #   environment_variables:
  #      AZUREML_WORKSPACE_CONNECTION_ID_AOAI : ${{parent.inputs.embedding_connection}}
  ############
  create_acs_index_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_update_acs_index:0.0.28'
    inputs:
      embeddings:
        type: uri_folder
        path: ${{parent.jobs.embeddings_job.outputs.embeddings}}
      acs_config: ${{parent.inputs.acs_config}}
    outputs:
      index: ${{parent.outputs.acs_index}}
    environment_variables:
       AZUREML_WORKSPACE_CONNECTION_ID_AOAI : ${{parent.inputs.embedding_connection}}
       AZUREML_WORKSPACE_CONNECTION_ID_ACS : ${{parent.inputs.acs_connection}}
  ############
  register_mlindex_asset_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_register_mlindex_asset:0.0.28'
    inputs:
      storage_uri: ${{parent.jobs.create_acs_index_job.outputs.index}}
      asset_name: ${{parent.inputs.embeddings_dataset_name}}
    outputs:
      asset_id:
        type: uri_file
