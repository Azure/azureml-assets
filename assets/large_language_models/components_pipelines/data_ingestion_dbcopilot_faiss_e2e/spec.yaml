$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
type: pipeline

name: llm_ingest_dbcopilot_faiss_e2e
version: 0.0.4
display_name: Data Ingestion for DB Data Output to FAISS E2E Deployment
description: Single job pipeline to chunk data from AzureML DB Datastore and create faiss embeddings index

settings:
  default_compute: serverless

inputs:
  db_datastore:
    type: string
    description: "database datastore uri in the format of 'azureml://datastores/{datastore_name}'"
  sample_data:
    type: uri_folder
    description: "Sample data to be used for data ingestion. format: 'azureml:samples-test:1'"
    optional: true
    # path: "azureml:samples-test:1"
  # data ingest setting
  embeddings_model:
    type: string
    description: "The model used to generate embeddings. 'azure_open_ai://endpoint/{endpoint_name}/deployment/{deployment_name}/model/{model_name}'"
  chat_aoai_deployment_name:
    type: string
    description: "The name of the chat AOAI deployment"
    optional: true
  embedding_aoai_deployment_name:
    type: string
    description: "The name of the embedding AOAI deployment"
  # grounding settings
  max_tables:
    type: integer
    optional: true
  max_columns:
    type: integer
    optional: true
  max_rows:
    type: integer
    optional: true
  max_sampling_rows:
    type: integer
    optional: true
  max_text_length:
    type: integer
    optional: true
  selected_tables:
    type: string
    optional: true
    description: 'The list of tables to be ingested. If not specified, all tables will be ingested. Format: ["table1","table2","table3"]'
  column_settings:
    type: string
    optional: true
  # copilot settings
  tools:
    type: string
    optional: true
    description: 'The name of the tools for dbcopilot. Supported tools: "tsql", "python". Format: ["tsql", "python"]'
  # deploy settings
  endpoint_name:
    type: string
    description: "The name of the endpoint"
  deployment_name:
    type: string
    description: "The name of the deployment"
    default: "blue"
  mir_environment:
    type: string
    description: "The name of the mir environment. Format: azureml://registries/{registry_name}/environments/llm-dbcopilot-mir"
  # compute settings
  serverless_instance_count:
    type: integer
    default: 1
    optional: true
  serverless_instance_type:
    type: string
    default: "Standard_DS3_v2"
    optional: true
  embedding_connection:
    type: string
    optional: true
    description: "Azure OpenAI workspace connection ARM ID for embeddings"
  llm_connection:
    type: string
    optional: true
    description: "Azure OpenAI workspace connection ARM ID for llm"
  temperature:
    type: number
    default: 0.0
    optional: true
  top_p:
    type: number
    default: 0.0
    optional: true
  include_builtin_examples:
    type: boolean
    default: true
    optional: true
  knowledge_pieces:
    type: string
    optional: true
    description: "The list of knowledge pieces to be used for grounding."
outputs:
  grounding_index:
    type: uri_folder
  db_context:
    type: uri_folder
jobs:
  #########################################
  db_meta_loading_generator:
    type: command
    component: "azureml:llm_dbcopilot_grounding:0.0.28"
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    inputs:
      asset_uri: ${{parent.inputs.db_datastore}}
      max_tables: ${{parent.inputs.max_tables}}
      max_columns: ${{parent.inputs.max_columns}}
      max_rows: ${{parent.inputs.max_rows}}
      max_sampling_rows: ${{parent.inputs.max_sampling_rows}}
      max_text_length: ${{parent.inputs.max_text_length}}
      selected_tables: ${{parent.inputs.selected_tables}}
      column_settings: ${{parent.inputs.column_settings}}
    outputs:
      output_chunk_file:
        type: uri_folder
      output_grounding_context_file:
        type: uri_folder
        path: ${{parent.outputs.db_context}}
  #########################################
  generate_meta_embeddings:
    type: command
    component: "azureml:llm_rag_generate_embeddings:0.0.24"
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    inputs:
      chunks_source:
        type: uri_folder
        path: ${{parent.jobs.db_meta_loading_generator.outputs.output_chunk_file}}
      embeddings_model: ${{parent.inputs.embeddings_model}}
    outputs:
      embeddings:
        type: uri_folder
        mode: upload
    environment_variables:
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI: ${{parent.inputs.embedding_connection}}
  #########################################
  create_meta_faiss_index_job:
    type: command
    component: "azureml:llm_rag_create_faiss_index:0.0.29"
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    inputs:
      embeddings:
        type: uri_folder
        path: ${{parent.jobs.generate_meta_embeddings.outputs.embeddings}}
    outputs:
      index:
        type: uri_folder
        path: ${{parent.outputs.grounding_index}}
    environment_variables:
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI: ${{parent.inputs.embedding_connection}}
  #########################################
  db_sample_loading_generator:
    type: command
    component: "azureml:llm_dbcopilot_grounding_ground_samples:0.0.4"
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    inputs:
      sample_folder:
        type: uri_folder
        path: ${{parent.inputs.sample_data}}
      include_builtin: ${{parent.inputs.include_builtin_examples}}
      tools: ${{parent.inputs.tools}}
      grounding_context: ${{parent.jobs.db_meta_loading_generator.outputs.output_grounding_context_file}}
    outputs:
      output_chunk_file:
        type: uri_folder
  #########################################
  generate_sample_embeddings:
    type: command
    component: "azureml:llm_rag_generate_embeddings:0.0.24"
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    inputs:
      chunks_source:
        type: uri_file
        path: ${{parent.jobs.db_sample_loading_generator.outputs.output_chunk_file}}
      embeddings_model: ${{parent.inputs.embeddings_model}}
    outputs:
      embeddings:
        type: uri_folder
        mode: upload
    environment_variables:
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI: ${{parent.inputs.embedding_connection}}
  #########################################
  create_sample_faiss_index_job:
    type: command
    component: "azureml:llm_rag_create_faiss_index:0.0.29"
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    inputs:
      embeddings:
        type: uri_folder
        path: ${{parent.jobs.generate_sample_embeddings.outputs.embeddings}}
    outputs:
      index:
        type: uri_folder
  #########################################
  endpoint_deployment_job:
    type: command
    component: "azureml:llm_dbcopilot_deploy_endpoint:0.0.4"
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    identity:
      type: user_identity
    inputs:
      grounding_embedding_uri:
        type: uri_folder
        path: ${{parent.jobs.create_meta_faiss_index_job.outputs.index}}
      example_embedding_uri:
        type: uri_folder
        path: ${{parent.jobs.create_sample_faiss_index_job.outputs.index}}
      db_context_uri:
        type: uri_file
        path: ${{parent.jobs.db_meta_loading_generator.outputs.output_grounding_context_file}}
      endpoint_name: ${{parent.inputs.endpoint_name}}
      deployment_name: ${{parent.inputs.deployment_name}}
      asset_uri: ${{parent.inputs.db_datastore}}
      embedding_aoai_deployment_name: ${{parent.inputs.embedding_aoai_deployment_name}}
      chat_aoai_deployment_name: ${{parent.inputs.chat_aoai_deployment_name}}
      mir_environment: ${{parent.inputs.mir_environment}}
      selected_tables: ${{parent.inputs.selected_tables}}
      max_tables: ${{parent.inputs.max_tables}}
      max_rows: ${{parent.inputs.max_rows}}
      max_columns: ${{parent.inputs.max_columns}}
      max_text_length: ${{parent.inputs.max_text_length}}
      tools: ${{parent.inputs.tools}}
      temperature: ${{parent.inputs.temperature}}
      top_p: ${{parent.inputs.top_p}}
      knowledge_pieces: ${{parent.inputs.knowledge_pieces}}
    environment_variables:
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI_EMBEDDING: ${{parent.inputs.embedding_connection}}
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI_CHAT: ${{parent.inputs.llm_connection}}
