$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
type: pipeline
tags:
  Preview: ""
name: llm_ingest_db_to_faiss
display_name: LLM - SQL Datastore to FAISS Pipeline
version: 0.0.97
description: Single job pipeline to chunk data from AzureML sql data store, and create FAISS embeddings index
settings:
  default_compute: serverless
inputs:
  db_datastore:
    type: string
    description: database datastore uri in the format of 'azureml://datastores/{datastore_name}'
  embeddings_model:
    type: string
    description: The model used to generate embeddings. 'azure_open_ai://endpoint/{endpoint_name}/deployment/{deployment_name}/model/{model_name}'
  chat_aoai_deployment_name:
    type: string
    optional: true
    description: The name of the chat AOAI deployment
  embedding_aoai_deployment_name:
    type: string
    description: The name of the embedding AOAI deployment
  embeddings_dataset_name:
    type: string
    description: The name of the faiss index
  max_tables:
    type: integer
    optional: true
  max_columns:
    type: integer
    optional: true
  max_rows:
    type: integer
    optional: true
  max_sampling_rows:
    type: integer
    optional: true
  max_text_length:
    type: integer
    optional: true
  max_knowledge_pieces:
    type: integer
    optional: true
  selected_tables:
    type: string
    optional: true
  column_settings:
    type: string
    optional: true
  llm_config:
    type: string
    optional: true
    description: The name of the llm config
  serverless_instance_count:
    type: integer
    optional: true
    default: "1"
  serverless_instance_type:
    type: string
    optional: true
    default: Standard_DS3_v2
  embedding_connection:
    type: string
    optional: true
    description: Azure OpenAI workspace connection ARM ID for embeddings
  llm_connection:
    type: string
    optional: true
    description: Azure OpenAI workspace connection ARM ID for LLM
  runtime:
    type: string
    optional: false
    description: The name of the runtime
  sample_data:
    type: uri_folder
    description: "Sample data to be used for data ingestion. format: 'azureml:samples-test:1'"
    optional: true
    # path: "azureml:samples-test:1"
  # data ingest setting
  include_builtin_examples:
    type: boolean
    default: true
    optional: true
  tools:
    type: string
    optional: true
    description: 'The name of the tools for dbcopilot. Supported tools: "tsql", "python". Format: ["tsql", "python"]'
  knowledge_pieces:
    type: string
    optional: true
    description: "The list of knowledge pieces to be used for grounding."
  include_views:
    type: boolean
    optional: true
    description: "Whether to turn on views."
  instruct_template:
    type: string
    optional: true
    description: "The instruct template for the LLM."
  managed_identity_enabled:
    type: boolean
    default: false
    optional: true
    description: "Whether to connect using managed identity."
outputs:
  grounding_index:
    type: uri_folder
  db_context:
    type: uri_folder
jobs:
  db_meta_loading_generator:
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    inputs:
      asset_uri:
        path: ${{parent.inputs.db_datastore}}
      max_tables:
        path: ${{parent.inputs.max_tables}}
      max_columns:
        path: ${{parent.inputs.max_columns}}
      max_rows:
        path: ${{parent.inputs.max_rows}}
      max_sampling_rows:
        path: ${{parent.inputs.max_sampling_rows}}
      max_text_length:
        path: ${{parent.inputs.max_text_length}}
      max_knowledge_pieces:
        path: ${{parent.inputs.max_knowledge_pieces}}
      selected_tables:
        path: ${{parent.inputs.selected_tables}}
      column_settings:
        path: ${{parent.inputs.column_settings}}
      include_views:
        path :${{parent.inputs.include_views}}
    outputs:
      output_chunk_file:
        type: uri_folder
      output_grounding_context_file: ${{parent.outputs.db_context}}
    environment_variables:
      MANAGED_IDENTITY_ENABLED: ${{parent.inputs.managed_identity_enabled}}
    component: "azureml:llm_dbcopilot_grounding:0.0.70"
    type: command
  generate_meta_embeddings:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: "azureml:llm_rag_generate_embeddings:0.0.66"
    inputs:
      chunks_source:
        type: uri_folder
        path: ${{parent.jobs.db_meta_loading_generator.outputs.output_chunk_file}}
      embeddings_model: ${{parent.inputs.embeddings_model}}
    outputs:
      embeddings:
        mode: upload
        type: uri_folder
    environment_variables:
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI: ${{parent.inputs.embedding_connection}}
  # generate_meta_embeddings:
  #   resources:
  #     instance_count: ${{parent.inputs.serverless_instance_count}}
  #     instance_type: ${{parent.inputs.serverless_instance_type}}
  #     properties:
  #       compute_specification:
  #         automatic: true
  #   retry_settings:
  #     timeout: 3600
  #     max_retries: 3
  #   environment_variables:
  #     AZUREML_WORKSPACE_CONNECTION_ID_AOAI: ${{parent.inputs.embedding_connection}}
  #   inputs:
  #     chunks_source:
  #       path: ${{parent.jobs.db_meta_loading_generator.outputs.output_chunk_file}}
  #     embeddings_model:
  #       path: ${{parent.inputs.embeddings_model}}
  #   outputs:
  #     embeddings:
  #       mode: upload
  #       type: uri_folder
  #   component: "azureml:llm_rag_generate_embeddings_parallel:0.0.9"
  #   type: parallel
  create_meta_faiss_index_job:
    environment_variables:
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI: ${{parent.inputs.embedding_connection}}
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    inputs:
      embeddings:
        path: ${{parent.jobs.generate_meta_embeddings.outputs.embeddings}}
    outputs:
      index: ${{parent.outputs.grounding_index}}
    component: "azureml:llm_rag_create_faiss_index:0.0.71"
    type: command

  #########################################
  db_sample_loading_generator:
    type: command
    component: "azureml:llm_dbcopilot_grounding_ground_samples:0.0.45"
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    inputs:
      sample_folder:
        type: uri_folder
        path: ${{parent.inputs.sample_data}}
      include_builtin: ${{parent.inputs.include_builtin_examples}}
      tools: ${{parent.inputs.tools}}
      grounding_context: ${{parent.jobs.db_meta_loading_generator.outputs.output_grounding_context_file}}
    outputs:
      output_chunk_file:
        type: uri_folder
  #########################################
  generate_sample_embeddings:
    type: command
    component: "azureml:llm_rag_generate_embeddings:0.0.66"
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    inputs:
      chunks_source:
        type: uri_file
        path: ${{parent.jobs.db_sample_loading_generator.outputs.output_chunk_file}}
      embeddings_model: ${{parent.inputs.embeddings_model}}
    outputs:
      embeddings:
        type: uri_folder
        mode: upload
    environment_variables:
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI: ${{parent.inputs.embedding_connection}}
  #########################################
  create_sample_faiss_index_job:
    type: command
    component: "azureml:llm_rag_create_faiss_index:0.0.71"
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    inputs:
      embeddings:
        type: uri_folder
        path: ${{parent.jobs.generate_sample_embeddings.outputs.embeddings}}
    outputs:
      index:
        type: uri_folder
  #########################################
  register_mlindex_asset_job:
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    inputs:
      storage_uri:
        path: ${{parent.jobs.create_meta_faiss_index_job.outputs.index}}
      asset_name:
        path: ${{parent.inputs.embeddings_dataset_name}}
    outputs:
      asset_id:
        type: uri_file
    component: "azureml:llm_rag_register_mlindex_asset:0.0.70"
    type: command
  create_prompt_flow:
    environment_variables:
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI_EMBEDDING: ${{parent.inputs.embedding_connection}}
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI_CHAT: ${{parent.inputs.llm_connection}}
      MANAGED_IDENTITY_ENABLED: ${{parent.inputs.managed_identity_enabled}}
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    identity:
      type: user_identity
    inputs:
      index_name:
        path: ${{parent.inputs.embeddings_dataset_name}}
      grounding_embedding_uri:
        path: ${{parent.jobs.create_meta_faiss_index_job.outputs.index}}
      example_embedding_uri:
        path: ${{parent.jobs.create_sample_faiss_index_job.outputs.index}}
      db_context_uri:
        path: ${{parent.jobs.db_meta_loading_generator.outputs.output_grounding_context_file}}
      asset_uri:
        path: ${{parent.inputs.db_datastore}}
      embedding_aoai_deployment_name:
        path: ${{parent.inputs.embedding_aoai_deployment_name}}
      chat_aoai_deployment_name:
        path: ${{parent.inputs.chat_aoai_deployment_name}}
      llm_config:
        path: ${{parent.inputs.llm_config}}
      runtime:
        path: ${{parent.inputs.runtime}}
      knowledge_pieces:
        path: ${{parent.inputs.knowledge_pieces}}
      include_views:
        path: ${{parent.inputs.include_views}}
      instruct_template:
        path: ${{parent.inputs.instruct_template}}
    component: "azureml:llm_dbcopilot_create_promptflow:0.0.70"
    type: command
