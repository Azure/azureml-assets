$schema: 
  https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
type: command

name: benchmark_embedding_model
display_name: Benchmark Embedding Model
description: Component for benchmarking an embedding model via MTEB.
version: 0.0.4

inputs:
  endpoint_url:
    type: string
    optional: true
    description: For AOAI, the base endpoint url. For OAI, this will be ignored.
      For OSS_MaaS, the target url.
  deployment_type:
    type: string
    optional: false
    description: The deployment type.
    enum:
    - AOAI
    - OAI
    - OSS_MaaS
  deployment_name:
    type: string
    optional: true
    description: For AOAI, the deployment name. For OAI, the model name. For 
      OSS_MaaS, this wil be ignored.
  connections_name:
    type: string
    optional: false
    description: Used for authenticating endpoint.
  tasks:
    type: string
    optional: true
    description: Comma separated string denoting the tasks to benchmark the 
      model on.
  task_types:
    type: string
    optional: true
    description: >-
      Comma separated string denoting the task type to benchmark the model on. Choose
      from
      the following task types: BitextMining, Classification, Clustering, PairClassification,
      Reranking,
      Retrieval, STS, Summarization.
  task_langs:
    type: string
    optional: true
    description: Comma separated string denoting the task languages to benchmark
      the model on.
  preset:
    type: string
    optional: false
    description: Choose from one of the presets for benchmarking.
    enum:
    - None
    - mteb_main_en
    default: None

outputs:
  metrics:
    type: uri_folder
    description: Directory where the benchmark metrics will be saved.

code: ../src
environment: 
  azureml://registries/azureml/environments/model-evaluation/labels/latest
command: >-
  python -m aml_benchmark.benchmark_embedding_model.main
  $[[--endpoint_url ${{inputs.endpoint_url}}]]
  --deployment_type ${{inputs.deployment_type}}
  $[[--deployment_name ${{inputs.deployment_name}}]]
  --connections_name '${{inputs.connections_name}}'
  $[[--tasks '${{inputs.tasks}}']]
  $[[--task_types '${{inputs.task_types}}']]
  $[[--task_langs '${{inputs.task_langs}}']]
  --preset ${{inputs.preset}}
  --output_metrics_dir ${{outputs.metrics}}
