$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
type: pipeline

tags:
  Preview: ""
version: 0.0.88
name: llm_ingest_dataset_to_faiss_user_id
display_name: LLM - Dataset to FAISS Pipeline
is_deterministic: false

description: Single job pipeline to chunk data from AzureML data asset, and create
  FAISS embeddings index

settings:
  default_compute: serverless

inputs:
      # llm_model config
  llm_config:
    type: string
    default: '{"type": "azure_open_ai", "model_name": "gpt-35-turbo", "deployment_name":
      "gpt-35-turbo", "temperature": 0, "max_tokens": 2000}'
    description: "JSON describing the LLM provider and model details to use for prompt\
      \ generation."
  llm_connection:
    type: string
    optional: true
    description: "Azure OpenAI workspace connection ARM ID"
  # register settings
  embeddings_dataset_name:
    type: string
    optional: true
    default: "VectorIndexDS"
    description: "Name of the vector index"
  # compute settings
  serverless_instance_count:
    type: integer
    default: 1
    optional: true
    description: "Instance count to use for the serverless compute"
  serverless_instance_type:
    type: string
    default: "Standard_E8s_v3"
    optional: true
    description: "The Instance Type to be used for the serverless compute"
  # data to import
  input_data:
    type: uri_folder
    mode: rw_mount
  # Data Chunker
  chunk_size:
    type: integer
    default: 1024
    description: "Chunk size (by token) to pass into the text splitter before performing\
      \ embeddings"
  chunk_overlap:
    type: integer
    default: 0
    description: "Overlap of content (by token) between the chunks"
  input_glob:
    type: string
    optional: true
    description: "Glob pattern to filter files from the input folder. e.g. 'articles/**/*''"
  max_sample_files:
    type: integer
    default: -1
    optional: true
    description: "Number of files read in during QA test data generation"
  data_source_url:
    type: string
    description: "The url which can be appended to file names to form citation links\
      \ for documents"
  document_path_replacement_regex:
    type: string
    optional: true
    description: "A JSON string with two fields, 'match_pattern' and 'replacement_pattern'\
      \ to be used with re.sub on the source url. e.g. '{\"match_pattern\": \"(.*)/articles/(.*)(\\\
      \\.[^.]+)$\", \"replacement_pattern\": \"\\\\1/\\\\2\"}' would remove '/articles'\
      \ from the middle of the url."
  # Embeddings components
  embeddings_container:
    type: uri_folder
    optional: true
    mode: direct
    description: "Folder to contain generated embeddings. Should be parent folder\
      \ of the 'embeddings' output path used for for this component. Will compare\
      \ input data to existing embeddings and only embed changed/new data, reusing\
      \ existing chunks."
  embeddings_model:
    type: string
    default: "azure_open_ai://deployment/text-embedding-ada-002/model/text-embedding-ada-002"
    description: "The model to use to embed data. E.g. 'hugging_face://model/sentence-transformers/all-mpnet-base-v2'\
      \ or 'azure_open_ai://deployment/{deployment_name}/model/{model_name}'"
  embedding_connection:
    type: string
    optional: true
    description: "Azure OpenAI workspace connection ARM ID for embeddings"

outputs:
  faiss_index:
    type: uri_folder
    description: "Folder containing the FAISS MLIndex. Deserialized using azureml.rag.mlindex.MLIndex(uri)."

#defaults:
#  compute: azureml:cpu-cluster
jobs:
  #############
  validate_deployments_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_validate_deployments:0.0.86'
    identity:
      type: user_identity
    inputs:
      embeddings_model: ${{parent.inputs.embeddings_model}}
      embeddings_connection: ${{parent.inputs.embedding_connection}}
      llm_config: ${{parent.inputs.llm_config}}
      llm_connection: ${{parent.inputs.llm_connection}}
    outputs:
      output_data:
        type: uri_file
    environment_variables:
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI_EMBEDDING: ${{parent.inputs.embedding_connection}}
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI_COMPLETION: ${{parent.inputs.llm_connection}}
  ############
  data_chunking_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_crack_and_chunk:0.0.83'
    identity:
      type: user_identity
    inputs:
      input_data: ${{parent.inputs.input_data}}
      input_glob: ${{parent.inputs.input_glob}}
      chunk_size: ${{parent.inputs.chunk_size}}
      chunk_overlap: ${{parent.inputs.chunk_overlap}}
      data_source_url: ${{parent.inputs.data_source_url}}
      document_path_replacement_regex: ${{parent.inputs.document_path_replacement_regex}}
      max_sample_files: ${{parent.inputs.max_sample_files}}
    outputs:
      output_chunks:
        type: uri_folder
    environment_variables:
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI: ${{parent.inputs.llm_connection}}
  ############
  embeddings_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_generate_embeddings:0.0.76'
    identity:
      type: user_identity
    inputs:
      chunks_source:
        type: uri_folder
        path: ${{parent.jobs.data_chunking_job.outputs.output_chunks}}
      embeddings_container: ${{parent.inputs.embeddings_container}}
      embeddings_model: ${{parent.inputs.embeddings_model}}
      deployment_validation: ${{parent.jobs.validate_deployments_job.outputs.output_data}}
    outputs:
      embeddings:
        type: uri_folder
    environment_variables:
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI: ${{parent.inputs.embedding_connection}}
  ############
  # embeddings_parallel_job:
  #   type: command
  #   resources:
  #     instance_count: ${{parent.inputs.serverless_instance_count}}
  #     instance_type: ${{parent.inputs.serverless_instance_type}}
  #     properties:
  #       compute_specification:
  #         automatic: true
  #   retry_settings:
  #     timeout: 3600
  #     max_retries: 3
  #   mini_batch_size: "3"
  #   mini_batch_error_threshold: 0
  #   logging_level: "INFO"
  #   component: 'azureml:llm_rag_generate_embeddings_parallel:0.0.10'
  #  identity:
  #    type: user_identity
  #   inputs:
  #     chunks_source:
  #       type: uri_folder
  #       path: ${{parent.jobs.data_chunking_job.outputs.output_chunks}}
  #     embeddings_container: ${{parent.inputs.embeddings_container}}
  #     embeddings_model: ${{parent.inputs.embeddings_model}}
  #     deployment_validation: ${{parent.jobs.validate_deployments_job.outputs.output_data}}
  #   outputs:
  #     embeddings:
  #       type: uri_folder
  #   environment_variables:
  #      AZUREML_WORKSPACE_CONNECTION_ID_AOAI : ${{parent.inputs.embedding_connection}}
  ############
  create_faiss_index_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_create_faiss_index:0.0.81'
    identity:
      type: user_identity
    inputs:
      embeddings:
        type: uri_folder
        path: ${{parent.jobs.embeddings_job.outputs.embeddings}}
    outputs:
      index: ${{parent.outputs.faiss_index}}
    environment_variables:
      AZUREML_WORKSPACE_CONNECTION_ID_AOAI: ${{parent.inputs.embedding_connection}}
  ############
  register_mlindex_asset_job:
    type: command
    resources:
      instance_count: ${{parent.inputs.serverless_instance_count}}
      instance_type: ${{parent.inputs.serverless_instance_type}}
      properties:
        compute_specification:
          automatic: true
    component: 'azureml:llm_rag_register_mlindex_asset:0.0.80'
    identity:
      type: user_identity
    inputs:
      storage_uri: ${{parent.jobs.create_faiss_index_job.outputs.index}}
      asset_name: ${{parent.inputs.embeddings_dataset_name}}
    outputs:
      asset_id:
        type: uri_file
