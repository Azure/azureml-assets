$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: question_answering_datapreprocess
version: 0.0.1
type: command

is_deterministic: true

display_name: Question Answering DataPreProcess
description: Component to preprocess data for question answering task

environment: azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/4

code: ../../../src/preprocess

inputs:
  # Task arguments
  question_key:
    type: string
    optional: false
    description: The question whose answer needs to be extracted from the provided
      context

  context_key:
    type: string
    optional: false
    description: The context that contains the answer to the question

  answers_key:
    type: string
    optional: false
    description: "The value of this field is text in JSON format with two nested keys:\
      \ answer_start_key and answer_text_key with their corresponding values"

  answer_start_key:
    type: string
    optional: false
    description: Refers to the position where the answer beings in context. Needs
      a value that maps to a nested key in the values of the answers_key parameter

  answer_text_key:
    type: string
    optional: false
    description: Contains the answer to the question. Needs a value that maps to a
      nested key in the values of the answers_key parameter

  doc_stride:
    type: integer
    default: 128
    optional: true
    description: Number of ovelapping context tokens to use while splitting the examples
      into multiple splits based on the model sequence length

  n_best_size:
    type: integer
    default: 20
    optional: true
    description: Topk results to use for the predicted start and end token positions

  max_answer_length_in_tokens:
    type: integer
    default: 30
    optional: true
    description: Maximum answer length to consider

  batch_size:
    type: integer
    optional: true
    default: 1000
    description: Number of examples to batch before calling the tokenization function

  # Inputs
  train_file_path:
    type: uri_file
    optional: true
    description: Enter the train file path

  validation_file_path:
    type: uri_file
    optional: true
    description: Enter the validation file path

  test_file_path:
    type: uri_file
    optional: true
    description: Enter the test file path

  train_mltable_path:
    type: mltable
    optional: true
    description: Enter the train mltable path

  validation_mltable_path:
    type: mltable
    optional: true
    description: Enter the validation mltable path

  test_mltable_path:
    type: mltable
    optional: true
    description: Enter the test mltable path

  # Dataset parameters
  model_selector_output:
    type: uri_folder
    optional: false
    description: output folder of model selector containing model metadata like config,
      checkpoints, tokenizer config

outputs:
  output_dir:
    type: uri_folder
    description: folder to store preprocessed outputs of input data

command: >-
  python preprocess.py --task_name QuestionAnswering --question_key ${{inputs.question_key}}
  --context_key ${{inputs.context_key}} --answers_key ${{inputs.answers_key}} --answer_start_key
  ${{inputs.answer_start_key}} --answer_text_key ${{inputs.answer_text_key}} $[[--doc_stride
  ${{inputs.doc_stride}}]] $[[--n_best_size ${{inputs.n_best_size}}]] $[[--max_answer_length_in_tokens
  ${{inputs.max_answer_length_in_tokens}}]] $[[--batch_size ${{inputs.batch_size}}]]
  $[[--train_file_path '${{inputs.train_file_path}}']] $[[--validation_file_path '${{inputs.validation_file_path}}']]
  $[[--test_file_path '${{inputs.test_file_path}}']] $[[--train_mltable_path '${{inputs.train_mltable_path}}']]
  $[[--validation_mltable_path '${{inputs.validation_mltable_path}}']] $[[--test_mltable_path
  '${{inputs.test_mltable_path}}']] --model_selector_output ${{inputs.model_selector_output}}
  --output_dir ${{outputs.output_dir}}

