$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: token_classification_datapreprocess
version: 0.0.3
type: command

is_deterministic: true

display_name: Token Classification DataPreProcess
description: Component to preprocess data for token classification task. See [docs](https://aka.ms/azureml/components/token_classification_datapreprocess)
  to learn more.

environment: azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/6

code: ../../../src/preprocess

inputs:
  # task arguments
  # sample input
  # {`tokens_column`: [ "EU", "rejects", "German", "call", "to", "boycott", "British", "lamb", "." ], `ner_tags_column`: '["B-ORG", "O", "B-MISC", "O", "O", "O", "B-MISC", "O", "O"]'}
  # For the above dataset pattern, `token_key` should be set as tokens_column and `tag_key` as ner_tags_column
  token_key:
    type: string
    optional: false
    description: Key for tokens in each example line

  tag_key:
    type: string
    optional: false
    description: Key for tags in each example line

  batch_size:
    type: integer
    optional: true
    default: 1000
    description: Number of examples to batch before calling the tokenization function

  # Data inputs
  # Please note that either `train_file_path` or `train_mltable_path` needs to be passed. In case both are passed, `mltable path` will take precedence. The validation and test paths are optional and an automatic split from train data happens if they are not passed.
  # If both validation and test files are missing, 10% of train data will be assigned to each of them and the remaining 80% will be used for training
  # If anyone of the file is missing, 20% of the train data will be assigned to it and the remaining 80% will be used for training
  train_file_path:
    type: uri_file
    optional: true
    description: Path to the registered training data asset. The supported data formats
      are `jsonl`, `json`, `csv`, `tsv` and `parquet`

  validation_file_path:
    type: uri_file
    optional: true
    description: Path to the registered validation data asset. The supported data
      formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`.

  test_file_path:
    type: uri_file
    optional: true
    description: Path to the registered test data asset. The supported data formats
      are `jsonl`, `json`, `csv`, `tsv` and `parquet`.

  train_mltable_path:
    type: mltable
    optional: true
    description: Path to the registered training data asset in `mltable` format.

  validation_mltable_path:
    type: mltable
    optional: true
    description: Path to the registered validation data asset in `mltable` format.

  test_mltable_path:
    type: mltable
    optional: true
    description: Path to the registered test data asset in `mltable` format.

  # Dataset parameters
  model_selector_output:
    type: uri_folder
    optional: false
    description: output folder of model selector containing model metadata like config,
      checkpoints, tokenizer config

outputs:
  output_dir:
    type: uri_folder
    description: The folder contains the tokenized output of the train, validation
      and test data along with the tokenizer files used to tokenize the data

command: >-
  python preprocess.py --task_name NamedEntityRecognition --token_key ${{inputs.token_key}}
  --tag_key ${{inputs.tag_key}} $[[--batch_size ${{inputs.batch_size}}]] $[[--train_file_path
  '${{inputs.train_file_path}}']] $[[--validation_file_path '${{inputs.validation_file_path}}']]
  $[[--test_file_path '${{inputs.test_file_path}}']] $[[--train_mltable_path '${{inputs.train_mltable_path}}']]
  $[[--validation_mltable_path '${{inputs.validation_mltable_path}}']] $[[--test_mltable_path
  '${{inputs.test_mltable_path}}']] --model_selector_output ${{inputs.model_selector_output}}
  --output_dir ${{outputs.output_dir}}
