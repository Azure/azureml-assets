$schema: 
  https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
name: grpo_chat_completion_pipeline
version: 0.0.5
type: pipeline
display_name: GRPO Chat Completion Pipeline
description: Pipeline component for fine-tuning Hugging Face chat completion 
  models with Group Relative Policy Optimization(GRPO)

inputs:
  instance_type_model_import:
    type: string
    optional: true
    default: Standard_d12_v2
    description: Instance type to be used for model_import component in case of 
      serverless compute, eg. standard_d12_v2. The parameter 
      compute_model_import must be set to 'serverless' for instance_type to be 
      used
  instance_type_finetune:
    type: string
    optional: true
    default: Standard_ND96isr_H100_v5
    description: Instance type to be used for finetune component in case of 
      serverless compute, eg. standard_nc24rs_v3. The parameter compute_finetune
      must be set to 'serverless' for instance_type to be used
  shm_size_finetune:
    type: string
    optional: true
    default: 5g
    description: Shared memory size to be used for finetune component. It is 
      useful while using Nebula (via DeepSpeed) which uses shared memory to save
      model and optimizer states.
  num_nodes_finetune:
    type: integer
    min: 1
    default: 1
    optional: true
    description: number of nodes to be used for finetuning (used for distributed
      training)
  number_of_gpu_to_use_finetuning:
    type: integer
    min: 1
    default: 1
    optional: true
    description: number of gpus to be used per node for finetuning, should be 
      equal to number of gpu per node in the compute SKU used for finetune

  # Model Import parameters (See [docs](https://aka.ms/azureml/components/chat_completion_model_import) to learn more)
  huggingface_id:
    type: string
    description: The string can be any valid Hugging Face id from the [Hugging 
      Face models 
      webpage](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads).
      Models from Hugging Face are subject to third party license terms 
      available on the Hugging Face model details page. It is your 
      responsibility to comply with the model's license terms.
    optional: true

  pytorch_model_path:
    type: custom_model
    optional: true
    description: Pytorch model asset path. Special characters like \ and ' are 
      invalid in the parameter value.
    mode: rw_mount

  mlflow_model_path:
    type: mlflow_model
    optional: true
    description: MLflow model asset path. Special characters like \ and ' are 
      invalid in the parameter value.
    mode: rw_mount

  #Finetune parameters

  dataset_name:
    type: string
    default: ''
    optional: true
    description: Name of the Hugging Face dataset to pull in

  dataset_prompt_column:
    type: string
    default: problem
    optional: false
    description: Column in the dataset containing the prompt for the chat 
      completion template

  dataset_train_split:
    type: uri_file
    optional: true
    description: Path to the training dataset in JSONL format
    mode: rw_mount

  dataset_validation_split:
    type: uri_file
    optional: true
    description: Path to the validation dataset in JSONL format
    mode: rw_mount

  eval_strategy:
    type: string
    enum:
    - disable
    - steps
    - epoch
    default: disable
    optional: true
    description: Evaluation strategy to use during training. Options are 
      'disable', 'steps', or 'epoch'.

  num_iterations:
    type: integer
    default: 5
    optional: true
    description: Number of training iterations

  epsilon:
    type: number
    default: 0.5
    optional: true
    description: Epsilon value for training

  per_device_train_batch_size:
    type: integer
    min: 1
    default: 8
    optional: true
    description: Per device batch size used for training

  per_device_eval_batch_size:
    type: integer
    min: 1
    default: 8
    optional: true
    description: Per device batch size used for evaluation

  gradient_accumulation_steps:
    type: integer
    default: 1
    optional: true
    description: Number of steps to accumulate gradients before performing a 
      backward pass

  learning_rate:
    type: number
    default: 1e-6
    optional: true
    description: Learning rate for training

  logging_steps:
    type: number
    default: 5
    optional: true
    description: Number of steps between logging updates.

  lr_scheduler_type:
    type: string
    enum:
    - "linear"
    - "cosine"
    - "cosine_with_restarts"
    - "polynomial"
    - "constant"
    - "constant_with_warmup"
    - "inverse_sqrt"
    - "reduce_lr_on_plateau"
    default: "cosine"
    optional: true
    description: The scheduler type to use for learning rate scheduling.

  num_train_epochs:
    type: number
    default: 4.0
    optional: true
    description: Number of training epochs

  max_grad_norm:
    type: number
    default: 1.0
    optional: true
    description: Maximum gradient norm for gradient clipping

  warmup_ratio:
    type: number
    default: 0.1
    optional: true
    description: Ratio of total training steps used for warmup

  max_steps:
    type: integer
    default: -1
    optional: true
    description: If set to a positive number, this will override 
      num_train_epochs and train for exactly this many steps. Set to -1 to 
      disable (default).

  eval_steps:
    type: integer
    default: 1
    optional: true
    description: Number of steps between evaluations

  optim:
    type: string
    enum:
    - "adamw_torch"
    - "adamw_torch_fused"
    - "adafactor"
    - "ademamix"
    - "sgd"
    - "adagrad"
    - "rmsprop"
    - "galore_adamw"
    - "grokadamw"
    - "schedule_free_sgd"
    default: "adamw_torch"
    optional: true
    description: The optimizer to use.

  use_liger_kernel:
    type: boolean
    default: false
    optional: true
    description: Whether to use the Liger kernel

  deepspeed_config:
    type: uri_file
    optional: false
    description: Path to a custom DeepSpeed configuration file in JSON format
    mode: ro_mount

  max_prompt_length:
    type: integer
    default: 512
    optional: true
    description: Maximum length of the input prompt

  num_generations:
    type: integer
    default: 4
    optional: true
    description: Number of generations to produce

  max_completion_length:
    type: integer
    default: 256
    optional: true
    description: Maximum length of the completion

  save_steps:
    type: integer
    default: 100
    optional: true
    description: Number of steps between saving checkpoints.

  save_total_limit:
    type: integer
    default: 20
    optional: true
    description: Maximum number of checkpoints to keep.

  shuffle_dataset:
    type: boolean
    default: true
    optional: true
    description: Whether to shuffle the dataset

  temperature:
    type: number
    default: 1.0
    optional: true
    description: Temperature for sampling

  top_p:
    type: number
    default: 1.0
    optional: true
    description: Top-p value for nucleus sampling

  vllm_gpu_memory_utilization:
    type: number
    default: 0.3
    optional: true
    description: GPU memory utilization for VLLM

  vllm_tensor_parallel_size:
    type: integer
    default: 1
    optional: true
    description: Tensor parallel size for VLLM

  beta:
    type: number
    default: 0.04
    optional: true
    description: Beta parameter for training

  # Compute parameters
  compute_model_import:
    type: string
    optional: true
    default: serverless
    description: compute to be used for model_import eg. provide 'FT-Cluster' if
      your compute is named 'FT-Cluster'. Special characters like \ and ' are 
      invalid in the parameter value. If compute cluster name is provided, 
      instance_type field will be ignored and the respective cluster will be 
      used
  compute_finetune:
    type: string
    optional: true
    default: serverless
    description: compute to be used for finetune eg. provide 'FT-Cluster' if 
      your compute is named 'FT-Cluster'. Special characters like \ and ' are 
      invalid in the parameter value. If compute cluster name is provided, 
      instance_type field will be ignored and the respective cluster will be 
      used

outputs:
  output_model_path:
    type: uri_folder
    description: Path to the output model folder containing the checkpoints
    mode: rw_mount

  mlflow_model_folder:
    type: mlflow_model
    description: output folder containing _best_ finetuned model in mlflow 
      format.
    mode: rw_mount

jobs:
  chat_completion_model_import:
    type: command
    component: azureml:chat_completion_model_import:0.0.80
    compute: '${{parent.inputs.compute_model_import}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_model_import}}'
    inputs:
      huggingface_id: '${{parent.inputs.huggingface_id}}'
      pytorch_model_path: '${{parent.inputs.pytorch_model_path}}'
      mlflow_model_path: '${{parent.inputs.mlflow_model_path}}'
  group_relative_policy_optimization:
    type: command
    component: azureml:group_relative_policy_optimization:0.0.2
    compute: '${{parent.inputs.compute_finetune}}'
    environment_variables:
      _AZUREML_CR_ENABLE_ITP_CAP: "false"
    distribution:
      type: pytorch
      process_count_per_instance: '${{parent.inputs.number_of_gpu_to_use_finetuning}}'
    resources:
      instance_count: '${{parent.inputs.num_nodes_finetune}}'
      instance_type: '${{parent.inputs.instance_type_finetune}}'
      shm_size: '${{parent.inputs.shm_size_finetune}}'
    inputs:
      beta: '${{parent.inputs.beta}}'
      dataset_name: '${{parent.inputs.dataset_name}}'
      dataset_prompt_column: '${{parent.inputs.dataset_prompt_column}}'
      dataset_train_split: '${{parent.inputs.dataset_train_split}}'
      dataset_validation_split: '${{parent.inputs.dataset_validation_split}}'
      deepspeed_config: '${{parent.inputs.deepspeed_config}}'
      epsilon: '${{parent.inputs.epsilon}}'
      eval_steps: '${{parent.inputs.eval_steps}}'
      eval_strategy: '${{parent.inputs.eval_strategy}}'
      gradient_accumulation_steps: '${{parent.inputs.gradient_accumulation_steps}}'
      learning_rate: '${{parent.inputs.learning_rate}}'
      logging_steps: '${{parent.inputs.logging_steps}}'
      lr_scheduler_type: '${{parent.inputs.lr_scheduler_type}}'
      max_completion_length: '${{parent.inputs.max_completion_length}}'
      max_grad_norm: '${{parent.inputs.max_grad_norm}}'
      max_prompt_length: '${{parent.inputs.max_prompt_length}}'
      max_steps: '${{parent.inputs.max_steps}}'
      model_name_or_path: '${{parent.jobs.chat_completion_model_import.outputs.output_dir}}'
      num_generations: '${{parent.inputs.num_generations}}'
      num_iterations: '${{parent.inputs.num_iterations}}'
      num_train_epochs: '${{parent.inputs.num_train_epochs}}'
      optim: '${{parent.inputs.optim}}'
      per_device_train_batch_size: '${{parent.inputs.per_device_train_batch_size}}'
      per_device_eval_batch_size: '${{parent.inputs.per_device_eval_batch_size}}'
      save_steps: '${{parent.inputs.save_steps}}'
      save_total_limit: '${{parent.inputs.save_total_limit}}'
      shuffle_dataset: '${{parent.inputs.shuffle_dataset}}'
      temperature: '${{parent.inputs.temperature}}'
      top_p: '${{parent.inputs.top_p}}'
      use_liger_kernel: '${{parent.inputs.use_liger_kernel}}'
      vllm_gpu_memory_utilization: '${{parent.inputs.vllm_gpu_memory_utilization}}'
      vllm_tensor_parallel_size: '${{parent.inputs.vllm_tensor_parallel_size}}'
      warmup_ratio: '${{parent.inputs.warmup_ratio}}'
    outputs:
      output_model_path: '${{parent.outputs.output_model_path}}'
      mlflow_model_folder: '${{parent.outputs.mlflow_model_folder}}'
