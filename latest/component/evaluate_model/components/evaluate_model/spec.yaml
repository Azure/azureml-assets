$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: evaluate_model
display_name: Evaluate Model
description: Evaluate MLFlow models for supported task types.

version: 0.0.3

tags:
  type: evaluation
  sub_type: evaluate_model
  Preview: ""

inputs:
  task:
    type: string
    default: tabular-classification
    enum: [tabular-classification, tabular-classification-multilabel, tabular-regression,
      text-classification, text-classification-multilabel, text-named-entity-recognition,
      text-summarization, question-answering, text-translation]
    description: "Task type"
  test_data:
    type: uri_file
    optional: true
    description: "Test Data as JSON Lines URI_FILE"
  test_data_mltable:
    type: mltable
    optional: true
    description: "Test Data as MLTable folder"
  evaluation_config:
    type: uri_file
    description: "Additional parameters required for evaluation."
    optional: true
  test_data_label_column_name:
    type: string
    description: Column name of target values
  test_data_input_column_names:
    type: string
    optional: true
    description: Comma separated values of feature column names
  mlflow_model:
    type: mlflow_model
    optional: true
    description: "Mlflow Model - registered model or output of a job with type mlflow_model\
      \ in a pipeline"
  model_uri:
    type: string
    optional: true
    description: "The location, in URI format, of the MLflow model, for example: -\
      \ ``runs:/<mlflow_run_id>/run-relative/path/to/model`` - ``models:/<model_name>/<model_version>``\
      \ - ``models:/<model_name>/<stage>``"
  device:
    type: string
    optional: true
    default: cpu
    enum: [cpu, gpu]
  batch_size:
    type: integer
    optional: true
  evaluation_config_params:
    type: string
    optional: true
    description: "JSON Serielized string of evaluation_config"

outputs:
  evaluation_result:
    type: uri_folder

code: ../../src
environment: azureml://registries/azureml/environments/model-evaluation/versions/5
command: >-
  python download_dependencies.py
  $[[--mlflow-model '${{inputs.mlflow_model}}']]
  $[[--model-uri ${{inputs.model_uri}}]] ;
  python evaluate_model.py
  $[[--data '${{inputs.test_data}}']]
  $[[--data-mltable '${{inputs.test_data_mltable}}']]
  --task ${{inputs.task}}
  --output '${{outputs.evaluation_result}}'
  --label-column-name ${{inputs.test_data_label_column_name}}
  $[[--input-column-names ${{inputs.test_data_input_column_names}}]]
  $[[--mlflow-model '${{inputs.mlflow_model}}']]
  $[[--model-uri ${{inputs.model_uri}}]]
  $[[--config-file-name '${{inputs.evaluation_config}}']]
  $[[--device ${{inputs.device}}]]
  $[[--batch-size ${{inputs.batch_size}}]]
  $[[--config_str '${{inputs.evaluation_config_params}}']]
