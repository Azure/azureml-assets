$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: evaluate_model
display_name: Evaluate Model
description: Evaluate MLFlow models for supported task types.

version: 0.0.20
type: command
tags:
  type: evaluation
  sub_type: evaluate_model
  Preview: ""

inputs:
  task:
    type: string
    optional: false
    default: tabular-classification
    enum: [tabular-classification, tabular-classification-multilabel, tabular-regression,
      tabular-forecasting, text-classification, text-classification-multilabel, text-named-entity-recognition,
      text-summarization, question-answering, text-translation, fill-mask, text-generation,
      image-classification, image-classification-multilabel, chat-completion, image-object-detection,
      image-instance-segmentation]
    description: "Task type"
  test_data:
    type: uri_folder
    optional: false
    mode: ro_mount
    description: "Test Data as JSON Lines URI_FILE"
  evaluation_config:
    type: uri_file
    optional: true
    mode: ro_mount
    description: "Additional parameters required for evaluation."
  test_data_label_column_name:
    type: string
    optional: false
    description: Column name of target values
  test_data_input_column_names:
    type: string
    optional: true
    description: Comma separated values of feature column names
  mlflow_model:
    type: mlflow_model
    optional: false
    mode: ro_mount
    description: "Mlflow Model - registered model or output of a job with type mlflow_model\
      \ in a pipeline"
  device:
    type: string
    optional: false
    default: auto
    enum: [auto, cpu, gpu]
  batch_size:
    type: integer
    optional: true
  evaluation_config_params:
    type: string
    optional: true
    description: "JSON Serialized string of evaluation_config"

outputs:
  evaluation_result:
    type: uri_folder

is_deterministic: true
code: ../../src
environment: azureml://registries/azureml/environments/model-evaluation/versions/16

command: >-
  python download_dependencies.py
  --mlflow-model '${{inputs.mlflow_model}}' ;
  python evaluate_model.py
  --task '${{inputs.task}}'
  --output '${{outputs.evaluation_result}}'
  --label-column-name '${{inputs.test_data_label_column_name}}'
  --mlflow-model '${{inputs.mlflow_model}}'
  --device '${{inputs.device}}'
  --data '${{inputs.test_data}}'
  $[[--input-column-names '${{inputs.test_data_input_column_names}}']]
  $[[--config-file-name '${{inputs.evaluation_config}}']]
  $[[--batch-size '${{inputs.batch_size}}']]
  $[[--config_str '${{inputs.evaluation_config_params}}']]
