$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
name: nlp_textclassification_multiclass
version: 0.0.6
type: pipeline
display_name: PipelineComponent for AutoML NLP Multiclass TextClassification
description: Pipeline component for AutoML NLP Multiclass Text classification
inputs:
  compute_model_import:
    type: string
    optional: false
    description: compute to be used for model_selector eg. provide 'FT-Cluster' if
      your compute is named 'FT-Cluster'
  compute_preprocess:
    type: string
    optional: false
    description: compute to be used for preprocess eg. provide 'FT-Cluster' if your
      compute is named 'FT-Cluster'
  compute_finetune:
    type: string
    optional: false
    description: compute to be used for finetune eg. provide 'FT-Cluster' if your
      compute is named 'FT-Cluster'
  num_nodes_finetune:
    type: integer
    default: 1
    optional: true
    description: number of nodes to be used for finetuning (used for distributed training)
  process_count_per_instance_finetune:
    type: integer
    default: 1
    optional: true
    description: number of gpus to be used per node for finetuning, should be equal
      to number of gpu per node in the compute SKU used for finetune
  model_name:
    type: string
    default: bert-base-uncased
    description: model id used to load model checkpoint.

  # Data PreProcess parameters (See [docs](https://aka.ms/azureml/components/text_classification_datapreprocess) to learn more)
  label_column_name:
    type: string
    optional: false
    description: label key name

  # Dataset parameters
  training_data:
    type: uri_file
    optional: false
    description: Enter the train file path

  validation_data:
    type: uri_file
    optional: false
    description: Enter the validation file path

  # Training parameters
  training_batch_size:
    type: integer
    default: 32
    optional: true
    description: Train batch size

  validation_batch_size:
    type: integer
    default: 32
    optional: true
    description: Validation batch size

  number_of_epochs:
    type: integer
    default: 3
    optional: true
    description: Number of epochs to train

  gradient_accumulation_steps:
    type: integer
    default: 1
    optional: true
    description: Gradient accumulation steps

  learning_rate:
    type: number
    default: 0.00005
    optional: true
    description: Start learning rate. Defaults to linear scheduler.

  warmup_steps:
    type: integer
    default: 0
    optional: true
    description: Number of steps used for a linear warmup from 0 to learning_rate

  weight_decay:
    type: number
    default: 0.0
    optional: true
    description: The weight decay to apply (if not zero) to all layers except all
      bias and LayerNorm weights in AdamW optimizer

  learning_rate_scheduler:
    type: string
    default: linear
    optional: true
    enum:
    - linear
    - cosine
    - cosine_with_restarts
    - polynomial
    - constant
    - constant_with_warmup
    description: The scheduler type to use

  # AutoML NLP parameters
  enable_long_range_text:
    type: boolean
    optional: true
    default: true
    description: label key name

  precision:
    type: string
    enum:
    - '32'
    - '16'
    default: '16'
    optional: true
    description: Apply mixed precision training. This can reduce memory footprint
      by performing operations in half-precision.

  # MLFlow Parameters
  enable_full_determinism:
    type: string
    enum:
    - 'true'
    - 'false'
    default: 'false'
    optional: true
    description: Ensure reproducible behavior during distributed training

  evaluation_strategy:
    type: string
    default: epoch
    optional: true
    enum:
    - epoch
    - steps
    description: The evaluation strategy to adopt during training

  evaluation_steps_interval:
    type: number
    default: 0.0
    optional: true
    description: The evaluation steps in fraction of an epoch steps to adopt during
      training. Overwrites evaluation_steps if not 0.

  evaluation_steps:
    type: integer
    default: 500
    optional: true
    description: Number of update steps between two evals if evaluation_strategy='steps'

  logging_strategy:
    type: string
    default: steps
    optional: true
    enum:
    - epoch
    - steps
    description: The logging strategy to adopt during training.

  logging_steps:
    type: integer
    default: 500
    optional: true
    description: Number of update steps between two logs if logging_strategy='steps'

  primary_metric:
    type: string
    default: accuracy
    optional: true
    enum:
    - loss
    - f1_macro
    - mcc
    - accuracy
    - precision_macro
    - recall_macro
    description: Specify the metric to use to compare two different models

  # Deepspeed Parameters
  apply_deepspeed:
    type: string
    enum:
    - 'true'
    - 'false'
    default: 'true'
    optional: true
    description: If set to true, will enable deepspeed for training

  # ORT Parameters
  apply_ort:
    type: string
    enum:
    - 'true'
    - 'false'
    default: 'true'
    optional: true
    description: If set to true, will use the ONNXRunTime training

  deepspeed_config:
    type: uri_file
    optional: true
    description: Deepspeed config to be used for finetuning

outputs:
  pytorch_model_folder_finetune:
    type: uri_folder
    description: Output dir to save the finetune model and other metadata

  mlflow_model_folder_finetune:
    type: mlflow_model
    description: Output dir to save the finetune model as mlflow model

jobs:
  model_import:
    type: command
    component: azureml:text_classification_model_import:0.0.76
    compute: ${{parent.inputs.compute_model_import}}
    inputs:
      huggingface_id: ${{parent.inputs.model_name}}
  preprocess:
    type: command
    component: azureml:nlp_multiclass_datapreprocessing:0.0.4
    compute: ${{parent.inputs.compute_preprocess}}
    inputs:
      label_column_name: ${{parent.inputs.label_column_name}}
      train_file_path: ${{parent.inputs.training_data}}
      valid_file_path: ${{parent.inputs.validation_data}}
      model_selector_output: ${{parent.jobs.model_import.outputs.output_dir}}
      enable_long_range_text: ${{parent.inputs.enable_long_range_text}}
  finetune:
    type: command
    component: azureml:text_classification_finetune:0.0.76
    compute: ${{parent.inputs.compute_finetune}}
    distribution:
      type: pytorch
      process_count_per_instance: ${{parent.inputs.process_count_per_instance_finetune}}
    resources:
      instance_count: ${{parent.inputs.num_nodes_finetune}}
    inputs:
      per_device_train_batch_size: ${{parent.inputs.training_batch_size}}
      per_device_eval_batch_size: ${{parent.inputs.validation_batch_size}}
      num_train_epochs: ${{parent.inputs.number_of_epochs}}
      gradient_accumulation_steps: ${{parent.inputs.gradient_accumulation_steps}}
      learning_rate: ${{parent.inputs.learning_rate}}
      warmup_steps: ${{parent.inputs.warmup_steps}}
      weight_decay: ${{parent.inputs.weight_decay}}
      lr_scheduler_type: ${{parent.inputs.learning_rate_scheduler}}
      precision: ${{parent.inputs.precision}}
      enable_full_determinism: ${{parent.inputs.enable_full_determinism}}
      evaluation_strategy: ${{parent.inputs.evaluation_strategy}}
      evaluation_steps_interval: ${{parent.inputs.evaluation_steps_interval}}
      eval_steps: ${{parent.inputs.evaluation_steps}}
      logging_strategy: ${{parent.inputs.logging_strategy}}
      logging_steps: ${{parent.inputs.logging_steps}}
      metric_for_best_model: ${{parent.inputs.primary_metric}}
      apply_deepspeed: ${{parent.inputs.apply_deepspeed}}
      deepspeed: ${{parent.inputs.deepspeed_config}}
      apply_ort: ${{parent.inputs.apply_ort}}
      model_selector_output: ${{parent.jobs.model_import.outputs.output_dir}}
      preprocess_output: ${{parent.jobs.preprocess.outputs.output_dir}}
    outputs:
      pytorch_model_folder: ${{parent.outputs.pytorch_model_folder_finetune}}
  text_classification_model_converter:
    type: command
    component: azureml:ft_nlp_model_converter:0.0.76
    compute: ${{parent.inputs.compute_finetune}}
    resources:
      instance_type: ${{parent.inputs.instance_type_finetune}}
    inputs:
      model_path: ${{parent.jobs.finetune.outputs.pytorch_model_folder}}
    outputs:
      mlflow_model_folder: ${{parent.outputs.mlflow_model_folder_finetune}}
