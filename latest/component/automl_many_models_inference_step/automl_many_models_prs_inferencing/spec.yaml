$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json

name: automl_many_models_inference_step
display_name: AutoML Many Models - Inference Step
version: 0.0.9
is_deterministic: false

type: parallel
inputs:
  partitioned_data:
    type: uri_folder
    description: 'Folder URI with partioned data.'
  metadata:
    type: uri_folder
    description: 'The metadata calculated from the setup step.'
  instance_count:
    type: integer
    description: 'Number of nodes in a compute cluster we will run the inference step
      on.'
  max_concurrency_per_instance:
    type: integer
    description: 'Number of processes that will be run concurrently on any given node.
      This number should not be larger than 1/2 of the number of cores in an individual
      node in the specified cluster'
  parallel_step_timeout_in_seconds:
    type: integer
    description: 'The PRS step time out settings.'
  enable_event_logger:
    type: boolean
    optional: true
    description: 'Enable event logger.'

outputs:
  output_metadata:
    mode: mount
    type: uri_folder
    description: 'The calculated metadata from inference step.'
  prediction:
    mode: mount
    type: uri_folder
    description: 'The prediction from inference step.'
  prs_output:
    mode: mount
    type: uri_folder
    description: 'The default PRS output, not used by this step.'

input_data: ${{inputs.partitioned_data}}
output_data: ${{outputs.prs_output}}
mini_batch_size: "1"

resources:
  instance_count: 2
max_concurrency_per_instance: 4

logging_level: "DEBUG"
mini_batch_error_threshold: -1
retry_settings:
  max_retries: 2
  timeout: 3600

task:
  type: run_function
  code: ../src/inference
  entry_script: inference.py
  environment: azureml://registries/azureml/environments/automl-gpu/versions/48
  program_arguments: >-
    --setup-metadata ${{inputs.metadata}}
    --error_threshold -1
    --task_overhead_timeout 86400
    --progress_update_timeout ${{inputs.parallel_step_timeout_in_seconds}}
    --copy_logs_to_parent True
    --resource_monitor_interval 20
    --output-metadata ${{outputs.output_metadata}}
    --nodes-count ${{inputs.instance_count}}
    --process_count_per_node ${{inputs.max_concurrency_per_instance}}
    --output-prediction ${{outputs.prediction}}
    $[[--enable-event-logger ${{inputs.enable_event_logger}}]]
