$schema: http://azureml/sdk-2-0/CommandComponent.json
name: rai_vision_insights
display_name: RAI Vision Insights
version: 0.0.21
tags:
  Preview: ""
type: command
inputs:
  task_type:
    type: string
    enum: ['image_classification', 'multilabel_image_classification', 'object_detection']
    description: The type of task to perform
  model_input: # mlflow model name:version
    type: mlflow_model
    optional: false
    description: The model name to be used for computing insights
  model_info:
    type: string # model name:version
    optional: false
    description: The model name to be used for computing insights
  test_dataset:
    type: mltable
    description: The test dataset to be used for computing insights
  target_column_name:
    type: string
    description: The target column name
  maximum_rows_for_test_dataset:
    type: integer
    default: 5000
    description: The maximum number of rows to use from the test dataset
  classes:
    type: string # Optional[List[str]]
    default: '[]'
    description: The list of class names for the target column
  categorical_metadata_features:
    type: string # Optional[List[str]]
    default: '[]'
    description: The list of categorical metadata feature names
  dropped_metadata_features:
    type: string # Optional[List[str]]
    default: '[]'
    description: The list of dropped metadata feature names
  precompute_explanation:
    type: boolean
    default: true
    description: Whether to precompute explanations
  enable_error_analysis:
    type: boolean
    default: true
    description: Whether to enable computation of error analysis
  use_model_dependency:
    type: boolean
    default: false
    description: Whether to install the MLFlow model's dependencies in the RAI environment
  use_conda:
    type: boolean
    default: false
    description: Whether to use conda to install dependencies
  model_type:
    type: string
    enum: ['pyfunc', 'fastai', 'pytorch']
    description: The type of MLFlow model to deserialize
  # guided_gradcam doesn't work with transformer vision models
  # and shap isn't supported for automl images models
  # for more details on XAI parameters, refer to following link
  # https://learn.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=cli#generate-explanations-for-predictions
  xai_algorithm:
    type: string
    enum: ['guided_backprop', 'guided_gradcam', 'integrated_gradients', 'xrai', 'shap']
    default: 'guided_backprop'
    optional: true
    description: The explanation algorithm to use for AutoML vision models, always
      set to shap for others
  n_steps:
    type: integer
    optional: true
    description: The number of steps for the integrated gradients and XRAI algorithms
  xrai_fast:
    type: boolean
    optional: true
    description: Whether to use the faster version of the XRAI algorithm
  approximation_method:
    type: string
    enum: ['gausslegendre', 'riemann_middle']
    optional: true
    description: The approximation method to use for the integrated gradients algorithm
  confidence_score_threshold_multilabel:
    type: number
    optional: true
    description: The confidence score threshold for multilabel classification explanations,
      above which the labels are selected for generating explanations
  image_width_in_inches:
    type: number
    optional: true
    description: The width to resize the image to in inches
  max_evals:
    type: integer
    optional: true
    description: The maximum number of evaluations to run in shap's hierarchical image
      explainer.
  num_masks:
    type: integer
    optional: true
    description: The number of masks to use for the DRISE image explainer for object
      detection.
  mask_res:
    type: integer
    optional: true
    description: The resolution of the masks to use for the DRISE image explainer
      for object detection.
  dataset_type:
    type: string
    default: public
    enum: ['private', 'public']
    description: The type of image dataset to use, whether the images are on private
      azure blob storage or public urls
outputs:
  dashboard:
    type: path
    description: Path to which RAIVisionInsights is serialized to for connecting to
      compute instance
  ux_json:
    type: path
    description: Json file to which UX is serialized to for viewing in static AzureML
      Studio UI
code: ../src
environment: azureml://registries/azureml/environments/responsibleai-vision/versions/22
command: >-
  python ./rai_vision_insights.py
  --task_type ${{inputs.task_type}}
  --model_input '${{inputs.model_input}}'
  --model_info '${{inputs.model_info}}'
  --test_dataset ${{inputs.test_dataset}}
  --dataset_type ${{inputs.dataset_type}}
  --target_column_name '${{inputs.target_column_name}}'
  --maximum_rows_for_test_dataset ${{inputs.maximum_rows_for_test_dataset}}
  --classes '${{inputs.classes}}'
  --categorical_metadata_features '${{inputs.categorical_metadata_features}}'
  --dropped_metadata_features '${{inputs.dropped_metadata_features}}'
  --precompute_explanation ${{inputs.precompute_explanation}}
  --enable_error_analysis ${{inputs.enable_error_analysis}}
  --use_model_dependency ${{inputs.use_model_dependency}}
  --use_conda ${{inputs.use_conda}}
  --model_type ${{inputs.model_type}}
  $[[--xai_algorithm ${{inputs.xai_algorithm}}]]
  $[[--n_steps ${{inputs.n_steps}}]]
  $[[--xrai_fast ${{inputs.xrai_fast}}]]
  $[[--approximation_method ${{inputs.approximation_method}}]]
  $[[--confidence_score_threshold_multilabel ${{inputs.confidence_score_threshold_multilabel}}]]
  $[[--image_width_in_inches ${{inputs.image_width_in_inches}}]]
  $[[--max_evals ${{inputs.max_evals}}]]
  $[[--num_masks ${{inputs.num_masks}}]]
  $[[--mask_res ${{inputs.mask_res}}]]
  --device -1
  --component_name rai_vision_insights
  --component_version 0.0.21
  --dashboard ${{outputs.dashboard}}
  --ux_json ${{outputs.ux_json}}
