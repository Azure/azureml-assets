$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
type: pipeline

version: 0.0.10
name: diffusers_text_to_image_dreambooth_pipeline
display_name: Text to Image Dreambooth Finetuning Diffusers Pipeline
description: Pipeline component for text to image dreambooth training using diffusers
  library and transformers models.

is_deterministic: false

inputs:
  compute_model_import:
    type: string
    optional: false
    description: Compute to be used for model_import eg. provide 'FT-Cluster' if your
      compute is named 'FT-Cluster'

  compute_finetune:
    type: string
    optional: false
    description: Compute to be used for finetune eg. provide 'FT-Cluster' if your
      compute is named 'FT-Cluster'

  instance_count:
    type: integer
    default: 1
    optional: true
    description: Number of nodes to be used for finetuning (used for distributed training)

  process_count_per_instance:
    type: integer
    default: 1
    optional: true
    description: Number of gpus to be used per node for finetuning, should be equal
      to number of gpu per node in the compute SKU used for finetune

  # ########################### Model Selector Component ########################### #
  # Model family
  model_family:
    type: string
    optional: true
    default: HuggingFaceImage
    enum:
    - HuggingFaceImage
    description: Which framework the model belongs to.

  model_name:
    type: string
    optional: true
    description: Please select models from AzureML Model Assets for all supported
      models. For HuggingFace models, which are not supported in AuzreML model registry,
      input HuggingFace model_name here. The Model will be downloaded from HuggingFace
      hub using this model_name and are subject to third party license terms available
      on the HuggingFace model details page. It is the user responsibility to comply
      with the model's license terms.

  pytorch_model:
    type: custom_model
    optional: true
    description: Pytorch Model registered in AzureML Asset.

  mlflow_model:
    type: mlflow_model
    optional: true
    description: Mlflow Model registered in AzureML Asset.

  download_from_source:
    type: boolean
    optional: true
    default: false
    description: Download model directly from HuggingFace instead of system registry

  # ########################### Finetuning Component ########################### #

  # component input: Instance data dir
  instance_data_dir:
    type: uri_folder
    optional: false
    description: A folder containing the training data of instance images.

  class_data_dir:
    type: uri_folder
    optional: true
    mode: download
    description: (Optional) A folder containing the training data of class images.
      You can place existing images in class_data_dir, and the training job will generate
      any additional images so that num_class_images are present in class_data_dir
      during training time.

  task_name:
    type: string
    enum:
    - stable-diffusion-text-to-image
    description: Which task the model is solving.

  # Instance prompt
  instance_prompt:
    type: string
    optional: true # Failure to be caught on ES side
    description: The prompt with identifier specifying the instance.

  resolution:
    type: integer
    optional: true
    default: 512
    description: The image resolution for training.

  # Lora parameters
  # LoRA reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights. This vastly reduces the storage requirement for large models adapted to specific tasks and enables efficient task-switching during deployment all without introducing inference latency. LoRA also outperforms several other adaptation methods including adapter, prefix-tuning, and fine-tuning.
  apply_lora:
    type: boolean
    default: true
    optional: false
    description: If "true" enables lora.

  lora_alpha:
    type: integer
    default: 128
    optional: true
    description: alpha attention parameter for lora.

  lora_r:
    type: integer
    default: 8
    optional: true
    description: lora dimension

  lora_dropout:
    type: number
    default: 0.0
    optional: true
    description: lora dropout value

  # Tokenizer
  tokenizer_max_length:
    type: integer
    optional: true
    description: The maximum length of the tokenizer. If not set, will default to
      the tokenizer's max length.

  # Text Encoder
  text_encoder_type:
    type: string
    enum:
    - CLIPTextModel
    - T5EncoderModel
    optional: true
    description: Text encoder to be used.

  text_encoder_name:
    type: string
    optional: true
    description: Huggingface id of text encoder. This model should of type specified
      in `text_encoder_type`. If not specified the default from the model will be
      used.

  train_text_encoder:
    type: boolean
    default: false
    optional: true
    description: Whether to train the text encoder. If set, the text encoder should
      be float32 precision.

  pre_compute_text_embeddings:
    type: boolean
    default: true
    optional: true
    description: Whether or not to pre-compute text embeddings. If text embeddings
      are pre-computed, the text encoder will not be kept in memory during training
      and will leave more GPU memory available for training the rest of the model.
      This is not compatible with `--train_text_encoder`.

  text_encoder_use_attention_mask:
    type: boolean
    default: false
    optional: true
    description: Whether to use attention mask for the text encoder

  # UNET related
  class_labels_conditioning:
    type: string
    optional: true
    description: The optional `class_label` conditioning to pass to the unet, available
      values are `timesteps`.

  # Noise Scheduler
  noise_scheduler_name:
    type: string
    enum:
    - DPMSolverMultistepScheduler
    - DDPMScheduler
    - PNDMScheduler
    optional: true
    description: Noise scheduler to be used.

  noise_scheduler_num_train_timesteps:
    type: integer
    optional: true
    description: The number of diffusion steps to train the model.

  noise_scheduler_variance_type:
    type: string
    enum:
    - fixed_small
    - fixed_small_log
    - fixed_large
    - fixed_large_log
    - learned
    - learned_range
    optional: true
    description: Clip the variance when adding noise to the denoised sample.

  noise_scheduler_prediction_type:
    type: string
    enum:
    - epsilon
    - sample
    - v_prediction
    optional: true
    description: Prediction type of the scheduler function; can be `epsilon` (predicts
      the noise of the diffusion process), `sample` (directly predicts the noisy sample`)
      or `v_prediction` (see section 2.4 of [Imagen Video](https://imagen.research.google/video/paper.pdf)
      paper).

  noise_scheduler_timestep_spacing:
    type: string
    optional: true
    description: The way the timesteps should be scaled. Refer to Table 2 of the [Common
      Diffusion Noise Schedules and Sample Steps are Flawed](https://huggingface.co/papers/2305.08891)
      for more information.

  noise_scheduler_steps_offset:
    type: integer
    optional: true
    description: An offset added to the inference steps. You can use a combination
      of `offset=1` and `set_alpha_to_one=False` to make the last step use step 0
      for the previous alpha product like in Stable Diffusion.

  extra_noise_scheduler_args:
    type: string
    optional: true
    description: Optional additional arguments that are supplied to noise scheduler.
      The arguments should be semi-colon separated key value pairs and should be enclosed
      in double quotes. For example, "clip_sample_range=1.0; clip_sample=True" for
      DDPMScheduler.

  offset_noise:
    type: boolean
    optional: true
    description: Fine-tuning against a modified noise. See https://www.crosslabs.org//blog/diffusion-with-offset-noise
      for more information.

  # Prior preservation loss
  with_prior_preservation:
    type: boolean
    default: true
    description: Flag to add prior preservation loss.
  class_prompt:
    type: string
    optional: true
    description: The prompt to specify images in the same class as provided instance
      images.
  num_class_images:
    type: integer
    default: 100
    optional: true
    description: Minimal class images for prior preservation loss. If there are not
      enough images already present in class_data_dir, additional images will be sampled
      with class_prompt.
  prior_generation_precision:
    type: string
    optional: true
    default: "fp32"
    enum:
    - "fp32"
    - "fp16"
    - "bf16"
    description: Choose prior generation precision between fp32, fp16 and bf16 (bfloat16).
      Bf16 requires PyTorch >= 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if
      a GPU is available else fp32.
  prior_loss_weight:
    type: number
    default: 1.0
    optional: true
    description: The weight of prior preservation loss.

  sample_batch_size:
    type: integer
    default: 4
    optional: true
    description: "Batch size (per device) for sampling class images when training\
      \ with_prior_preservation set to True."

  # Validation parameters
  num_validation_images:
    type: integer
    default: 0
    description: "Specify number of images to generate using instance_prompt. Images\
      \ are stored in the output/checkpoint-* directories. Please note that this will\
      \ increase the training time. If you select num_validation_images = 0, then\
      \ run will generate 5 images in last checkpoint."

  # Training related
  number_of_workers:
    type: integer
    default: 6
    optional: true
    description: Number of subprocesses to use for data loading (PyTorch only). 0
      means that the data will be loaded in the main process.

  number_of_epochs:
    type: integer
    optional: true
    description: Number of training epochs. If left empty, will be chosen automatically
      based on the task type and model selected.

  max_steps:
    type: integer
    optional: true
    description: If set to a positive number, the total number of training steps to
      perform. Overrides 'number_of_epochs'. In case of using a finite iterable dataset
      the training may stop before reaching the set number of steps when all data
      is exhausted. If left empty, will be chosen automatically based on the task
      type and model selected.

  training_batch_size:
    type: integer
    default: 1
    optional: true
    description: Train batch size. If left empty, will be chosen automatically based
      on the task type and model selected.

  auto_find_batch_size:
    type: boolean
    default: false
    optional: true
    description: Flag to enable auto finding of batch size. If the provided 'per_device_train_batch_size'
      goes into Out Of Memory (OOM) enabling auto_find_batch_size will find the correct
      batch size by iteratively reducing 'per_device_train_batch_size' by a factor
      of 2 till the OOM is fixed.

  # learning rate and learning rate scheduler
  learning_rate:
    type: number
    optional: true
    description: Start learning rate. Defaults to linear scheduler. If left empty,
      will be chosen automatically based on the task type and model selected.

  learning_rate_scheduler:
    type: string
    optional: true
    enum:
    - warmup_linear
    - warmup_cosine
    - warmup_cosine_with_restarts
    - warmup_polynomial
    - constant
    - warmup_constant
    description: The scheduler type to use. If left empty, will be chosen automatically
      based on the task type and model selected.

  warmup_steps:
    type: integer
    default: 0
    optional: true
    description: Number of steps used for a linear warmup from 0 to learning_rate.
      If left empty, will be chosen automatically based on the task type and model
      selected.

  # optimizer
  optimizer:
    type: string
    optional: true
    enum:
    - adamw_hf
    - adamw
      # - adamw_torch_xla
      # - adamw_apex_fused
      # - adamw_bnb_8bit
      # - adamw_anyprecision
    - sgd
    - adafactor
    - adagrad
    - adamw_ort_fused
    description: optimizer to be used while training. 'adamw_ort_fused' optimizer
      is only supported for ORT training. If left empty, will be chosen automatically
      based on the task type and model selected.

  weight_decay:
    type: number
    default: 0
    optional: true
    description: The weight decay to apply (if not zero) to all layers except all
      bias and LayerNorm weights in AdamW and sgd optimizer. If left empty, will be
      chosen automatically based on the task type and model selected.

  extra_optim_args:
    type: string
    default: ""
    optional: true
    description: Optional additional arguments that are supplied to SGD Optimizer.
      The arguments should be semi-colon separated key value pairs and should be enclosed
      in double quotes. For example, "momentum=0.5; nesterov=True" for sgd. Please
      make sure to use a valid parameter names for the chosen optimizer. For exact
      parameter names, please refer https://pytorch.org/docs/1.13/generated/torch.optim.SGD.html#torch.optim.SGD
      for SGD. Parameters supplied in extra_optim_args will take precedence over the
      parameter supplied via other arguments such as weight_decay. If weight_decay
      is provided via "weight_decay" parameter and via extra_optim_args both, values
      specified in extra_optim_args will be used.

  # gradient accumulation
  gradient_accumulation_step:
    type: integer
    optional: true
    description: Number of update steps to accumulate the gradients for, before performing
      a backward/update pass. If left empty, will be chosen automatically based on
      the task type and model selected.

  max_grad_norm:
    type: number
    optional: true
    description: Maximum gradient norm (for gradient clipping). If left empty, will
      be chosen automatically based on the task type and model selected.

  # mixed precision training
  precision:
    type: string
    enum:
    - "32"
    - "16"
    default: "32"
    optional: true
    description: Apply mixed precision training. This can reduce memory footprint
      by performing operations in half-precision.

  # random seed
  random_seed:
    type: integer
    default: 42
    optional: true
    description: Random seed that will be set at the beginning of training.

  # logging strategy parameters
  logging_strategy:
    type: string
    default: epoch
    optional: true
    enum:
    - epoch
    - steps
    description: The logging strategy to adopt during training.

  logging_steps:
    type: integer
    default: 500
    optional: true
    description: Number of update steps between two logs if logging_strategy='steps'.

  save_total_limit:
    type: integer
    default: 5
    optional: true
    description: If a value is passed, will limit the total amount of checkpoints.
      Deletes the older checkpoints in output_dir. If the value is -1 saves all checkpoints".

  # save mlflow model
  save_as_mlflow_model:
    type: boolean
    default: true
    optional: true
    description: Save as mlflow model with pyfunc as flavour.

outputs:
  # ########################### Finetuning Component ########################### #
  mlflow_model_folder:
    type: mlflow_model
    description: Output dir to save the finetune model as mlflow model.
  pytorch_model_folder:
    type: custom_model
    description: Output dir to save the finetune model as torch model.

jobs:
  text_to_image_model_import:
    type: command
    component: azureml:diffusers_text_to_image_model_import:0.0.9
    compute: ${{parent.inputs.compute_model_import}}
    inputs:
      model_family: ${{parent.inputs.model_family}}
      model_name: ${{parent.inputs.model_name}}
      pytorch_model: ${{parent.inputs.pytorch_model}}
      mlflow_model: ${{parent.inputs.mlflow_model}}
      download_from_source: ${{parent.inputs.download_from_source}}

  text_to_image_dreambooth_finetune:
    type: command
    component: azureml:diffusers_text_to_image_finetune:0.0.9
    compute: ${{parent.inputs.compute_finetune}}
    distribution:
      type: pytorch
      process_count_per_instance: ${{parent.inputs.process_count_per_instance}}
    resources:
      instance_count: ${{parent.inputs.instance_count}}
      shm_size: "16g"
    inputs:
      # Model path is same as what is output of model selector
      model_path: ${{parent.jobs.text_to_image_model_import.outputs.output_dir}}
      instance_data_dir: ${{parent.inputs.instance_data_dir}}
      class_data_dir: ${{parent.inputs.class_data_dir}}
      task_name: ${{parent.inputs.task_name}}
      instance_prompt: ${{parent.inputs.instance_prompt}}
      resolution: ${{parent.inputs.resolution}}
      apply_lora: ${{parent.inputs.apply_lora}}
      lora_alpha: ${{parent.inputs.lora_alpha}}
      lora_r: ${{parent.inputs.lora_r}}
      lora_dropout: ${{parent.inputs.lora_dropout}}
      tokenizer_max_length: ${{parent.inputs.tokenizer_max_length}}
      text_encoder_type: ${{parent.inputs.text_encoder_type}}
      text_encoder_name: ${{parent.inputs.text_encoder_name}}
      train_text_encoder: ${{parent.inputs.train_text_encoder}}
      pre_compute_text_embeddings: ${{parent.inputs.pre_compute_text_embeddings}}
      text_encoder_use_attention_mask: ${{parent.inputs.text_encoder_use_attention_mask}}
      class_labels_conditioning: ${{parent.inputs.class_labels_conditioning}}
      noise_scheduler_name: ${{parent.inputs.noise_scheduler_name}}
      noise_scheduler_num_train_timesteps: ${{parent.inputs.noise_scheduler_num_train_timesteps}}
      noise_scheduler_variance_type: ${{parent.inputs.noise_scheduler_variance_type}}
      noise_scheduler_prediction_type: ${{parent.inputs.noise_scheduler_prediction_type}}
      noise_scheduler_timestep_spacing: ${{parent.inputs.noise_scheduler_timestep_spacing}}
      noise_scheduler_steps_offset: ${{parent.inputs.noise_scheduler_steps_offset}}
      extra_noise_scheduler_args: ${{parent.inputs.extra_noise_scheduler_args}}
      offset_noise: ${{parent.inputs.offset_noise}}
      with_prior_preservation: ${{parent.inputs.with_prior_preservation}}
      class_prompt: ${{parent.inputs.class_prompt}}
      num_class_images: ${{parent.inputs.num_class_images}}
      prior_generation_precision: ${{parent.inputs.prior_generation_precision}}
      prior_loss_weight: ${{parent.inputs.prior_loss_weight}}
      sample_batch_size: ${{parent.inputs.sample_batch_size}}
      num_validation_images: ${{parent.inputs.num_validation_images}}
      number_of_workers: ${{parent.inputs.number_of_workers}}
      number_of_epochs: ${{parent.inputs.number_of_epochs}}
      max_steps: ${{parent.inputs.max_steps}}
      training_batch_size: ${{parent.inputs.training_batch_size}}
      auto_find_batch_size: ${{parent.inputs.auto_find_batch_size}}
      learning_rate: ${{parent.inputs.learning_rate}}
      learning_rate_scheduler: ${{parent.inputs.learning_rate_scheduler}}
      warmup_steps: ${{parent.inputs.warmup_steps}}
      optimizer: ${{parent.inputs.optimizer}}
      weight_decay: ${{parent.inputs.weight_decay}}
      extra_optim_args: ${{parent.inputs.extra_optim_args}}
      gradient_accumulation_step: ${{parent.inputs.gradient_accumulation_step}}
      max_grad_norm: ${{parent.inputs.max_grad_norm}}
      precision: ${{parent.inputs.precision}}
      random_seed: ${{parent.inputs.random_seed}}
      logging_strategy: ${{parent.inputs.logging_strategy}}
      logging_steps: ${{parent.inputs.logging_steps}}
      save_total_limit: ${{parent.inputs.save_total_limit}}
      save_as_mlflow_model: ${{parent.inputs.save_as_mlflow_model}}
    outputs:
      mlflow_model_folder: ${{parent.outputs.mlflow_model_folder}}
      pytorch_model_folder: ${{parent.outputs.pytorch_model_folder}}
