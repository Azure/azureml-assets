$schema: https://azuremlschemas.azureedge.net/latest/model.schema.json
name: deformable_detr_twostage_refine_r50_16x2_50e_coco
path: ./
properties:
  SharedComputeCapacityEnabled: true
  SHA: c709d38a04d1f3ceb13e916a0709089a92d721da
  finetuning-tasks: image-object-detection
  finetune-min-sku-spec: 4|1|28|176
  finetune-recommended-sku: Standard_NC4as_T4_v3, Standard_NC6s_v3, 
    Standard_NC8as_T4_v3, Standard_NC12s_v3, Standard_NC16as_T4_v3, 
    Standard_NC24s_v3, Standard_NC64as_T4_v3, Standard_NC96ads_A100_v4, 
    Standard_ND96asr_v4, Standard_ND96amsr_A100_v4, Standard_ND40rs_v2
  evaluation-min-sku-spec: 4|1|28|176
  evaluation-recommended-sku: Standard_NC4as_T4_v3, Standard_NC6s_v3, 
    Standard_NC8as_T4_v3, Standard_NC12s_v3, Standard_NC16as_T4_v3, 
    Standard_NC24s_v3, Standard_NC64as_T4_v3, Standard_NC96ads_A100_v4, 
    Standard_ND96asr_v4, Standard_ND96amsr_A100_v4, Standard_ND40rs_v2
  inference-min-sku-spec: 4|0|14|28
  inference-recommended-sku: Standard_DS3_v2, Standard_D4a_v4, Standard_D4as_v4,
    Standard_DS4_v2, Standard_D8a_v4, Standard_D8as_v4, Standard_DS5_v2, 
    Standard_D16a_v4, Standard_D16as_v4, Standard_D32a_v4, Standard_D32as_v4, 
    Standard_D48a_v4, Standard_D48as_v4, Standard_D64a_v4, Standard_D64as_v4, 
    Standard_D96a_v4, Standard_D96as_v4, Standard_FX4mds, Standard_F8s_v2, 
    Standard_FX12mds, Standard_F16s_v2, Standard_F32s_v2, Standard_F48s_v2, 
    Standard_F64s_v2, Standard_F72s_v2, Standard_FX24mds, Standard_FX36mds, 
    Standard_FX48mds, Standard_E4s_v3, Standard_E8s_v3, Standard_E16s_v3, 
    Standard_E32s_v3, Standard_E48s_v3, Standard_E64s_v3, Standard_NC4as_T4_v3, 
    Standard_NC6s_v3, Standard_NC8as_T4_v3, Standard_NC12s_v3, 
    Standard_NC16as_T4_v3, Standard_NC24s_v3, Standard_NC64as_T4_v3, 
    Standard_NC24ads_A100_v4, Standard_NC48ads_A100_v4, 
    Standard_NC96ads_A100_v4, Standard_ND96asr_v4, Standard_ND96amsr_A100_v4, 
    Standard_ND40rs_v2
tags:
  Deprecated: ''
  SharedComputeCapacityEnabled: ''
  openmmlab_model_id: deformable_detr_twostage_refine_r50_16x2_50e_coco
  training_dataset: COCO
  license: apache-2.0
  model_specific_defaults:
    apply_deepspeed: 'false'
    apply_ort: 'false'
  task: image-segmentation
  inference_compute_allow_list: [Standard_DS3_v2, Standard_D4a_v4, 
      Standard_D4as_v4, Standard_DS4_v2, Standard_D8a_v4, Standard_D8as_v4, 
      Standard_DS5_v2, Standard_D16a_v4, Standard_D16as_v4, Standard_D32a_v4, 
      Standard_D32as_v4, Standard_D48a_v4, Standard_D48as_v4, Standard_D64a_v4, 
      Standard_D64as_v4, Standard_D96a_v4, Standard_D96as_v4, Standard_FX4mds, 
      Standard_F8s_v2, Standard_FX12mds, Standard_F16s_v2, Standard_F32s_v2, 
      Standard_F48s_v2, Standard_F64s_v2, Standard_F72s_v2, Standard_FX24mds, 
      Standard_FX36mds, Standard_FX48mds, Standard_E4s_v3, Standard_E8s_v3, 
      Standard_E16s_v3, Standard_E32s_v3, Standard_E48s_v3, Standard_E64s_v3, 
      Standard_NC4as_T4_v3, Standard_NC6s_v3, Standard_NC8as_T4_v3, 
      Standard_NC12s_v3, Standard_NC16as_T4_v3, Standard_NC24s_v3, 
      Standard_NC64as_T4_v3, Standard_NC24ads_A100_v4, Standard_NC48ads_A100_v4, 
      Standard_NC96ads_A100_v4, Standard_ND96asr_v4, Standard_ND96amsr_A100_v4, 
      Standard_ND40rs_v2]
  evaluation_compute_allow_list: [Standard_NC4as_T4_v3, Standard_NC6s_v3, 
      Standard_NC8as_T4_v3, Standard_NC12s_v3, Standard_NC16as_T4_v3, 
      Standard_NC24s_v3, Standard_NC64as_T4_v3, Standard_NC96ads_A100_v4, 
      Standard_ND96asr_v4, Standard_ND96amsr_A100_v4, Standard_ND40rs_v2]
  finetune_compute_allow_list: [Standard_NC4as_T4_v3, Standard_NC6s_v3, 
      Standard_NC8as_T4_v3, Standard_NC12s_v3, Standard_NC16as_T4_v3, 
      Standard_NC24s_v3, Standard_NC64as_T4_v3, Standard_NC96ads_A100_v4, 
      Standard_ND96asr_v4, Standard_ND96amsr_A100_v4, Standard_ND40rs_v2]
version: 11
description: |
  `deformable_detr_twostage_refine_r50_16x2_50e_coco` model is from <a href="https://github.com/open-mmlab/mmdetection/tree/v2.28.2" target="_blank">OpenMMLab's MMDetection library</a>. This model is reported to obtain <a href="https://github.com/open-mmlab/mmdetection/blob/e9cae2d0787cd5c2fc6165a6061f92fa09e48fb1/configs/deformable_detr/metafile.yml#L55" target="_blank">box AP of 46.8 for object-detection task on COCO dataset</a>. To understand the naming style used, please refer to <a href="https://mmdetection.readthedocs.io/en/v2.28.2/tutorials/config.html#config-name-style" target="_blank">MMDetection's Config Name Style</a>.

  DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.

  > The above abstract is from MMDetection website. Review the <a href="https://github.com/open-mmlab/mmdetection/tree/v2.28.2/configs/deformable_detr" target="_blank">original-model-card</a> to understand the data used to train the model, evaluation metrics, license, intended uses, limitations and bias before using the model.

  > <b>Deprecation Warning</b>: This model is only compatible with mmdet <= 2.28 and is deprecated, will be deleted from Model Catalog by the End of January 2024. We recommend using `mmd-3x-deformable-detr_refine_twostage_r50_16xb2-50e_coco` from <a href="https://ml.azure.com/model/catalog" target="_blank">the AzureML model catalog</a>. In our model catalog, the models prefixed with mmdet-3x are compatible with mmdet >= 3.1.0.

  ### Inference samples

  Inference type|Python sample (Notebook)|CLI with YAML
  |--|--|--|
  Real time|<a href="https://aka.ms/azureml-infer-sdk-image-object-detection" target="_blank">image-object-detection-online-endpoint.ipynb</a>|<a href="https://aka.ms/azureml-infer-cli-image-object-detection" target="_blank">image-object-detection-online-endpoint.sh</a>
  Batch |<a href="https://aka.ms/azureml-infer-batch-sdk-image-object-detection" target="_blank">image-object-detection-batch-endpoint.ipynb</a>|<a href="https://aka.ms/azureml-infer-batch-cli-image-object-detection" target="_blank">image-object-detection-batch-endpoint.sh</a>

  ### Finetuning samples

  Task|Use case|Dataset|Python sample (Notebook)|CLI with YAML
  |---|--|--|--|--|
  Image object detection|Image object detection|[fridgeObjects](https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip)|<a href="https://aka.ms/azureml-ft-sdk-image-object-detection" target="_blank">fridgeobjects-object-detection.ipynb</a>|<a href="https://aka.ms/azureml-ft-cli-image-object-detection" target="_blank">fridgeobjects-object-detection.sh</a>

  ### Model Evaluation

  |Task|Use case|Dataset|Python sample (Notebook)|
  |---|--|--|--|
  Image object detection|Image object detection|[fridgeObjects](https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip)|<a href="https://aka.ms/azureml-evaluation-sdk-image-object-detection" target="_blank">image-object-detection.ipynb</a>|

  ### Sample inputs and outputs (for real-time inference)

  #### Sample input

  ```json
  {
    "input_data": {
      "columns": [
        "image"
      ],
      "index": [0, 1],
      "data": ["image1", "image2"]
    }
  }
  ```

  Note: "image1" and "image2" string should be in base64 format or publicly accessible urls.

  #### Sample output

  ```json
  [
      {
          "boxes": [
              {
                  "box": {
                      "topX": 0.1,
                      "topY": 0.2,
                      "bottomX": 0.8,
                      "bottomY": 0.7
                  },
                  "label": "carton",
                  "score": 0.98
              }
          ]
      },
      {
          "boxes": [
              {
                  "box": {
                      "topX": 0.2,
                      "topY": 0.3,
                      "bottomX": 0.6,
                      "bottomY": 0.5
                  },
                  "label": "can",
                  "score": 0.97
              }
          ]
      }
  ]
  ```

  Note: Please refer to object detection output <a href="https://learn.microsoft.com/en-us/azure/machine-learning/reference-automl-images-schema?view=azureml-api-2#object-detection-1" target="_blank">data schema</a> for more detail.

  #### Model inference - visualization for a sample image

  <img src="https://automlcesdkdataresources.blob.core.windows.net/finetuning-image-models/images/Model_Result_Visualizations(Do_not_delete)/plot_deformable_detr_twostage_refine_r50_16x2_50e_coco_OD.png" alt="od visualization">
