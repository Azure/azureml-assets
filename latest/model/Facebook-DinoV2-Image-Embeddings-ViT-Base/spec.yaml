$schema: https://azuremlschemas.azureedge.net/latest/model.schema.json
name: Facebook-DinoV2-Image-Embeddings-ViT-Base
path: ./
properties:
  SharedComputeCapacityEnabled: true
  inference-min-sku-spec: 2|0|7|14
  inference-recommended-sku: Standard_DS2_v2, Standard_D2a_v4, Standard_D2as_v4,
    Standard_DS3_v2, Standard_D4a_v4, Standard_D4as_v4, Standard_DS4_v2, 
    Standard_D8a_v4, Standard_D8as_v4, Standard_DS5_v2, Standard_D16a_v4, 
    Standard_D16as_v4, Standard_D32a_v4, Standard_D32as_v4, Standard_D48a_v4, 
    Standard_D48as_v4, Standard_D64a_v4, Standard_D64as_v4, Standard_D96a_v4, 
    Standard_D96as_v4, Standard_F4s_v2, Standard_FX4mds, Standard_F8s_v2, 
    Standard_FX12mds, Standard_F16s_v2, Standard_F32s_v2, Standard_F48s_v2, 
    Standard_F64s_v2, Standard_F72s_v2, Standard_FX24mds, Standard_FX36mds, 
    Standard_FX48mds, Standard_E2s_v3, Standard_E4s_v3, Standard_E8s_v3, 
    Standard_E16s_v3, Standard_E32s_v3, Standard_E48s_v3, Standard_E64s_v3, 
    Standard_NC4as_T4_v3, Standard_NC6s_v3, Standard_NC8as_T4_v3, 
    Standard_NC12s_v3, Standard_NC16as_T4_v3, Standard_NC24s_v3, 
    Standard_NC64as_T4_v3, Standard_NC24ads_A100_v4, Standard_NC48ads_A100_v4, 
    Standard_NC96ads_A100_v4, Standard_ND96asr_v4, Standard_ND96amsr_A100_v4, 
    Standard_ND40rs_v2
tags:
  Preview: ''
  huggingface_model_id: facebook/dinov2-base
  SharedComputeCapacityEnabled: ''
  author: Meta
  license: apache-2.0
  task: embeddings
  hiddenlayerscanned: ""
  inference_compute_allow_list: [Standard_DS2_v2, Standard_D2a_v4, 
      Standard_D2as_v4, Standard_DS3_v2, Standard_D4a_v4, Standard_D4as_v4, 
      Standard_DS4_v2, Standard_D8a_v4, Standard_D8as_v4, Standard_DS5_v2, 
      Standard_D16a_v4, Standard_D16as_v4, Standard_D32a_v4, Standard_D32as_v4, 
      Standard_D48a_v4, Standard_D48as_v4, Standard_D64a_v4, Standard_D64as_v4, 
      Standard_D96a_v4, Standard_D96as_v4, Standard_F4s_v2, Standard_FX4mds, 
      Standard_F8s_v2, Standard_FX12mds, Standard_F16s_v2, Standard_F32s_v2, 
      Standard_F48s_v2, Standard_F64s_v2, Standard_F72s_v2, Standard_FX24mds, 
      Standard_FX36mds, Standard_FX48mds, Standard_E2s_v3, Standard_E4s_v3, 
      Standard_E8s_v3, Standard_E16s_v3, Standard_E32s_v3, Standard_E48s_v3, 
      Standard_E64s_v3, Standard_NC4as_T4_v3, Standard_NC6s_v3, 
      Standard_NC8as_T4_v3, Standard_NC12s_v3, Standard_NC16as_T4_v3, 
      Standard_NC24s_v3, Standard_NC64as_T4_v3, Standard_NC24ads_A100_v4, 
      Standard_NC48ads_A100_v4, Standard_NC96ads_A100_v4, Standard_ND96asr_v4, 
      Standard_ND96amsr_A100_v4, Standard_ND40rs_v2]
version: 3
description: |
  The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion with the DinoV2 method.

  Images are presented to the model as a sequence of fixed-size patches, which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.

  Note that this model does not include any fine-tuned heads.

  By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.

  ### Limitations and Biases

  Despite improvements thanks to the training method not using annotations, we still observe significant biases in our models toward rich households from Western countries. We expect fine-tuning will increase the biases in the features produced by the model as they will be tuned to the fine-tuning labels.

  ### License

  Apache License 2.0

  ### Inference samples

  Inference type|Python sample (Notebook)|CLI with YAML
  |--|--|--|
  Real time|<a href="https://aka.ms/azureml-infer-sdk-image-embeddings" target="_blank">image-embeddings-online-endpoint.ipynb</a>|<a href="https://aka.ms/azureml-infer-cli-image-embeddings" target="_blank">image-embeddings-online-endpoint.sh</a>

  ### Sample input and output

  #### Sample input

  ```json
  {
     "input_data":{
        "columns":[
           "image"
        ],
        "index":[0, 1],
        "data":[
           ["image1"],
           ["image2"]
        ]
     }
  }
  ```
  Note: "image1" and "image2" should be publicly accessible urls or strings in `base64` format.

  #### Sample output

  ```json
  [
      {
          "image_features": [0.55, 0.32, -0.82, ... , 0.29],
      },
      {
          "image_features": [-0.36, -0.97, 0.43, ... , 0.11],
      }
  ]
  ```
  Note: returned features have dimension 768 and are not normalized.
