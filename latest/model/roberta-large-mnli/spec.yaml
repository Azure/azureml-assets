$schema: https://azuremlschemas.azureedge.net/latest/model.schema.json
name: roberta-large-mnli
path: ./
properties:
  SHA: 0dcbcf20673c006ac2d1e324954491b96f0c0015
  datasets: multi_nli, wikipedia, bookcorpus
  finetuning-tasks: text-classification, token-classification
  languages: en
tags:
  Preview: ''
  license: mit
  min_inference_sku: Standard_DS2_v2
  task: text-classification
version: 5
description: |
  Roberta-large-MNLI is a fine-tuned version of the RoBERTa large model on the Multi-Genre Natural Language Inference (MNLI) corpus. It is a transformer-based language model for English. The model is developed on GitHub Repo by some developers, also licensed under MIT. The fine-tuned model can be used for zero-shot classification tasks. The model should not be used to intentionally create hostile or alienating environments for people, it was not trained to be factual or true representations of people or events. The RoBERTa large model card notes that: "The training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral." The predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. The  model uses Multi-Genre Natural Language Inference (MNLI) corpus, see the Repo and data card for more information. The preprocessing include tokenization and masking procedure, pretraining is done on V100 gpus with a specific settings. The fine-tuned model can be used for zero-shot classification tasks, including zero-shot sentence-pair classification (examples can be accessed on GitHub Repo) and zero-shot sequence classification.


  > The above summary was generated using ChatGPT. Review the [original model card](https://huggingface.co/roberta-large-mnli) to understand the data used to train the model, evaluation metrics, license, intended uses, limitations and bias before using the model.


  ### Inference samples

  Inference type|Python sample (Notebook)|CLI with YAML
  |--|--|--|
  Real time|[entailment-contradiction-online.ipynb](https://aka.ms/azureml-infer-online-sdk-text-classification)|[text-classification-online-endpoint.sh](https://aka.ms/azureml-infer-online-cli-text-classification)
  Batch | coming soon


  ### Model Evaluation

  |Task|Use case|Dataset|Python sample (Notebook)|
  |---|--|--|--|
  |Text Classification|Emotion Detection|[GoEmotions](https://huggingface.co/datasets/go_emotions)|[evaluate-model-text-classification.ipynb](https://aka.ms/azureml-eval-sdk-text-classification)|


  ### Finetuning samples

  Task|Use case|Dataset|Python sample (Notebook)|CLI with YAML
  |---|--|--|--|--|
  Text Classification|Emotion Detection|[Emotion](https://huggingface.co/datasets/dair-ai/emotion)|[emotion-detection.ipynb](https://aka.ms/azureml-ft-sdk-emotion-detection)|[emotion-detection.sh](https://aka.ms/azureml-ft-cli-emotion-detection)
  Token Classification|Token Classification|[Conll2003](https://huggingface.co/datasets/conll2003)|[token-classification.ipynb](https://aka.ms/azureml-ft-sdk-token-classification)|[token-classification.sh](https://aka.ms/azureml-ft-cli-token-classification)


  ### Sample inputs and outputs (for real-time inference)

  #### Sample input
  ```json
  {
      "inputs": {
          "input_string": ["Today was an amazing day!", "It was an unfortunate series of events."]
      }
  }
  ```

  #### Sample output
  ```json
  [
      {
          "label": "NEUTRAL",
          "score": 0.655748724937439
      },
      {
          "label": "NEUTRAL",
          "score": 0.7130465507507324
      }
  ]
  ```
