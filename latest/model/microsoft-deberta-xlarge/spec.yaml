$schema: https://azuremlschemas.azureedge.net/latest/model.schema.json
name: microsoft-deberta-xlarge
path: ./
properties:
  SHA: 971e67361eb2580900e26b7062470dee4773d324
  datasets: ''
  evaluation-min-sku-spec: 8|0|28|56
  evaluation-recommended-sku: Standard_DS4_v2
  finetune-min-sku-spec: 4|1|28|176
  finetune-recommended-sku: Standard_NC24rs_v3
  finetuning-tasks: text-classification, token-classification, question-answering
  inference-min-sku-spec: 8|0|28|56
  inference-recommended-sku: Standard_DS4_v2, Standard_D8a_v4, Standard_D8as_v4, Standard_DS5_v2,
    Standard_D16a_v4, Standard_D16as_v4, Standard_D32a_v4, Standard_D32as_v4, Standard_D48a_v4,
    Standard_D48as_v4, Standard_D64a_v4, Standard_D64as_v4, Standard_D96a_v4, Standard_D96as_v4,
    Standard_FX12mds, Standard_F16s_v2, Standard_F32s_v2, Standard_F48s_v2, Standard_F64s_v2,
    Standard_F72s_v2, Standard_FX24mds, Standard_FX36mds, Standard_FX48mds, Standard_E8s_v3,
    Standard_E16s_v3, Standard_E32s_v3, Standard_E48s_v3, Standard_E64s_v3, Standard_NC4as_T4_v3,
    Standard_NC6s_v3, Standard_NC8as_T4_v3, Standard_NC12s_v3, Standard_NC16as_T4_v3,
    Standard_NC24s_v3, Standard_NC64as_T4_v3, Standard_NC24ads_A100_v4, Standard_NC48ads_A100_v4,
    Standard_NC96ads_A100_v4, Standard_ND96asr_v4, Standard_ND96amsr_A100_v4, Standard_ND40rs_v2
  languages: en
tags:
  Preview: ''
  computes_allow_list:
  - Standard_NC6s_v3
  - Standard_NC12s_v3
  - Standard_NC24s_v3
  - Standard_NC24rs_v3
  - Standard_NC6s_v2
  - Standard_NC12s_v2
  - Standard_NC24s_v2
  - Standard_NC24rs_v2
  - Standard_NC4as_T4_v3
  - Standard_NC8as_T4_v3
  - Standard_NC16as_T4_v3
  - Standard_NC64as_T4_v3
  - Standard_ND6s
  - Standard_ND12s
  - Standard_ND24s
  - Standard_ND24rs
  - Standard_ND40rs_v2
  - Standard_ND96asr_v4
  license: mit
  model_specific_defaults:
    apply_deepspeed: 'true'
    apply_lora: 'true'
    apply_ort: 'true'
  task: fill-mask
version: 13
description: |
  DeBERTa is a model that improves on the BERT and RoBERTa models by using disentangled attention and an enhanced mask decoder. It performance better on several NLU tasks than RoBERTa with 80GB training data. The DeBERTa XLarge model has 48 layers and a hidden size of 1024 with 750 million parameters. It demonstrates good results when fine-tuned on several NLU tasks like SQuAD and GLUE benchmark. If you use DeBERTa in your work, the authors request that you cite their papers.
  <br>Please Note: This model accepts masks in `[mask]` format. See Sample input for reference.Â 
  > The above summary was generated using ChatGPT. Review the <a href="https://huggingface.co/microsoft/deberta-xlarge" target="_blank">original model card</a> to understand the data used to train the model, evaluation metrics, license, intended uses, limitations and bias before using the model.

  ### Inference samples

  Inference type|Python sample (Notebook)|CLI with YAML
  |--|--|--|
  Real time|<a href="https://aka.ms/azureml-infer-online-sdk-fill-mask" target="_blank">fill-mask-online-endpoint.ipynb</a>|<a href="https://aka.ms/azureml-infer-online-cli-fill-mask" target="_blank">fill-mask-online-endpoint.sh</a>
  Batch |<a href="https://aka.ms/azureml-infer-batch-sdk-fill-mask" target="_blank">fill-mask-batch-endpoint.ipynb</a>| coming soon


  ### Finetuning samples

  Task|Use case|Dataset|Python sample (Notebook)|CLI with YAML
  |--|--|--|--|--|
  Text Classification|Emotion Detection|<a href="https://huggingface.co/datasets/dair-ai/emotion" target="_blank">Emotion</a>|<a href="https://aka.ms/azureml-ft-sdk-emotion-detection" target="_blank">emotion-detection.ipynb</a>|<a href="https://aka.ms/azureml-ft-cli-emotion-detection" target="_blank">emotion-detection.sh</a>
  Token Classification|Named Entity Recognition|<a href="https://huggingface.co/datasets/conll2003" target="_blank">Conll2003</a>|<a href="https://aka.ms/azureml-ft-sdk-token-classification" target="_blank">named-entity-recognition.ipynb</a>|<a href="https://aka.ms/azureml-ft-cli-token-classification" target="_blank">named-entity-recognition.sh</a>
  Question Answering|Extractive Q&A|<a href="https://huggingface.co/datasets/squad" target="_blank">SQUAD (Wikipedia)</a>|<a href="https://aka.ms/azureml-ft-sdk-extractive-qa" target="_blank">extractive-qa.ipynb</a>|<a href="https://aka.ms/azureml-ft-cli-extractive-qa" target="_blank">extractive-qa.sh</a>


  ### Model Evaluation

  Task| Use case| Python sample (Notebook)| CLI with YAML
  |--|--|--|--|
  Fill Mask | Fill Mask | <a href="https://huggingface.co/datasets/rcds/wikipedia-for-mask-filling" target="_blank">rcds/wikipedia-for-mask-filling</a> | <a href="https://aka.ms/azureml-eval-sdk-fill-mask/" target="_blank">evaluate-model-fill-mask.ipynb</a> | <a href="https://aka.ms/azureml-eval-cli-fill-mask/" target="_blank">evaluate-model-fill-mask.yml</a>


  ### Sample inputs and outputs (for real-time inference)

  #### Sample input
  ```json
  {
      "inputs": {
          "input_string": ["Paris is the [MASK] of France.", "Today is a [MASK] day!"]
      }
  }
  ```

  #### Sample output
  ```json
  [
    {
      "0": "ews"
    },
    {
      "0": "rew"
    }
  ]
  ```
