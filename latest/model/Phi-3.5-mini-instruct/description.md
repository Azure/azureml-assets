Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.

### Resources
üè° [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>
üì∞ [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>
üìñ [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>
üë©‚Äçüç≥ [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>

### Model Summary
|      |      |
|------|------|
| **Architecture** | Phi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini. |
| **Inputs** | Text. It is best suited for prompts using chat format. |
| **Context length** | 128K tokens |
| **GPUs** | 512 H100-80G |
| **Training time** | 10 days |
| **Training data** | 3.4T tokens |
| **Outputs** | Generated text in response to the input |
| **Dates** | Trained between June and August 2024 |
| **Status** | This is a static model trained on an offline dataset with cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models. |
| **Supported languages** | Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian |
| **Release date** | August 20, 2024 |
| License | MIT |