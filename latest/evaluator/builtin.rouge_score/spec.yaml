type: "evaluator"
name: "builtin.rouge_score"
version: 1
displayName: "Rouge-Score-Evaluator"
description: "| | |\n| -- | -- |\n| Score range | Float [0-1]: higher means better
  quality. |\n| What is this metric? | ROUGE (Recall-Oriented Understudy for Gisting
  Evaluation) is a set of metrics used to evaluate automatic summarization and machine
  translation. It measures the overlap between generated text and reference summaries.
  ROUGE focuses on recall-oriented measures to assess how well the generated text
  covers the reference text. The ROUGE score is composed of precision, recall, and
  F1 score. |\n| How does it work? | The ROUGE score evaluates the similarity between
  the generated text and reference text based on n-gram overlap, including ROUGE-N
  (unigram, bigram, etc.), and ROUGE-L (longest common subsequence). It calculates
  precision, recall, and F1 scores to capture how well the generated text matches
  the reference text. Rouge type options are \"rouge1\" (Unigram overlap), \"rouge2\"\
  \ (Bigram overlap), \"rouge3\" (Trigram overlap),  \"rouge4\" (4-gram overlap),
  \"rouge5\" (5-gram overlap), \"rougeL\" (L-graph overlap) |\n| When to use it? |
  The recommended scenario is Natural Language Processing (NLP) tasks. Text summarization
  and document comparison are among the recommended use cases for ROUGE, especially
  when focusing on recall and the ability to capture relevant information from the
  reference text. |\n| What does it need as input? | Response, Ground Truth |\n"
evaluatorType: "builtin"
evaluatorSubType: "code"
categories: ["quality"]
tags:
  provider: "Microsoft"
initParameterSchema:
  type: "object"
  properties:
    rouge_type:
      type: "string"
      enum: ["rouge1", "rouge2", "rouge3", "rouge4", "rouge5", "rougeL"]
    f1_score_threshold:
      type: "number"
      minimum: 0
      maximum: 1
      multipleOf: 0.1
    precision_threshold:
      type: "number"
      minimum: 0
      maximum: 1
      multipleOf: 0.1
    recall_threshold:
      type: "number"
      minimum: 0
      maximum: 1
      multipleOf: 0.1
  required: ["f1_score_threshold", "precision_threshold", "recall_threshold"]
dataMappingSchema:
  type: "object"
  properties:
    ground_truth:
      type: "string"
    response:
      type: "string"
  required: ["ground_truth", "response"]
outputSchema:
  rouge_precision:
    type: "continuous"
    desirable_direction: "increase"
    min_value: 0
    max_value: 1
  rouge_recall:
    type: "continuous"
    desirable_direction: "increase"
    min_value: 0
    max_value: 1
  rouge_f1_score:
    type: "continuous"
    desirable_direction: "increase"
    min_value: 0
    max_value: 1
path: ./evaluator
