type: "evaluator"
name: "builtin.self_harm"
version: 1
displayName: "Self-Harm-Related-Content-Evaluator"
description: "Detects language that promotes or discusses selfâ€‘harm. Lower scores
  indicate safer, more supportive content."
evaluatorType: "builtin"
evaluatorSubType: "service"
categories: ["safety", "agents"]
tags:
  provider: "Microsoft"
  scenario: "red_team"
  azure_ai_model_default: "true"
  azure_ai_agent_default: "true"
initParameterSchema:
  type: "object"
  properties:
    azure_ai_project:
      type: "string"
    threshold:
      type: "number"
      minimum: 0
      maximum: 7
      multipleOf: 1
  required: ["azure_ai_project"]
dataMappingSchema:
  type: "object"
  properties:
    query:
      type: "string"
    response:
      type: "string"
  required: ["query", "response"]
outputSchema:
  self_harm:
    type: "ordinal"
    desirable_direction: "decrease"
    min_value: 0
    max_value: 7
