$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: microsoft_gllm_classification_multi_label_data_preprocess_component
version: 0.0.1
type: command

is_deterministic: True

display_name: LLM Multilabel Classification Data preprocess
description: Component to preprocess data for multilabel classification task

environment: azureml:gllm-ptca-aifx-stable-ubuntu2004-cu113-py38-torch1110:0.0.1

code: ../../scripts/preprocess

inputs:
  # Model and task name
  language_model:
    type: string
    enum:
      - "microsoft/deberta-base"
      - "microsoft/deberta-large"
      - "microsoft/deberta-xlarge"
      - "microsoft/deberta-base-mnli"
      - "microsoft/deberta-large-mnli"
      - "microsoft/deberta-xlarge-mnli"
      - "distilbert-base-uncased"
      - "distilbert-base-uncased-distilled-squad"
      - "distilbert-base-cased"
      - "distilbert-base-cased-distilled-squad"
      - "roberta-base"
      - "roberta-large"
      - "roberta-large-mnli"
      - "distilroberta-base"
      - "roberta-base-openai-detector"
      - "roberta-large-openai-detector"
      - "bert-base-uncased"
      - "bert-large-uncased"
      - "bert-base-cased"
      - "bert-large-cased"
      - "gpt2"
      - "gpt2-medium"
      - "gpt2-large"
      - "gpt2-xl"
      - "distilgpt2"
    default: bert-base-uncased
    description: hugging face model_name_or_path

  # Sequence Classification task arguments
  sentence1_key:
    type: string
    optional: true

  sentence2_key:
    type: string
    optional: true

  label_key:
    type: string
    optional: true

  label_separator:
    type: string
    optional: true
    default: "\n"
    enum:
      - "\n"
      - ","

  # Tokenizer settings
  max_seq_length:
    type: integer
    default: -1
    optional: true
    description: Max tokens of single example, default value will be max seq length of pretrained model tokenizer

  pad_to_max_length:
    type: string
    default: true
    optional: true
    description: (
      "If true, all samples get padded to `max_seq_length`. using model_max_length"
      "If false, will pad the samples dynamically when batching to the maximum length in the batch.")

  # Inputs
  train_file_path:
    type: uri_file
    optional: false
    description: Enter the train file path

  valid_file_path:
    type: uri_file
    optional: false
    description: Enter the train file path

outputs:
  output_dir:
    type: uri_folder
    description: folder to store preprocessed outputs of input data

command: >-
  python preprocess.py --task_name MultiLabelClassification --model_name_or_path ${{inputs.language_model}} [--sentence1_key ${{inputs.sentence1_key}}] [--sentence2_key ${{inputs.sentence2_key}}] [--label_key ${{inputs.label_key}}] [--label_separator ${{inputs.label_separator}}] [--max_seq_length ${{inputs.max_seq_length}}] [--pad_to_max_length ${{inputs.pad_to_max_length}}] --train_file_path ${{inputs.train_file_path}} --valid_file_path ${{inputs.valid_file_path}} --output_dir ${{outputs.output_dir}}
