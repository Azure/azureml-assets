Roberta-large-MNLI is a fine-tuned version of the RoBERTa large model on the Multi-Genre Natural Language Inference (MNLI) corpus. It is a transformer-based language model for English. The model is developed on GitHub Repo by some developers, also licensed under MIT. The fine-tuned model can be used for zero-shot classification tasks. The model should not be used to intentionally create hostile or alienating environments for people, it was not trained to be factual or true representations of people or events. The RoBERTa large model card notes that: "The training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral." The predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. The  model uses Multi-Genre Natural Language Inference (MNLI) corpus, see the Repo and data card for more information. The preprocessing include tokenization and masking procedure, pretraining is done on V100 gpus with a specific settings. The fine-tuned model can be used for zero-shot classification tasks, including zero-shot sentence-pair classification (examples can be accessed on GitHub Repo) and zero-shot sequence classification.


> The above summary was generated using ChatGPT. Review the <a href="https://huggingface.co/roberta-large-mnli" target="_blank">original model card</a> to understand the data used to train the model, evaluation metrics, license, intended uses, limitations and bias before using the model.


### Inference samples

Inference type|Python sample (Notebook)|CLI with YAML
|--|--|--|
Real time|<a href="https://aka.ms/azureml-infer-online-sdk-text-classification" target="_blank">entailment-contradiction-online.ipynb</a>|<a href="https://aka.ms/azureml-infer-online-cli-text-classification" target="_blank">text-classification-online-endpoint.sh</a>
Batch | coming soon


### Model Evaluation

Task| Use case| Dataset| Python sample (Notebook)| CLI with YAML
|--|--|--|--|--|
Text Classification | Emotion Detection | <a href="https://huggingface.co/datasets/glue/viewer/mnli/validation_matched" target="_blank">MNLI</a> | <a href="https://aka.ms/azureml-eval-sdk-text-classification" target="_blank">evaluate-model-text-classification.ipynb</a> | <a href="https://aka.ms/azureml-eval-cli-text-classification" target="_blank">evaluate-model-text-classification.yml</a>


### Finetuning samples

Task|Use case|Dataset|Python sample (Notebook)|CLI with YAML
|--|--|--|--|--|
Text Classification|Emotion Detection|<a href="https://huggingface.co/datasets/dair-ai/emotion" target="_blank">Emotion</a>|<a href="https://aka.ms/azureml-ft-sdk-emotion-detection" target="_blank">emotion-detection.ipynb</a>|<a href="https://aka.ms/azureml-ft-cli-emotion-detection" target="_blank">emotion-detection.sh</a>
Token Classification|Token Classification|<a href="https://huggingface.co/datasets/conll2003" target="_blank">Conll2003</a>|<a href="https://aka.ms/azureml-ft-sdk-token-classification" target="_blank">token-classification.ipynb</a>|<a href="https://aka.ms/azureml-ft-cli-token-classification" target="_blank">token-classification.sh</a>


### Sample inputs and outputs (for real-time inference)

#### Sample input
```json
{
    "inputs": {
        "input_string": ["Today was an amazing day!", "It was an unfortunate series of events."]
    }
}
```

#### Sample output
```json
[
    {
        "label": "NEUTRAL",
        "score": 0.655748724937439
    },
    {
        "label": "NEUTRAL",
        "score": 0.7130465507507324
    }
]
```
