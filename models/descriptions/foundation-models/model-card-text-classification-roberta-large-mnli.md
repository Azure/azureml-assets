Roberta-large-MNLI is a fine-tuned version of the RoBERTa large model on the Multi-Genre Natural Language Inference (MNLI) corpus. It is a transformer-based language model for English. The model is developed on GitHub Repo by some developers, also licensed under MIT. The fine-tuned model can be used for zero-shot classification tasks. The model should not be used to intentionally create hostile or alienating environments for people, it was not trained to be factual or true representations of people or events. The RoBERTa large model card notes that: "The training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral." The predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. The  model uses Multi-Genre Natural Language Inference (MNLI) corpus, see the Repo and data card for more information. The preprocessing include tokenization and masking procedure, pretraining is done on V100 gpus with a specific settings. The fine-tuned model can be used for zero-shot classification tasks, including zero-shot sentence-pair classification (examples can be accessed on GitHub Repo) and zero-shot sequence classification.


> The above summary was generated using ChatGPT. Review the [original model card](https://huggingface.co/roberta-large-mnli) to understand the data used to train the model, evaluation metrics, license, intended uses, limitations and bias before using the model.


### Model Evaluation

|Task|Use case|Dataset|Python sample (Notebook)|
|---|--|--|--|
|Text Classification||[GoEmotions](https://huggingface.co/datasets/go_emotions)|[evaluate-model-text-classification.ipynb](https://aka.ms/azureml-eval-sdk-text-classification)|
